{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling NIQS Hardware TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src_tf/')\n",
    "\n",
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "from qiskit.quantum_info import Operator\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from loss_functions import *\n",
    "from optimization import *\n",
    "from quantum_maps import *\n",
    "from quantum_tools import *\n",
    "from experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_map(state, kraus_list):\n",
    "    state = [K@state@tf.linalg.adjoint(K) for K in kraus_list]\n",
    "    state = tf.math.reduce_sum(tf.stack(state), axis=0)\n",
    "    return state\n",
    "\n",
    "\n",
    "def expectation_value(state, observable):\n",
    "    ev = tf.linalg.trace(observable@state)\n",
    "    return ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n = 3\n",
    "d = 2**n\n",
    "rank = 8\n",
    "\n",
    "A = tf.cast(tf.random.normal((rank*d, d), 0, 1), dtype=tf.complex64)\n",
    "B = tf.cast(tf.random.normal((rank*d, d), 0, 1), dtype=tf.complex64)\n",
    "G = A + 1j*B\n",
    "Q, R = tf.linalg.qr(G, full_matrices = False)\n",
    "D = tf.linalg.tensor_diag_part(R)\n",
    "D = tf.math.sign(D)\n",
    "D = tf.linalg.diag(D)\n",
    "U = Q@D\n",
    "\n",
    "\n",
    "kraus_target_list =  [U[i*d:(i+1)*d, :d] for i in range(rank)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "state_index, observ_index = index_generator(n, N, trace=False)\n",
    "\n",
    "input_list = []\n",
    "circuit_list = []\n",
    "for i, j in zip(state_index, observ_index):\n",
    "\n",
    "    config = numberToBase(i, 6, n)\n",
    "    state = prepare_input(config)\n",
    "\n",
    "    config = numberToBase(j, 3, n)\n",
    "    observable = pauli_observable(config)\n",
    "    \n",
    "    input_list.append([state, observable])\n",
    "\n",
    "target_list = [expectation_value(apply_map(input[0], kraus_target_list), input[1]) for input in input_list]\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0292+0.0000j, 1.301522135734558\n",
      "loss:  0.0258+0.0000j, 1.3030211925506592\n",
      "loss:  0.0229+0.0000j, 1.3061857223510742\n",
      "loss:  0.0203+0.0000j, 1.3110737800598145\n",
      "loss:  0.0181+0.0000j, 1.3191564083099365\n",
      "loss:  0.0163+0.0000j, 1.328444004058838\n",
      "loss:  0.0148+0.0000j, 1.3391997814178467\n",
      "loss:  0.0136+0.0000j, 1.3502404689788818\n",
      "loss:  0.0125+0.0000j, 1.3619322776794434\n",
      "loss:  0.0115+0.0000j, 1.3748447895050049\n",
      "loss:  0.0106+0.0000j, 1.3881510496139526\n",
      "loss:  0.0098+0.0000j, 1.4013822078704834\n",
      "loss:  0.0092+0.0000j, 1.4140253067016602\n",
      "loss:  0.0085+0.0000j, 1.4264130592346191\n",
      "loss:  0.0080+0.0000j, 1.4392167329788208\n",
      "loss:  0.0075+0.0000j, 1.4524592161178589\n",
      "loss:  0.0071+0.0000j, 1.4657750129699707\n",
      "loss:  0.0068+0.0000j, 1.4790937900543213\n",
      "loss:  0.0064-0.0000j, 1.4916974306106567\n",
      "loss:  0.0061-0.0000j, 1.5038169622421265\n",
      "loss:  0.0058+0.0000j, 1.515364408493042\n",
      "loss:  0.0056+0.0000j, 1.5264441967010498\n",
      "loss:  0.0053+0.0000j, 1.5372774600982666\n",
      "loss:  0.0051+0.0000j, 1.5479930639266968\n",
      "loss:  0.0049+0.0000j, 1.5584406852722168\n",
      "loss:  0.0047+0.0000j, 1.568676233291626\n",
      "loss:  0.0045+0.0000j, 1.5787466764450073\n",
      "loss:  0.0043+0.0000j, 1.5885753631591797\n",
      "loss:  0.0042-0.0000j, 1.5980035066604614\n",
      "loss:  0.0040+0.0000j, 1.6067698001861572\n",
      "loss:  0.0039+0.0000j, 1.6151013374328613\n",
      "loss:  0.0037+0.0000j, 1.6232023239135742\n",
      "loss:  0.0036+0.0000j, 1.6309137344360352\n",
      "loss:  0.0035-0.0000j, 1.6382954120635986\n",
      "loss:  0.0034-0.0000j, 1.6451168060302734\n",
      "loss:  0.0033+0.0000j, 1.6513895988464355\n",
      "loss:  0.0032+0.0000j, 1.6571778059005737\n",
      "loss:  0.0031+0.0000j, 1.6625423431396484\n",
      "loss:  0.0030-0.0000j, 1.6676454544067383\n",
      "loss:  0.0029+0.0000j, 1.6729224920272827\n",
      "loss:  0.0028+0.0000j, 1.6782639026641846\n",
      "loss:  0.0027+0.0000j, 1.6836588382720947\n",
      "loss:  0.0026-0.0000j, 1.6891014575958252\n",
      "loss:  0.0026+0.0000j, 1.6946507692337036\n",
      "loss:  0.0025-0.0000j, 1.7001361846923828\n",
      "loss:  0.0024+0.0000j, 1.705362319946289\n",
      "loss:  0.0023+0.0000j, 1.7104709148406982\n",
      "loss:  0.0023-0.0000j, 1.7151597738265991\n",
      "loss:  0.0022+0.0000j, 1.7196567058563232\n",
      "loss:  0.0022+0.0000j, 1.7239794731140137\n",
      "loss:  0.0021+0.0000j, 1.7281031608581543\n",
      "loss:  0.0021+0.0000j, 1.732113242149353\n",
      "loss:  0.0020+0.0000j, 1.7360553741455078\n",
      "loss:  0.0020-0.0000j, 1.7397353649139404\n",
      "loss:  0.0019+0.0000j, 1.7431546449661255\n",
      "loss:  0.0019-0.0000j, 1.746312141418457\n",
      "loss:  0.0018-0.0000j, 1.7490679025650024\n",
      "loss:  0.0018+0.0000j, 1.7516202926635742\n",
      "loss:  0.0017+0.0000j, 1.7540034055709839\n",
      "loss:  0.0017-0.0000j, 1.7562345266342163\n",
      "loss:  0.0017+0.0000j, 1.758368730545044\n",
      "loss:  0.0016-0.0000j, 1.7604396343231201\n",
      "loss:  0.0016+0.0000j, 1.7624626159667969\n",
      "loss:  0.0015+0.0000j, 1.7644492387771606\n",
      "loss:  0.0015+0.0000j, 1.7664108276367188\n",
      "loss:  0.0015-0.0000j, 1.7684650421142578\n",
      "loss:  0.0014+0.0000j, 1.7705132961273193\n",
      "loss:  0.0014-0.0000j, 1.7725602388381958\n",
      "loss:  0.0014-0.0000j, 1.7746094465255737\n",
      "loss:  0.0014-0.0000j, 1.7766693830490112\n",
      "loss:  0.0013-0.0000j, 1.7787622213363647\n",
      "loss:  0.0013-0.0000j, 1.780886173248291\n",
      "loss:  0.0013+0.0000j, 1.783026099205017\n",
      "loss:  0.0013+0.0000j, 1.7851513624191284\n",
      "loss:  0.0012+0.0000j, 1.787277102470398\n",
      "loss:  0.0012-0.0000j, 1.789389967918396\n",
      "loss:  0.0012-0.0000j, 1.791473150253296\n",
      "loss:  0.0012-0.0000j, 1.793508529663086\n",
      "loss:  0.0011+0.0000j, 1.795478105545044\n",
      "loss:  0.0011-0.0000j, 1.797365427017212\n",
      "loss:  0.0011+0.0000j, 1.7991559505462646\n",
      "loss:  0.0011-0.0000j, 1.8008390665054321\n",
      "loss:  0.0011-0.0000j, 1.8024076223373413\n",
      "loss:  0.0011-0.0000j, 1.803860068321228\n",
      "loss:  0.0010-0.0000j, 1.8052020072937012\n",
      "loss:  0.0010-0.0000j, 1.8064548969268799\n",
      "loss:  0.0010-0.0000j, 1.8076586723327637\n",
      "loss:  0.0010-0.0000j, 1.8088157176971436\n",
      "loss:  0.0010-0.0000j, 1.8099249601364136\n",
      "loss:  0.0010-0.0000j, 1.8109979629516602\n",
      "loss:  0.0009+0.0000j, 1.8120465278625488\n",
      "loss:  0.0009-0.0000j, 1.8130757808685303\n",
      "loss:  0.0009-0.0000j, 1.8141157627105713\n",
      "loss:  0.0009-0.0000j, 1.815154790878296\n",
      "loss:  0.0009-0.0000j, 1.8161969184875488\n",
      "loss:  0.0009-0.0000j, 1.8172426223754883\n",
      "loss:  0.0009+0.0000j, 1.8182897567749023\n",
      "loss:  0.0008+0.0000j, 1.819343090057373\n",
      "loss:  0.0008-0.0000j, 1.820387601852417\n",
      "loss:  0.0008+0.0000j, 1.8214191198349\n",
      "loss:  0.0008-0.0000j, 1.8224356174468994\n",
      "loss:  0.0008-0.0000j, 1.8234374523162842\n",
      "loss:  0.0008-0.0000j, 1.8244249820709229\n",
      "loss:  0.0008+0.0000j, 1.825394868850708\n",
      "loss:  0.0008+0.0000j, 1.8263388872146606\n",
      "loss:  0.0007-0.0000j, 1.8272500038146973\n",
      "loss:  0.0007-0.0000j, 1.8281257152557373\n",
      "loss:  0.0007-0.0000j, 1.828967809677124\n",
      "loss:  0.0007+0.0000j, 1.8297812938690186\n",
      "loss:  0.0007-0.0000j, 1.830570936203003\n",
      "loss:  0.0007-0.0000j, 1.8313028812408447\n",
      "loss:  0.0007+0.0000j, 1.8320192098617554\n",
      "loss:  0.0007-0.0000j, 1.8327233791351318\n",
      "loss:  0.0007-0.0000j, 1.8334569931030273\n",
      "loss:  0.0007+0.0000j, 1.8341920375823975\n",
      "loss:  0.0007-0.0000j, 1.8349461555480957\n",
      "loss:  0.0006-0.0000j, 1.8357350826263428\n",
      "loss:  0.0006-0.0000j, 1.836543083190918\n",
      "loss:  0.0006-0.0000j, 1.837357521057129\n",
      "loss:  0.0006-0.0000j, 1.8381738662719727\n",
      "loss:  0.0006+0.0000j, 1.838991403579712\n",
      "loss:  0.0006-0.0000j, 1.839811086654663\n",
      "loss:  0.0006-0.0000j, 1.840638518333435\n",
      "loss:  0.0006-0.0000j, 1.841471791267395\n",
      "loss:  0.0006-0.0000j, 1.8423075675964355\n",
      "loss:  0.0006-0.0000j, 1.8431413173675537\n",
      "loss:  0.0006-0.0000j, 1.8439257144927979\n",
      "loss:  0.0006+0.0000j, 1.8447428941726685\n",
      "loss:  0.0006-0.0000j, 1.8455479145050049\n",
      "loss:  0.0006-0.0000j, 1.8463401794433594\n",
      "loss:  0.0005+0.0000j, 1.8471205234527588\n",
      "loss:  0.0005-0.0000j, 1.8479249477386475\n",
      "loss:  0.0005-0.0000j, 1.8487303256988525\n",
      "loss:  0.0005-0.0000j, 1.8495278358459473\n",
      "loss:  0.0005-0.0000j, 1.850329041481018\n",
      "loss:  0.0005-0.0000j, 1.8511297702789307\n",
      "loss:  0.0005+0.0000j, 1.8519225120544434\n",
      "loss:  0.0005+0.0000j, 1.8526654243469238\n",
      "loss:  0.0005-0.0000j, 1.8533945083618164\n",
      "loss:  0.0005-0.0000j, 1.8541042804718018\n",
      "loss:  0.0005-0.0000j, 1.8548017740249634\n",
      "loss:  0.0005+0.0000j, 1.8554733991622925\n",
      "loss:  0.0005-0.0000j, 1.8561184406280518\n",
      "loss:  0.0005-0.0000j, 1.8567378520965576\n",
      "loss:  0.0005-0.0000j, 1.8573565483093262\n",
      "loss:  0.0005-0.0000j, 1.8579559326171875\n",
      "loss:  0.0005-0.0000j, 1.8585193157196045\n",
      "loss:  0.0005-0.0000j, 1.8590750694274902\n",
      "loss:  0.0005-0.0000j, 1.8596265316009521\n",
      "loss:  0.0005-0.0000j, 1.8601638078689575\n",
      "loss:  0.0004-0.0000j, 1.8606994152069092\n",
      "loss:  0.0004-0.0000j, 1.8612356185913086\n",
      "loss:  0.0004-0.0000j, 1.8617992401123047\n",
      "loss:  0.0004-0.0000j, 1.8623746633529663\n",
      "loss:  0.0004-0.0000j, 1.8629703521728516\n",
      "loss:  0.0004+0.0000j, 1.8635802268981934\n",
      "loss:  0.0004-0.0000j, 1.8641985654830933\n",
      "loss:  0.0004-0.0000j, 1.8647942543029785\n",
      "loss:  0.0004-0.0000j, 1.8653976917266846\n",
      "loss:  0.0004-0.0000j, 1.8660154342651367\n",
      "loss:  0.0004-0.0000j, 1.8666067123413086\n",
      "loss:  0.0004+0.0000j, 1.8671996593475342\n",
      "loss:  0.0004-0.0000j, 1.8677785396575928\n",
      "loss:  0.0004-0.0000j, 1.8683669567108154\n",
      "loss:  0.0004+0.0000j, 1.8689215183258057\n",
      "loss:  0.0004-0.0000j, 1.8694374561309814\n",
      "loss:  0.0004-0.0000j, 1.8699125051498413\n",
      "loss:  0.0004-0.0000j, 1.8703522682189941\n",
      "loss:  0.0004+0.0000j, 1.8707621097564697\n",
      "loss:  0.0004-0.0000j, 1.8711442947387695\n",
      "loss:  0.0004-0.0000j, 1.871518850326538\n",
      "loss:  0.0004+0.0000j, 1.8719024658203125\n",
      "loss:  0.0004-0.0000j, 1.8723113536834717\n",
      "loss:  0.0004-0.0000j, 1.872754693031311\n",
      "loss:  0.0004-0.0000j, 1.8732285499572754\n",
      "loss:  0.0004-0.0000j, 1.8737192153930664\n",
      "loss:  0.0004-0.0000j, 1.87421452999115\n",
      "loss:  0.0004-0.0000j, 1.8747010231018066\n",
      "loss:  0.0004-0.0000j, 1.8751753568649292\n",
      "loss:  0.0004+0.0000j, 1.8756232261657715\n",
      "loss:  0.0004-0.0000j, 1.876001000404358\n",
      "loss:  0.0004-0.0000j, 1.8763422966003418\n",
      "loss:  0.0004-0.0000j, 1.8766515254974365\n",
      "loss:  0.0003-0.0000j, 1.876941204071045\n",
      "loss:  0.0003+0.0000j, 1.8772621154785156\n",
      "loss:  0.0003-0.0000j, 1.8776049613952637\n",
      "loss:  0.0003-0.0000j, 1.877976655960083\n",
      "loss:  0.0003-0.0000j, 1.8783667087554932\n",
      "loss:  0.0003-0.0000j, 1.8787639141082764\n",
      "loss:  0.0003-0.0000j, 1.8791613578796387\n",
      "loss:  0.0003-0.0000j, 1.8795545101165771\n",
      "loss:  0.0003+0.0000j, 1.879934310913086\n",
      "loss:  0.0003+0.0000j, 1.8802862167358398\n",
      "loss:  0.0003-0.0000j, 1.8806029558181763\n",
      "loss:  0.0003+0.0000j, 1.8808594942092896\n",
      "loss:  0.0003-0.0000j, 1.881096363067627\n",
      "loss:  0.0003+0.0000j, 1.8813380002975464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0003-0.0000j, 1.8816407918930054\n",
      "loss:  0.0003-0.0000j, 1.8820574283599854\n",
      "loss:  0.0003-0.0000j, 1.882530689239502\n",
      "loss:  0.0003-0.0000j, 1.8830106258392334\n",
      "loss:  0.0003+0.0000j, 1.883467197418213\n",
      "loss:  0.0003-0.0000j, 1.8839054107666016\n",
      "loss:  0.0003-0.0000j, 1.8840510845184326\n",
      "loss:  0.0003-0.0000j, 1.884171485900879\n",
      "loss:  0.0003-0.0000j, 1.8843109607696533\n",
      "loss:  0.0003-0.0000j, 1.8845058679580688\n",
      "loss:  0.0003-0.0000j, 1.8847758769989014\n",
      "loss:  0.0003-0.0000j, 1.8850957155227661\n",
      "loss:  0.0003-0.0000j, 1.8854484558105469\n",
      "loss:  0.0003+0.0000j, 1.8858273029327393\n",
      "loss:  0.0003-0.0000j, 1.8861910104751587\n",
      "loss:  0.0003-0.0000j, 1.8865411281585693\n",
      "loss:  0.0003-0.0000j, 1.8868656158447266\n",
      "loss:  0.0003-0.0000j, 1.8871735334396362\n",
      "loss:  0.0003+0.0000j, 1.8874690532684326\n",
      "loss:  0.0003-0.0000j, 1.8877735137939453\n",
      "loss:  0.0003-0.0000j, 1.888071060180664\n",
      "loss:  0.0003-0.0000j, 1.8883886337280273\n",
      "loss:  0.0003-0.0000j, 1.888731837272644\n",
      "loss:  0.0003-0.0000j, 1.8890447616577148\n",
      "loss:  0.0003-0.0000j, 1.8893309831619263\n",
      "loss:  0.0003+0.0000j, 1.8895909786224365\n",
      "loss:  0.0003-0.0000j, 1.8898262977600098\n",
      "loss:  0.0003-0.0000j, 1.8900377750396729\n",
      "loss:  0.0003-0.0000j, 1.8902548551559448\n",
      "loss:  0.0003-0.0000j, 1.8904812335968018\n",
      "loss:  0.0003-0.0000j, 1.8907222747802734\n",
      "loss:  0.0003-0.0000j, 1.890975832939148\n",
      "loss:  0.0003-0.0000j, 1.8912649154663086\n",
      "loss:  0.0003+0.0000j, 1.8915908336639404\n",
      "loss:  0.0003+0.0000j, 1.8919496536254883\n",
      "loss:  0.0003+0.0000j, 1.892306923866272\n",
      "loss:  0.0003+0.0000j, 1.892638921737671\n",
      "loss:  0.0003-0.0000j, 1.8929212093353271\n",
      "loss:  0.0003+0.0000j, 1.8931729793548584\n",
      "loss:  0.0003-0.0000j, 1.893399953842163\n",
      "loss:  0.0003-0.0000j, 1.8936169147491455\n",
      "loss:  0.0003-0.0000j, 1.8938400745391846\n",
      "loss:  0.0003-0.0000j, 1.8940764665603638\n",
      "loss:  0.0003-0.0000j, 1.8943266868591309\n",
      "loss:  0.0003-0.0000j, 1.8945951461791992\n",
      "loss:  0.0003-0.0000j, 1.8948845863342285\n",
      "loss:  0.0003-0.0000j, 1.895175814628601\n",
      "loss:  0.0002+0.0000j, 1.8954532146453857\n",
      "loss:  0.0002-0.0000j, 1.8957042694091797\n",
      "loss:  0.0002-0.0000j, 1.8959274291992188\n",
      "loss:  0.0002-0.0000j, 1.896146535873413\n",
      "loss:  0.0002-0.0000j, 1.8963549137115479\n",
      "loss:  0.0002-0.0000j, 1.8965694904327393\n",
      "loss:  0.0002-0.0000j, 1.8967900276184082\n",
      "loss:  0.0002-0.0000j, 1.8970394134521484\n",
      "loss:  0.0002-0.0000j, 1.8972762823104858\n",
      "loss:  0.0002-0.0000j, 1.8975419998168945\n",
      "loss:  0.0002-0.0000j, 1.8978251218795776\n",
      "loss:  0.0002-0.0000j, 1.898108720779419\n",
      "loss:  0.0002-0.0000j, 1.898380994796753\n",
      "loss:  0.0002-0.0000j, 1.8986378908157349\n",
      "loss:  0.0002-0.0000j, 1.8988792896270752\n",
      "loss:  0.0002+0.0000j, 1.8991115093231201\n",
      "loss:  0.0002-0.0000j, 1.8993438482284546\n",
      "loss:  0.0002-0.0000j, 1.8995835781097412\n",
      "loss:  0.0002-0.0000j, 1.8998353481292725\n",
      "loss:  0.0002-0.0000j, 1.9001033306121826\n",
      "loss:  0.0002+0.0000j, 1.9003866910934448\n",
      "loss:  0.0002-0.0000j, 1.9006730318069458\n",
      "loss:  0.0002+0.0000j, 1.9009448289871216\n",
      "loss:  0.0002-0.0000j, 1.9011917114257812\n",
      "loss:  0.0002-0.0000j, 1.9014132022857666\n",
      "loss:  0.0002+0.0000j, 1.901615858078003\n",
      "loss:  0.0002-0.0000j, 1.9018138647079468\n",
      "loss:  0.0002-0.0000j, 1.9020233154296875\n",
      "loss:  0.0002-0.0000j, 1.902256727218628\n",
      "loss:  0.0002-0.0000j, 1.902520775794983\n",
      "loss:  0.0002-0.0000j, 1.9028151035308838\n",
      "loss:  0.0002-0.0000j, 1.9031307697296143\n",
      "loss:  0.0002-0.0000j, 1.9034475088119507\n",
      "loss:  0.0002-0.0000j, 1.9037437438964844\n",
      "loss:  0.0002-0.0000j, 1.904006004333496\n",
      "loss:  0.0002-0.0000j, 1.9042307138442993\n",
      "loss:  0.0002+0.0000j, 1.9044241905212402\n",
      "loss:  0.0002-0.0000j, 1.904601812362671\n",
      "loss:  0.0002-0.0000j, 1.9047825336456299\n",
      "loss:  0.0002+0.0000j, 1.9049830436706543\n",
      "loss:  0.0002-0.0000j, 1.9052140712738037\n",
      "loss:  0.0002-0.0000j, 1.9054768085479736\n",
      "loss:  0.0002-0.0000j, 1.9057610034942627\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8dcbd0e8232e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1735\u001b[0m   \u001b[0mt_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m   \u001b[0mt_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mconj\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   4468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4469\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4470\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Conj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4471\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4472\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6698\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_on_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6699\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        G = A + 1j*B\n",
    "        Q, R = tf.linalg.qr(G, full_matrices = False)\n",
    "        D = tf.linalg.tensor_diag_part(R)\n",
    "        D = tf.math.sign(D)\n",
    "        D = tf.linalg.diag(D)\n",
    "        U = Q@D\n",
    "\n",
    "        kraus_model_list =  [U[i*d:(i+1)*d, :d] for i in range(rank)]\n",
    "        pred_list = [expectation_value(apply_map(input[0], kraus_model_list), input[1]) for input in input_list]\n",
    "        loss = tf.math.reduce_mean(tf.stack([(target - predicted)**2 for target, predicted in zip(target_list, pred_list)]))\n",
    "\n",
    "    grads = tape.gradient(loss, [A, B])\n",
    "    optimizer.apply_gradients(zip(grads, [A, B]))\n",
    "    \n",
    "    print(f\"loss: {loss: .4f}, {tf.math.reduce_mean(tf.math.abs(G))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
