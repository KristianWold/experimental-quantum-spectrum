{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "\n",
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "from qiskit.quantum_info import Operator\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cost_functions import *\n",
    "from optimization import *\n",
    "from quantum_maps import *\n",
    "from quantum_tools import *\n",
    "from src_torch import *\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_map(state, kraus_list):\n",
    "    state = [K@state@K.T.conj() for K in kraus_list]\n",
    "    state = torch.stack(state, dim=0).sum(dim=0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 3\n",
    "d = 2**n\n",
    "rank = 8\n",
    "\n",
    "_, A, B = generate_ginibre(rank*d, d, requires_grad=False)\n",
    "G = A + 1j*B\n",
    "\n",
    "U = generate_unitary(G)\n",
    "kraus_target_list =  [U[i*d:(i+1)*d, :d] for i in range(rank)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "state_index, observ_index = index_generator(n, N, trace=False)\n",
    "\n",
    "input_list = []\n",
    "circuit_list = []\n",
    "for i, j in zip(state_index, observ_index):\n",
    "\n",
    "    config = numberToBase(i, 6, n)\n",
    "    state = prepare_input(config)\n",
    "\n",
    "    config = numberToBase(j, 3, n)\n",
    "    observable = pauli_observable(config)\n",
    "    \n",
    "    input_list.append([state, observable])\n",
    "\n",
    "target_list = [expectation_value(apply_map(input[0], kraus_target_list), input[1]) for input in input_list]\n",
    "\n",
    "_, A, B = generate_ginibre(rank*d, d, requires_grad=True)\n",
    "optimizer = optim.Adam([A, B], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4edf8bfd4c840009d9105d1b9f6b2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0289,  1.2290, 3.5621\n",
      "0.0277,  1.2293, 3.5510\n",
      "0.0267,  1.2296, 3.5402\n",
      "0.0256,  1.2302, 3.5293\n",
      "0.0247,  1.2309, 3.5182\n",
      "0.0237,  1.2318, 3.5068\n",
      "0.0228,  1.2329, 3.4952\n",
      "0.0218,  1.2341, 3.4832\n",
      "0.0210,  1.2356, 3.4710\n",
      "0.0201,  1.2372, 3.4584\n",
      "0.0193,  1.2390, 3.4454\n",
      "0.0185,  1.2411, 3.4321\n",
      "0.0178,  1.2433, 3.4185\n",
      "0.0171,  1.2457, 3.4045\n",
      "0.0164,  1.2483, 3.3902\n",
      "0.0158,  1.2511, 3.3755\n",
      "0.0152,  1.2541, 3.3605\n",
      "0.0147,  1.2574, 3.3451\n",
      "0.0142,  1.2609, 3.3295\n",
      "0.0137,  1.2647, 3.3134\n",
      "0.0133,  1.2687, 3.2971\n",
      "0.0129,  1.2730, 3.2901\n",
      "0.0125,  1.2775, 3.2968\n",
      "0.0121,  1.2823, 3.3068\n",
      "0.0118,  1.2873, 3.3294\n",
      "0.0115,  1.2926, 3.3521\n",
      "0.0113,  1.2981, 3.3750\n",
      "0.0110,  1.3038, 3.3981\n",
      "0.0108,  1.3098, 3.4214\n",
      "0.0106,  1.3161, 3.4449\n",
      "0.0104,  1.3226, 3.4685\n",
      "0.0102,  1.3293, 3.4922\n",
      "0.0101,  1.3362, 3.5161\n",
      "0.0099,  1.3434, 3.5401\n",
      "0.0098,  1.3507, 3.5643\n",
      "0.0096,  1.3583, 3.5885\n",
      "0.0095,  1.3661, 3.6128\n",
      "0.0094,  1.3742, 3.6372\n",
      "0.0092,  1.3824, 3.6617\n",
      "0.0091,  1.3909, 3.6862\n",
      "0.0090,  1.3997, 3.7108\n",
      "0.0089,  1.4086, 3.7354\n",
      "0.0088,  1.4178, 3.7600\n",
      "0.0087,  1.4271, 3.7846\n",
      "0.0086,  1.4367, 3.8092\n",
      "0.0085,  1.4465, 3.8338\n",
      "0.0084,  1.4565, 3.8584\n",
      "0.0083,  1.4666, 3.8830\n",
      "0.0082,  1.4769, 3.9074\n",
      "0.0081,  1.4874, 3.9319\n",
      "0.0080,  1.4980, 3.9562\n",
      "0.0080,  1.5088, 3.9805\n",
      "0.0079,  1.5198, 4.0046\n",
      "0.0078,  1.5309, 4.0287\n",
      "0.0077,  1.5422, 4.0527\n",
      "0.0077,  1.5537, 4.0765\n",
      "0.0076,  1.5653, 4.1002\n",
      "0.0075,  1.5771, 4.1237\n",
      "0.0075,  1.5890, 4.1471\n",
      "0.0074,  1.6011, 4.1704\n",
      "0.0074,  1.6133, 4.1935\n",
      "0.0073,  1.6257, 4.2164\n",
      "0.0073,  1.6382, 4.2392\n",
      "0.0072,  1.6509, 4.2618\n",
      "0.0072,  1.6637, 4.2842\n",
      "0.0072,  1.6767, 4.3064\n",
      "0.0071,  1.6898, 4.3284\n",
      "0.0071,  1.7031, 4.3503\n",
      "0.0071,  1.7165, 4.3720\n",
      "0.0070,  1.7300, 4.3935\n",
      "0.0070,  1.7437, 4.4148\n",
      "0.0070,  1.7575, 4.4359\n",
      "0.0070,  1.7714, 4.4568\n",
      "0.0069,  1.7854, 4.4776\n",
      "0.0069,  1.7995, 4.4982\n",
      "0.0069,  1.8138, 4.5186\n",
      "0.0069,  1.8281, 4.5388\n",
      "0.0068,  1.8427, 4.5589\n",
      "0.0068,  1.8573, 4.5788\n",
      "0.0068,  1.8721, 4.5985\n",
      "0.0068,  1.8869, 4.6181\n",
      "0.0068,  1.9018, 4.6375\n",
      "0.0067,  1.9168, 4.6567\n",
      "0.0067,  1.9319, 4.6759\n",
      "0.0067,  1.9471, 4.6949\n",
      "0.0067,  1.9623, 4.7137\n",
      "0.0067,  1.9775, 4.7324\n",
      "0.0066,  1.9928, 4.7510\n",
      "0.0066,  2.0082, 4.7695\n",
      "0.0066,  2.0236, 4.7879\n",
      "0.0066,  2.0390, 4.8061\n",
      "0.0065,  2.0545, 4.8243\n",
      "0.0065,  2.0700, 4.8423\n",
      "0.0065,  2.0856, 4.8602\n",
      "0.0065,  2.1012, 4.8781\n",
      "0.0064,  2.1169, 4.8959\n",
      "0.0064,  2.1326, 4.9136\n",
      "0.0064,  2.1484, 4.9312\n",
      "0.0064,  2.1642, 4.9487\n",
      "0.0063,  2.1800, 4.9662\n",
      "0.0063,  2.1959, 4.9836\n",
      "0.0063,  2.2118, 5.0009\n",
      "0.0062,  2.2278, 5.0181\n",
      "0.0062,  2.2438, 5.0353\n",
      "0.0062,  2.2599, 5.0525\n",
      "0.0062,  2.2760, 5.0696\n",
      "0.0061,  2.2922, 5.0866\n",
      "0.0061,  2.3084, 5.1036\n",
      "0.0061,  2.3246, 5.1205\n",
      "0.0060,  2.3409, 5.1374\n",
      "0.0060,  2.3572, 5.1543\n",
      "0.0060,  2.3735, 5.1711\n",
      "0.0060,  2.3899, 5.1878\n",
      "0.0059,  2.4063, 5.2045\n",
      "0.0059,  2.4227, 5.2212\n",
      "0.0059,  2.4392, 5.2378\n",
      "0.0058,  2.4557, 5.2544\n",
      "0.0058,  2.4722, 5.2709\n",
      "0.0058,  2.4887, 5.2875\n",
      "0.0058,  2.5052, 5.3039\n",
      "0.0057,  2.5218, 5.3203\n",
      "0.0057,  2.5384, 5.3367\n",
      "0.0057,  2.5549, 5.3531\n",
      "0.0057,  2.5715, 5.3694\n",
      "0.0056,  2.5881, 5.3856\n",
      "0.0056,  2.6048, 5.4019\n",
      "0.0056,  2.6214, 5.4180\n",
      "0.0056,  2.6380, 5.4342\n",
      "0.0056,  2.6547, 5.4503\n",
      "0.0055,  2.6714, 5.4664\n",
      "0.0055,  2.6880, 5.4824\n",
      "0.0055,  2.7047, 5.5081\n",
      "0.0055,  2.7214, 5.5372\n",
      "0.0054,  2.7382, 5.5664\n",
      "0.0054,  2.7550, 5.5955\n",
      "0.0054,  2.7717, 5.6247\n",
      "0.0054,  2.7886, 5.6538\n",
      "0.0054,  2.8054, 5.6829\n",
      "0.0053,  2.8222, 5.7120\n",
      "0.0053,  2.8390, 5.7412\n",
      "0.0053,  2.8559, 5.7703\n",
      "0.0053,  2.8728, 5.7994\n",
      "0.0053,  2.8896, 5.8285\n",
      "0.0052,  2.9065, 5.8577\n",
      "0.0052,  2.9234, 5.8868\n",
      "0.0052,  2.9403, 5.9159\n",
      "0.0052,  2.9572, 5.9450\n",
      "0.0052,  2.9742, 5.9741\n",
      "0.0051,  2.9911, 6.0032\n",
      "0.0051,  3.0080, 6.0322\n",
      "0.0051,  3.0250, 6.0613\n",
      "0.0051,  3.0420, 6.0904\n",
      "0.0051,  3.0590, 6.1195\n",
      "0.0050,  3.0760, 6.1485\n",
      "0.0050,  3.0930, 6.1776\n",
      "0.0050,  3.1100, 6.2066\n",
      "0.0050,  3.1271, 6.2356\n",
      "0.0050,  3.1441, 6.2646\n",
      "0.0050,  3.1612, 6.2936\n",
      "0.0049,  3.1783, 6.3226\n",
      "0.0049,  3.1954, 6.3516\n",
      "0.0049,  3.2125, 6.3806\n",
      "0.0049,  3.2297, 6.4095\n",
      "0.0049,  3.2468, 6.4385\n",
      "0.0048,  3.2639, 6.4674\n",
      "0.0048,  3.2811, 6.4963\n",
      "0.0048,  3.2983, 6.5252\n",
      "0.0048,  3.3155, 6.5541\n",
      "0.0048,  3.3326, 6.5829\n",
      "0.0048,  3.3498, 6.6118\n",
      "0.0047,  3.3671, 6.6406\n",
      "0.0047,  3.3843, 6.6694\n",
      "0.0047,  3.4015, 6.6982\n",
      "0.0047,  3.4187, 6.7269\n",
      "0.0047,  3.4360, 6.7557\n",
      "0.0047,  3.4532, 6.7844\n",
      "0.0046,  3.4705, 6.8131\n",
      "0.0046,  3.4878, 6.8418\n",
      "0.0046,  3.5050, 6.8704\n",
      "0.0046,  3.5223, 6.8991\n",
      "0.0046,  3.5396, 6.9277\n",
      "0.0046,  3.5569, 6.9562\n",
      "0.0045,  3.5742, 6.9848\n",
      "0.0045,  3.5916, 7.0133\n",
      "0.0045,  3.6089, 7.0418\n",
      "0.0045,  3.6262, 7.0703\n",
      "0.0045,  3.6436, 7.0988\n",
      "0.0045,  3.6609, 7.1272\n",
      "0.0044,  3.6783, 7.1556\n",
      "0.0044,  3.6956, 7.1840\n",
      "0.0044,  3.7130, 7.2123\n",
      "0.0044,  3.7304, 7.2407\n",
      "0.0044,  3.7478, 7.2690\n",
      "0.0044,  3.7651, 7.2972\n",
      "0.0044,  3.7825, 7.3255\n",
      "0.0043,  3.7999, 7.3537\n",
      "0.0043,  3.8174, 7.3819\n",
      "0.0043,  3.8348, 7.4100\n",
      "0.0043,  3.8522, 7.4382\n",
      "0.0043,  3.8696, 7.4663\n",
      "0.0043,  3.8870, 7.4944\n",
      "0.0043,  3.9045, 7.5224\n",
      "0.0042,  3.9219, 7.5504\n",
      "0.0042,  3.9394, 7.5784\n",
      "0.0042,  3.9568, 7.6064\n",
      "0.0042,  3.9743, 7.6343\n",
      "0.0042,  3.9917, 7.6622\n",
      "0.0042,  4.0092, 7.6901\n",
      "0.0041,  4.0266, 7.7179\n",
      "0.0041,  4.0441, 7.7457\n",
      "0.0041,  4.0616, 7.7735\n",
      "0.0041,  4.0791, 7.8013\n",
      "0.0041,  4.0965, 7.8290\n",
      "0.0041,  4.1140, 7.8568\n",
      "0.0041,  4.1315, 7.8844\n",
      "0.0040,  4.1489, 7.9121\n",
      "0.0040,  4.1664, 7.9397\n",
      "0.0040,  4.1839, 7.9673\n",
      "0.0040,  4.2014, 7.9949\n",
      "0.0040,  4.2189, 8.0224\n",
      "0.0040,  4.2363, 8.0500\n",
      "0.0040,  4.2538, 8.0775\n",
      "0.0040,  4.2713, 8.1049\n",
      "0.0039,  4.2888, 8.1324\n",
      "0.0039,  4.3062, 8.1598\n",
      "0.0039,  4.3237, 8.1872\n",
      "0.0039,  4.3412, 8.2145\n",
      "0.0039,  4.3587, 8.2419\n",
      "0.0039,  4.3762, 8.2692\n",
      "0.0039,  4.3936, 8.2965\n",
      "0.0038,  4.4111, 8.3238\n",
      "0.0038,  4.4286, 8.3510\n",
      "0.0038,  4.4461, 8.3782\n",
      "0.0038,  4.4635, 8.4054\n",
      "0.0038,  4.4810, 8.4326\n",
      "0.0038,  4.4985, 8.4598\n",
      "0.0038,  4.5160, 8.4869\n",
      "0.0038,  4.5335, 8.5140\n",
      "0.0037,  4.5509, 8.5411\n",
      "0.0037,  4.5684, 8.5682\n",
      "0.0037,  4.5859, 8.5953\n",
      "0.0037,  4.6034, 8.6223\n",
      "0.0037,  4.6209, 8.6493\n",
      "0.0037,  4.6384, 8.6763\n",
      "0.0037,  4.6559, 8.7033\n",
      "0.0037,  4.6734, 8.7302\n",
      "0.0036,  4.6909, 8.7572\n",
      "0.0036,  4.7085, 8.7841\n",
      "0.0036,  4.7260, 8.8110\n",
      "0.0036,  4.7435, 8.8379\n",
      "0.0036,  4.7611, 8.8647\n",
      "0.0036,  4.7787, 8.8916\n",
      "0.0036,  4.7962, 8.9184\n",
      "0.0036,  4.8138, 8.9453\n",
      "0.0036,  4.8314, 8.9721\n",
      "0.0035,  4.8490, 8.9989\n",
      "0.0035,  4.8666, 9.0256\n",
      "0.0035,  4.8842, 9.0524\n",
      "0.0035,  4.9018, 9.0791\n",
      "0.0035,  4.9195, 9.1059\n",
      "0.0035,  4.9371, 9.1344\n",
      "0.0035,  4.9547, 9.1663\n",
      "0.0035,  4.9724, 9.1983\n",
      "0.0035,  4.9900, 9.2302\n",
      "0.0035,  5.0077, 9.2621\n",
      "0.0034,  5.0253, 9.2941\n",
      "0.0034,  5.0430, 9.3259\n",
      "0.0034,  5.0607, 9.3578\n",
      "0.0034,  5.0783, 9.3897\n",
      "0.0034,  5.0960, 9.4215\n",
      "0.0034,  5.1137, 9.4534\n",
      "0.0034,  5.1314, 9.4852\n",
      "0.0034,  5.1490, 9.5170\n",
      "0.0034,  5.1667, 9.5488\n",
      "0.0034,  5.1844, 9.5805\n",
      "0.0034,  5.2021, 9.6123\n",
      "0.0033,  5.2198, 9.6440\n",
      "0.0033,  5.2375, 9.6758\n",
      "0.0033,  5.2552, 9.7075\n",
      "0.0033,  5.2729, 9.7391\n",
      "0.0033,  5.2906, 9.7708\n",
      "0.0033,  5.3083, 9.8025\n",
      "0.0033,  5.3260, 9.8341\n",
      "0.0033,  5.3438, 9.8657\n",
      "0.0033,  5.3615, 9.8973\n",
      "0.0033,  5.3792, 9.9289\n",
      "0.0033,  5.3969, 9.9604\n",
      "0.0033,  5.4146, 9.9920\n",
      "0.0033,  5.4324, 10.0235\n",
      "0.0032,  5.4501, 10.0550\n",
      "0.0032,  5.4678, 10.0865\n",
      "0.0032,  5.4856, 10.1180\n",
      "0.0032,  5.5033, 10.1494\n",
      "0.0032,  5.5210, 10.1808\n",
      "0.0032,  5.5388, 10.2122\n",
      "0.0032,  5.5565, 10.2436\n",
      "0.0032,  5.5742, 10.2750\n",
      "0.0032,  5.5920, 10.3063\n",
      "0.0032,  5.6097, 10.3376\n",
      "0.0032,  5.6275, 10.3689\n",
      "0.0032,  5.6452, 10.4002\n",
      "0.0032,  5.6630, 10.4315\n",
      "0.0032,  5.6807, 10.4627\n",
      "0.0032,  5.6985, 10.4939\n",
      "0.0032,  5.7163, 10.5251\n",
      "0.0031,  5.7340, 10.5563\n",
      "0.0031,  5.7518, 10.5874\n",
      "0.0031,  5.7695, 10.6186\n",
      "0.0031,  5.7873, 10.6497\n",
      "0.0031,  5.8051, 10.6808\n",
      "0.0031,  5.8228, 10.7118\n",
      "0.0031,  5.8406, 10.7428\n",
      "0.0031,  5.8584, 10.7739\n",
      "0.0031,  5.8762, 10.8049\n",
      "0.0031,  5.8940, 10.8358\n",
      "0.0031,  5.9117, 10.8668\n",
      "0.0031,  5.9295, 10.8977\n",
      "0.0031,  5.9473, 10.9286\n",
      "0.0031,  5.9651, 10.9594\n",
      "0.0031,  5.9829, 10.9903\n",
      "0.0031,  6.0007, 11.0211\n",
      "0.0031,  6.0185, 11.0519\n",
      "0.0031,  6.0363, 11.0827\n",
      "0.0030,  6.0542, 11.1134\n",
      "0.0030,  6.0720, 11.1442\n",
      "0.0030,  6.0898, 11.1749\n",
      "0.0030,  6.1076, 11.2056\n",
      "0.0030,  6.1255, 11.2362\n",
      "0.0030,  6.1433, 11.2668\n",
      "0.0030,  6.1612, 11.2974\n",
      "0.0030,  6.1790, 11.3280\n",
      "0.0030,  6.1969, 11.3586\n",
      "0.0030,  6.2147, 11.3891\n",
      "0.0030,  6.2326, 11.4196\n",
      "0.0030,  6.2505, 11.4501\n",
      "0.0030,  6.2683, 11.4805\n",
      "0.0030,  6.2862, 11.5109\n",
      "0.0030,  6.3040, 11.5413\n",
      "0.0030,  6.3219, 11.5717\n",
      "0.0030,  6.3398, 11.6021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030,  6.3577, 11.6324\n",
      "0.0030,  6.3755, 11.6627\n",
      "0.0030,  6.3934, 11.6930\n",
      "0.0030,  6.4113, 11.7232\n",
      "0.0029,  6.4292, 11.7534\n",
      "0.0029,  6.4470, 11.7836\n",
      "0.0029,  6.4649, 11.8138\n",
      "0.0029,  6.4828, 11.8439\n",
      "0.0029,  6.5007, 11.8741\n",
      "0.0029,  6.5185, 11.9041\n",
      "0.0029,  6.5364, 11.9342\n",
      "0.0029,  6.5543, 11.9642\n",
      "0.0029,  6.5722, 11.9943\n",
      "0.0029,  6.5901, 12.0242\n",
      "0.0029,  6.6079, 12.0542\n",
      "0.0029,  6.6258, 12.0841\n",
      "0.0029,  6.6437, 12.1140\n",
      "0.0029,  6.6616, 12.1439\n",
      "0.0029,  6.6795, 12.1738\n",
      "0.0029,  6.6974, 12.2036\n",
      "0.0029,  6.7153, 12.2334\n",
      "0.0029,  6.7332, 12.2632\n",
      "0.0029,  6.7511, 12.2929\n",
      "0.0029,  6.7690, 12.3227\n",
      "0.0029,  6.7869, 12.3524\n",
      "0.0028,  6.8048, 12.3820\n",
      "0.0028,  6.8227, 12.4117\n",
      "0.0028,  6.8406, 12.4413\n",
      "0.0028,  6.8586, 12.4709\n",
      "0.0028,  6.8765, 12.5005\n",
      "0.0028,  6.8944, 12.5300\n",
      "0.0028,  6.9124, 12.5595\n",
      "0.0028,  6.9303, 12.5890\n",
      "0.0028,  6.9482, 12.6185\n",
      "0.0028,  6.9662, 12.6479\n",
      "0.0028,  6.9841, 12.6773\n",
      "0.0028,  7.0021, 12.7067\n",
      "0.0028,  7.0200, 12.7361\n",
      "0.0028,  7.0380, 12.7654\n",
      "0.0028,  7.0559, 12.7947\n",
      "0.0028,  7.0739, 12.8240\n",
      "0.0028,  7.0918, 12.8533\n",
      "0.0028,  7.1098, 12.8825\n",
      "0.0028,  7.1277, 12.9117\n",
      "0.0028,  7.1457, 12.9409\n",
      "0.0028,  7.1636, 12.9700\n",
      "0.0028,  7.1816, 12.9992\n",
      "0.0027,  7.1995, 13.0283\n",
      "0.0027,  7.2175, 13.0574\n",
      "0.0027,  7.2354, 13.0864\n",
      "0.0027,  7.2534, 13.1154\n",
      "0.0027,  7.2714, 13.1445\n",
      "0.0027,  7.2893, 13.1734\n",
      "0.0027,  7.3073, 13.2024\n",
      "0.0027,  7.3253, 13.2313\n",
      "0.0027,  7.3432, 13.2602\n",
      "0.0027,  7.3612, 13.2891\n",
      "0.0027,  7.3792, 13.3180\n",
      "0.0027,  7.3971, 13.3468\n",
      "0.0027,  7.4151, 13.3756\n",
      "0.0027,  7.4331, 13.4044\n",
      "0.0027,  7.4511, 13.4331\n",
      "0.0027,  7.4691, 13.4619\n",
      "0.0027,  7.4871, 13.4906\n",
      "0.0027,  7.5050, 13.5193\n",
      "0.0027,  7.5230, 13.5479\n",
      "0.0027,  7.5410, 13.5766\n",
      "0.0027,  7.5590, 13.6052\n",
      "0.0026,  7.5770, 13.6338\n",
      "0.0026,  7.5950, 13.6624\n",
      "0.0026,  7.6131, 13.6909\n",
      "0.0026,  7.6311, 13.7194\n",
      "0.0026,  7.6491, 13.7479\n",
      "0.0026,  7.6671, 13.7764\n",
      "0.0026,  7.6851, 13.8048\n",
      "0.0026,  7.7032, 13.8333\n",
      "0.0026,  7.7212, 13.8617\n",
      "0.0026,  7.7392, 13.8900\n",
      "0.0026,  7.7573, 13.9184\n",
      "0.0026,  7.7753, 13.9467\n",
      "0.0026,  7.7934, 13.9750\n",
      "0.0026,  7.8114, 14.0033\n",
      "0.0026,  7.8295, 14.0316\n",
      "0.0026,  7.8476, 14.0598\n",
      "0.0026,  7.8656, 14.0881\n",
      "0.0026,  7.8837, 14.1163\n",
      "0.0026,  7.9018, 14.1444\n",
      "0.0026,  7.9199, 14.1726\n",
      "0.0026,  7.9380, 14.2007\n",
      "0.0026,  7.9560, 14.2288\n",
      "0.0026,  7.9741, 14.2569\n",
      "0.0025,  7.9922, 14.2850\n",
      "0.0025,  8.0103, 14.3130\n",
      "0.0025,  8.0284, 14.3411\n",
      "0.0025,  8.0465, 14.3691\n",
      "0.0025,  8.0646, 14.3970\n",
      "0.0025,  8.0827, 14.4250\n",
      "0.0025,  8.1008, 14.4529\n",
      "0.0025,  8.1190, 14.4808\n",
      "0.0025,  8.1371, 14.5087\n",
      "0.0025,  8.1552, 14.5366\n",
      "0.0025,  8.1733, 14.5645\n",
      "0.0025,  8.1914, 14.5923\n",
      "0.0025,  8.2095, 14.6201\n",
      "0.0025,  8.2277, 14.6479\n",
      "0.0025,  8.2458, 14.6757\n",
      "0.0025,  8.2639, 14.7034\n",
      "0.0025,  8.2820, 14.7312\n",
      "0.0025,  8.3002, 14.7589\n",
      "0.0025,  8.3183, 14.7866\n",
      "0.0025,  8.3364, 14.8142\n",
      "0.0025,  8.3545, 14.8419\n",
      "0.0025,  8.3727, 14.8695\n",
      "0.0025,  8.3908, 14.8971\n",
      "0.0024,  8.4089, 14.9247\n",
      "0.0024,  8.4271, 14.9523\n",
      "0.0024,  8.4452, 14.9798\n",
      "0.0024,  8.4633, 15.0073\n",
      "0.0024,  8.4814, 15.0349\n",
      "0.0024,  8.4996, 15.0623\n",
      "0.0024,  8.5177, 15.0898\n",
      "0.0024,  8.5358, 15.1173\n",
      "0.0024,  8.5540, 15.1447\n",
      "0.0024,  8.5721, 15.1721\n",
      "0.0024,  8.5902, 15.1995\n",
      "0.0024,  8.6084, 15.2269\n",
      "0.0024,  8.6265, 15.2543\n",
      "0.0024,  8.6446, 15.2816\n",
      "0.0024,  8.6627, 15.3089\n",
      "0.0024,  8.6809, 15.3362\n",
      "0.0024,  8.6990, 15.3635\n",
      "0.0024,  8.7171, 15.3908\n",
      "0.0024,  8.7352, 15.4180\n",
      "0.0024,  8.7534, 15.4453\n",
      "0.0024,  8.7715, 15.4725\n",
      "0.0024,  8.7896, 15.4997\n",
      "0.0024,  8.8077, 15.5268\n",
      "0.0024,  8.8258, 15.5540\n",
      "0.0024,  8.8440, 15.5811\n",
      "0.0024,  8.8621, 15.6083\n",
      "0.0023,  8.8802, 15.6354\n",
      "0.0023,  8.8983, 15.6625\n",
      "0.0023,  8.9164, 15.6895\n",
      "0.0023,  8.9345, 15.7166\n",
      "0.0023,  8.9526, 15.7436\n",
      "0.0023,  8.9707, 15.7706\n",
      "0.0023,  8.9888, 15.7976\n",
      "0.0023,  9.0069, 15.8246\n",
      "0.0023,  9.0250, 15.8516\n",
      "0.0023,  9.0431, 15.8786\n",
      "0.0023,  9.0612, 15.9055\n",
      "0.0023,  9.0793, 15.9324\n",
      "0.0023,  9.0974, 15.9593\n",
      "0.0023,  9.1155, 15.9862\n",
      "0.0023,  9.1336, 16.0131\n",
      "0.0023,  9.1517, 16.0399\n",
      "0.0023,  9.1698, 16.0668\n",
      "0.0023,  9.1878, 16.0936\n",
      "0.0023,  9.2059, 16.1204\n",
      "0.0023,  9.2240, 16.1472\n",
      "0.0023,  9.2421, 16.1740\n",
      "0.0023,  9.2602, 16.2007\n",
      "0.0023,  9.2782, 16.2275\n",
      "0.0023,  9.2963, 16.2542\n",
      "0.0023,  9.3144, 16.2809\n",
      "0.0023,  9.3325, 16.3076\n",
      "0.0023,  9.3505, 16.3343\n",
      "0.0023,  9.3686, 16.3609\n",
      "0.0023,  9.3867, 16.3876\n",
      "0.0022,  9.4047, 16.4142\n",
      "0.0022,  9.4228, 16.4408\n",
      "0.0022,  9.4409, 16.4675\n",
      "0.0022,  9.4589, 16.4940\n",
      "0.0022,  9.4770, 16.5206\n",
      "0.0022,  9.4951, 16.5472\n",
      "0.0022,  9.5131, 16.5737\n",
      "0.0022,  9.5312, 16.6003\n",
      "0.0022,  9.5492, 16.6268\n",
      "0.0022,  9.5673, 16.6533\n",
      "0.0022,  9.5853, 16.6798\n",
      "0.0022,  9.6034, 16.7062\n",
      "0.0022,  9.6214, 16.7327\n",
      "0.0022,  9.6395, 16.7591\n",
      "0.0022,  9.6575, 16.7856\n",
      "0.0022,  9.6755, 16.8120\n",
      "0.0022,  9.6936, 16.8384\n",
      "0.0022,  9.7116, 16.8648\n",
      "0.0022,  9.7296, 16.8911\n",
      "0.0022,  9.7476, 16.9175\n",
      "0.0022,  9.7657, 16.9438\n",
      "0.0022,  9.7837, 16.9702\n",
      "0.0022,  9.8017, 16.9965\n",
      "0.0022,  9.8197, 17.0228\n",
      "0.0022,  9.8377, 17.0491\n",
      "0.0022,  9.8557, 17.0754\n",
      "0.0022,  9.8737, 17.1016\n",
      "0.0022,  9.8917, 17.1279\n",
      "0.0022,  9.9097, 17.1541\n",
      "0.0022,  9.9277, 17.1803\n",
      "0.0022,  9.9456, 17.2065\n",
      "0.0022,  9.9636, 17.2327\n",
      "0.0022,  9.9816, 17.2589\n",
      "0.0022,  9.9996, 17.2851\n",
      "0.0022,  10.0175, 17.3113\n",
      "0.0022,  10.0355, 17.3374\n",
      "0.0022,  10.0535, 17.3635\n",
      "0.0022,  10.0714, 17.3897\n",
      "0.0021,  10.0894, 17.4158\n",
      "0.0021,  10.1073, 17.4419\n",
      "0.0021,  10.1253, 17.4679\n",
      "0.0021,  10.1432, 17.4940\n",
      "0.0021,  10.1611, 17.5201\n",
      "0.0021,  10.1791, 17.5461\n",
      "0.0021,  10.1970, 17.5721\n",
      "0.0021,  10.2149, 17.5982\n",
      "0.0021,  10.2329, 17.6242\n",
      "0.0021,  10.2508, 17.6502\n",
      "0.0021,  10.2687, 17.6761\n",
      "0.0021,  10.2866, 17.7021\n",
      "0.0021,  10.3045, 17.7281\n",
      "0.0021,  10.3224, 17.7540\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fe3d37bff73b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpred_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpectation_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkraus_model_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{np.abs(loss.detach().numpy()):.4f},  {np.mean(np.abs(G.detach().numpy())):.4f}, {np.max(np.abs(G.detach().numpy())):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_qiskit/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    G = A + 1j*B\n",
    "    U = generate_unitary(G)\n",
    "    kraus_model_list =  [U[i*d:(i+1)*d, :d] for i in range(rank)]\n",
    "    pred_list = [expectation_value(apply_map(input[0], kraus_model_list), input[1]) for input in input_list]\n",
    "    loss = torch.mean(torch.stack([(target - predicted)**2 for target, predicted in zip(target_list, pred_list)]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"{np.abs(loss.detach().numpy()):.4f},  {np.mean(np.abs(G.detach().numpy())):.4f}, {np.max(np.abs(G.detach().numpy())):.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4967+0.j, -0.1383+0.j,  0.6477+0.j,  1.5230+0.j, -0.2342+0.j, -0.2341+0.j,\n",
      "          1.5792+0.j,  0.7674+0.j],\n",
      "        [-0.4695+0.j,  0.5426+0.j, -0.4634+0.j, -0.4657+0.j,  0.2420+0.j, -1.9133+0.j,\n",
      "         -1.7249+0.j, -0.5623+0.j],\n",
      "        [-1.0128+0.j,  0.3142+0.j, -0.9080+0.j, -1.4123+0.j,  1.4656+0.j, -0.2258+0.j,\n",
      "          0.0675+0.j, -1.4247+0.j],\n",
      "        [-0.5444+0.j,  0.1109+0.j, -1.1510+0.j,  0.3757+0.j, -0.6006+0.j, -0.2917+0.j,\n",
      "         -0.6017+0.j,  1.8523+0.j],\n",
      "        [-0.0135+0.j, -1.0577+0.j,  0.8225+0.j, -1.2208+0.j,  0.2089+0.j, -1.9597+0.j,\n",
      "         -1.3282+0.j,  0.1969+0.j],\n",
      "        [ 0.7385+0.j,  0.1714+0.j, -0.1156+0.j, -0.3011+0.j, -1.4785+0.j, -0.7198+0.j,\n",
      "         -0.4606+0.j,  1.0571+0.j],\n",
      "        [ 0.3436+0.j, -1.7630+0.j,  0.3241+0.j, -0.3851+0.j, -0.6769+0.j,  0.6117+0.j,\n",
      "          1.0310+0.j,  0.9313+0.j],\n",
      "        [-0.8392+0.j, -0.3092+0.j,  0.3313+0.j,  0.9755+0.j, -0.4792+0.j, -0.1857+0.j,\n",
      "         -1.1063+0.j, -1.1962+0.j]], dtype=torch.complex128,\n",
      "       requires_grad=True)\n",
      "tensor(1.4930e-15, dtype=torch.float64, grad_fn=<CopyBackwards>)\n",
      "tensor(4.8016, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 3\n",
    "d = 2**n\n",
    "rank = 1\n",
    "\n",
    "_, A, B = generate_ginibre(rank*d, d, requires_grad=True)\n",
    "G = A\n",
    "C = torch.rand(rank*d, d).type(torch.complex128)\n",
    "R = torch.rand(rank*d, rank*d).type(torch.complex128)\n",
    "R, _ = torch.linalg.qr(R)\n",
    "print(torch.linalg.cond(A))\n",
    "\n",
    "\n",
    "U1 = generate_unitary(G)\n",
    "out1 = torch.autograd.grad(U1, G, C)[0]\n",
    "G = A\n",
    "G = R@G\n",
    "\n",
    "U2 = R.T.conj()@generate_unitary(G)\n",
    "out2 = torch.autograd.grad(U2, G, C)[0]\n",
    "\n",
    "\n",
    "print(torch.norm(U1-U2))\n",
    "print(torch.norm(out1-out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-42b6cdf3b880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(3, 3)\n",
    "dx = np.random.randn(3, 3)\n",
    "\n",
    "primals, tangents = jax.jvp(jnp.linalg.qr, (x,), (dx,))\n",
    "q, r = primals\n",
    "dq, dr = tangents\n",
    "\n",
    "dt = 1e-6\n",
    "dq_ = (np.linalg.qr(x + dt * dx)[0] - np.linalg.qr(x)[0]) / dt\n",
    "dr_ = (np.linalg.qr(x + dt * dx)[1] - np.linalg.qr(x)[1]) / dt\n",
    "\n",
    "assert jnp.allclose(x, q @ r, atol=1e-5, rtol=1e-5)  # passes\n",
    "assert jnp.allclose(dq, dq_, atol=1e-5, rtol=1e-5)  # passes\n",
    "assert jnp.allclose(dx, q @ dr_ + dq_ @ r, atol=1e-5, rtol=1e-5)  # passes\n",
    "assert jnp.allclose(dr, dr_, atol=1e-5, rtol=1e-5)  # fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
