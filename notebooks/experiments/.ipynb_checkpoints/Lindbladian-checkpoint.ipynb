{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lindbladian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src_tf/')\n",
    "\n",
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from qiskit.quantum_info import DensityMatrix, random_unitary\n",
    "from qiskit.quantum_info import Operator\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from loss_functions import *\n",
    "from optimization import *\n",
    "from quantum_channel import *\n",
    "from quantum_tools import *\n",
    "from experimental import *\n",
    "from spam import *\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "n = 2\n",
    "d = 2**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelQuantumMap(\n",
    "                       # channel = LindbladMap(d, rank=d**2),\n",
    "                        channel = KrausMap(d, rank=d**2),\n",
    "                        loss_function = Conj3(index = 1, sign =-1),\n",
    "                        optimizer = tf.optimizers.Adam(learning_rate=0.005),\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7783a14f5ceb4ed5b58a63e16a10f20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: -0.0001418699213523943\n",
      "Step:1, train: -0.0002077159466784163\n",
      "Step:2, train: -0.00022946448184104647\n",
      "Step:3, train: -0.0002419461213667737\n",
      "Step:4, train: -0.00027960750766922916\n",
      "Step:5, train: -0.00033719193123189247\n",
      "Step:6, train: -0.0004122398173001311\n",
      "Step:7, train: -0.0004964359189398318\n",
      "Step:8, train: -0.000534773728923086\n",
      "Step:9, train: -0.0005707515274779834\n",
      "Step:10, train: -0.0006388650374267328\n",
      "Step:11, train: -0.0007069936235092311\n",
      "Step:12, train: -0.0007091592662494515\n",
      "Step:13, train: -0.0007455670309957648\n",
      "Step:14, train: -0.0007797938391596686\n",
      "Step:15, train: -0.000810824109518607\n",
      "Step:16, train: -0.0008619416872459445\n",
      "Step:17, train: -0.0009297996405620402\n",
      "Step:18, train: -0.0009586206955581254\n",
      "Step:19, train: -0.0009965588603652704\n",
      "Step:20, train: -0.0010139270697791134\n",
      "Step:21, train: -0.0010517809594427475\n",
      "Step:22, train: -0.0010770183639882353\n",
      "Step:23, train: -0.0011008859963854804\n",
      "Step:24, train: -0.0011282249813351687\n",
      "Step:25, train: -0.001176932277799206\n",
      "Step:26, train: -0.001200458986769809\n",
      "Step:27, train: -0.0012535225235357087\n",
      "Step:28, train: -0.001291080701437264\n",
      "Step:29, train: -0.0013367238680149214\n",
      "Step:30, train: -0.0013688021023975947\n",
      "Step:31, train: -0.0014271327128102598\n",
      "Step:32, train: -0.0014584462536531247\n",
      "Step:33, train: -0.0014825604153410035\n",
      "Step:34, train: -0.0015145478469271397\n",
      "Step:35, train: -0.0015698569001940295\n",
      "Step:36, train: -0.001592041686436136\n",
      "Step:37, train: -0.0016457551572760913\n",
      "Step:38, train: -0.0016919638147430758\n",
      "Step:39, train: -0.001726829974500416\n",
      "Step:40, train: -0.0017500233441275024\n",
      "Step:41, train: -0.00179968529317905\n",
      "Step:42, train: -0.00187137338323628\n",
      "Step:43, train: -0.0018963577111319263\n",
      "Step:44, train: -0.001958918717769078\n",
      "Step:45, train: -0.0019933076790089833\n",
      "Step:46, train: -0.0020416766200220928\n",
      "Step:47, train: -0.0020588711078735637\n",
      "Step:48, train: -0.002099382319618583\n",
      "Step:49, train: -0.002148955110854485\n",
      "Step:50, train: -0.0021921227537959513\n",
      "Step:51, train: -0.002245324497617138\n",
      "Step:52, train: -0.002264556598448367\n",
      "Step:53, train: -0.002298260345844148\n",
      "Step:54, train: -0.0023614885599449486\n",
      "Step:55, train: -0.002427261474745949\n",
      "Step:56, train: -0.0024603162082799854\n",
      "Step:57, train: -0.0024928307690612887\n",
      "Step:58, train: -0.0025405940403306604\n",
      "Step:59, train: -0.0025974797911281033\n",
      "Step:60, train: -0.002636708430600606\n",
      "Step:61, train: -0.002694233662390167\n",
      "Step:62, train: -0.00276394530308653\n",
      "Step:63, train: -0.0028092951078520093\n",
      "Step:64, train: -0.0028830023621935746\n",
      "Step:65, train: -0.002907344278850407\n",
      "Step:66, train: -0.002983359070716991\n",
      "Step:67, train: -0.003055083621114199\n",
      "Step:68, train: -0.0030742597600927936\n",
      "Step:69, train: -0.003134908210599821\n",
      "Step:70, train: -0.00313945744355249\n",
      "Step:71, train: -0.0032063914389513296\n",
      "Step:72, train: -0.0032899540955748465\n",
      "Step:73, train: -0.0033167357751979604\n",
      "Step:74, train: -0.003396102275331444\n",
      "Step:75, train: -0.003460436345549546\n",
      "Step:76, train: -0.003538651738330655\n",
      "Step:77, train: -0.0036364647051594524\n",
      "Step:78, train: -0.003700062073985042\n",
      "Step:79, train: -0.0037184796884625997\n",
      "Step:80, train: -0.0037789516027758423\n",
      "Step:81, train: -0.0038731296154024203\n",
      "Step:82, train: -0.003946462726149897\n",
      "Step:83, train: -0.0039900592520111175\n",
      "Step:84, train: -0.004088648812196477\n",
      "Step:85, train: -0.004165097046323329\n",
      "Step:86, train: -0.004216702088797415\n",
      "Step:87, train: -0.004294932084269407\n",
      "Step:88, train: -0.004344863919591297\n",
      "Step:89, train: -0.0044025702069122995\n",
      "Step:90, train: -0.00451043491501542\n",
      "Step:91, train: -0.004571163836780305\n",
      "Step:92, train: -0.004631104139293687\n",
      "Step:93, train: -0.004740615016786106\n",
      "Step:94, train: -0.004844733962185196\n",
      "Step:95, train: -0.004923877231144088\n",
      "Step:96, train: -0.004981282035172674\n",
      "Step:97, train: -0.00507855017306234\n",
      "Step:98, train: -0.005149638113044225\n",
      "Step:99, train: -0.0051954259914864485\n",
      "Step:100, train: -0.005319056835806454\n",
      "Step:101, train: -0.005409890271258896\n",
      "Step:102, train: -0.005500319852732862\n",
      "Step:103, train: -0.005603604406599859\n",
      "Step:104, train: -0.005693484766028796\n",
      "Step:105, train: -0.005754782371642226\n",
      "Step:106, train: -0.005843753424728983\n",
      "Step:107, train: -0.005928551654983521\n",
      "Step:108, train: -0.006068893155486398\n",
      "Step:109, train: -0.0061655893462217145\n",
      "Step:110, train: -0.006259854875841803\n",
      "Step:111, train: -0.006336246679045931\n",
      "Step:112, train: -0.00648572354629201\n",
      "Step:113, train: -0.006561972326282462\n",
      "Step:114, train: -0.006658321167529744\n",
      "Step:115, train: -0.006751662530903383\n",
      "Step:116, train: -0.006900385907406421\n",
      "Step:117, train: -0.007016097792571957\n",
      "Step:118, train: -0.007090400351173105\n",
      "Step:119, train: -0.007113954773772011\n",
      "Step:120, train: -0.007184285027697104\n",
      "Step:121, train: -0.007360819159746508\n",
      "Step:122, train: -0.007497102736818089\n",
      "Step:123, train: -0.007614880915839808\n",
      "Step:124, train: -0.007758303349895402\n",
      "Step:125, train: -0.007799744052806835\n",
      "Step:126, train: -0.007929868300956346\n",
      "Step:127, train: -0.007983117499552116\n",
      "Step:128, train: -0.008163792696007257\n",
      "Step:129, train: -0.008332251010893047\n",
      "Step:130, train: -0.008510665050124714\n",
      "Step:131, train: -0.008561503041499241\n",
      "Step:132, train: -0.008767501019193769\n",
      "Step:133, train: -0.008938236237394068\n",
      "Step:134, train: -0.009106144054915558\n",
      "Step:135, train: -0.0091048578956835\n",
      "Step:136, train: -0.00926185893039243\n",
      "Step:137, train: -0.009545749905640367\n",
      "Step:138, train: -0.009680646762626364\n",
      "Step:139, train: -0.009845849340169468\n",
      "Step:140, train: -0.010071474617982943\n",
      "Step:141, train: -0.010274347189691757\n",
      "Step:142, train: -0.010388319824705802\n",
      "Step:143, train: -0.010552454343003136\n",
      "Step:144, train: -0.010763101165243774\n",
      "Step:145, train: -0.01085048391053629\n",
      "Step:146, train: -0.011041024694001462\n",
      "Step:147, train: -0.011282373851742845\n",
      "Step:148, train: -0.011461819079233533\n",
      "Step:149, train: -0.01154050034183709\n",
      "Step:150, train: -0.011754817051803817\n",
      "Step:151, train: -0.012015556347534\n",
      "Step:152, train: -0.012090094478110117\n",
      "Step:153, train: -0.01231270407611371\n",
      "Step:154, train: -0.012514384716840332\n",
      "Step:155, train: -0.012692251450768657\n",
      "Step:156, train: -0.012993863319270613\n",
      "Step:157, train: -0.013204864341622493\n",
      "Step:158, train: -0.013532853955877455\n",
      "Step:159, train: -0.013778948359281985\n",
      "Step:160, train: -0.013929112391697201\n",
      "Step:161, train: -0.014058941228034727\n",
      "Step:162, train: -0.014308888900640323\n",
      "Step:163, train: -0.014616331618431152\n",
      "Step:164, train: -0.014891607107761076\n",
      "Step:165, train: -0.01513924861846188\n",
      "Step:166, train: -0.015275340116939023\n",
      "Step:167, train: -0.015531342241845643\n",
      "Step:168, train: -0.01597129705013071\n",
      "Step:169, train: -0.016270800107221154\n",
      "Step:170, train: -0.016484074390634953\n",
      "Step:171, train: -0.016778485557531704\n",
      "Step:172, train: -0.01688657707738375\n",
      "Step:173, train: -0.017294399624676382\n",
      "Step:174, train: -0.017517759887589263\n",
      "Step:175, train: -0.017621566241146986\n",
      "Step:176, train: -0.018005130962267826\n",
      "Step:177, train: -0.018311471278681945\n",
      "Step:178, train: -0.01864353954299663\n",
      "Step:179, train: -0.01909060197192187\n",
      "Step:180, train: -0.01938236691042366\n",
      "Step:181, train: -0.019804540244567\n",
      "Step:182, train: -0.020051504822427582\n",
      "Step:183, train: -0.020419317607170653\n",
      "Step:184, train: -0.02067448959551376\n",
      "Step:185, train: -0.021104359972513274\n",
      "Step:186, train: -0.021173936310655797\n",
      "Step:187, train: -0.021387571476042876\n",
      "Step:188, train: -0.02147571566655803\n",
      "Step:189, train: -0.02168878132516597\n",
      "Step:190, train: -0.022017079556673712\n",
      "Step:191, train: -0.022185141222238133\n",
      "Step:192, train: -0.02251457159095545\n",
      "Step:193, train: -0.02268445887726462\n",
      "Step:194, train: -0.02287192279520366\n",
      "Step:195, train: -0.023097456616972944\n",
      "Step:196, train: -0.023269487575287788\n",
      "Step:197, train: -0.02368926168115759\n",
      "Step:198, train: -0.02406502843708583\n",
      "Step:199, train: -0.024283334328704154\n",
      "Step:200, train: -0.024432163863659484\n",
      "Step:201, train: -0.024475548480067204\n",
      "Step:202, train: -0.024705754858002842\n",
      "Step:203, train: -0.025117038622857184\n",
      "Step:204, train: -0.025655169092564125\n",
      "Step:205, train: -0.026005546330518044\n",
      "Step:206, train: -0.02612934656451312\n",
      "Step:207, train: -0.02658029583576499\n",
      "Step:208, train: -0.026844834269236804\n",
      "Step:209, train: -0.02712166893430618\n",
      "Step:210, train: -0.027265473778748373\n",
      "Step:211, train: -0.027637449567868563\n",
      "Step:212, train: -0.028116806714675893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:213, train: -0.028477684797354227\n",
      "Step:214, train: -0.028742956794638532\n",
      "Step:215, train: -0.028953220572798654\n",
      "Step:216, train: -0.029339126054703738\n",
      "Step:217, train: -0.02990565398656004\n",
      "Step:218, train: -0.03008038272272897\n",
      "Step:219, train: -0.030418632593750765\n",
      "Step:220, train: -0.030714262578470364\n",
      "Step:221, train: -0.03106129314280373\n",
      "Step:222, train: -0.0316590306144999\n",
      "Step:223, train: -0.032080619257954184\n",
      "Step:224, train: -0.0323416861501549\n",
      "Step:225, train: -0.0326434500627988\n",
      "Step:226, train: -0.03335410852529727\n",
      "Step:227, train: -0.03377075229283513\n",
      "Step:228, train: -0.03409989626294574\n",
      "Step:229, train: -0.034480983979264815\n",
      "Step:230, train: -0.03498593927660917\n",
      "Step:231, train: -0.03551546303893327\n",
      "Step:232, train: -0.03602851673066498\n",
      "Step:233, train: -0.03641602133720351\n",
      "Step:234, train: -0.03672478407224903\n",
      "Step:235, train: -0.0372200861669417\n",
      "Step:236, train: -0.03788824517029659\n",
      "Step:237, train: -0.03826958911202121\n",
      "Step:238, train: -0.03870887100351776\n",
      "Step:239, train: -0.039175629512331604\n",
      "Step:240, train: -0.039534265814662996\n",
      "Step:241, train: -0.03985609222929136\n",
      "Step:242, train: -0.04024720207661672\n",
      "Step:243, train: -0.04099997722956027\n",
      "Step:244, train: -0.04149447390201445\n",
      "Step:245, train: -0.042108584057362286\n",
      "Step:246, train: -0.042576098208557775\n",
      "Step:247, train: -0.04295992044298917\n",
      "Step:248, train: -0.043658821046522876\n",
      "Step:249, train: -0.04398841332517518\n",
      "Step:250, train: -0.04445986712880668\n",
      "Step:251, train: -0.04527078870215152\n",
      "Step:252, train: -0.0458339672240113\n",
      "Step:253, train: -0.046480952199866615\n",
      "Step:254, train: -0.04707845302922274\n",
      "Step:255, train: -0.04750157547018559\n",
      "Step:256, train: -0.04779962289962779\n",
      "Step:257, train: -0.048165719538440246\n",
      "Step:258, train: -0.04912173398655189\n",
      "Step:259, train: -0.04940043274597946\n",
      "Step:260, train: -0.047829690929348814\n",
      "Step:261, train: -0.05067401803749693\n",
      "Step:262, train: -0.05113574919090183\n",
      "Step:263, train: -0.05148075231516115\n",
      "Step:264, train: -0.051890523912220377\n",
      "Step:265, train: -0.051756595345561325\n",
      "Step:266, train: -0.05263776195411853\n",
      "Step:267, train: -0.052997331475490066\n",
      "Step:268, train: -0.05335320074053059\n",
      "Step:269, train: -0.05377107348933577\n",
      "Step:270, train: -0.05420868400391308\n",
      "Step:271, train: -0.05497038142995739\n",
      "Step:272, train: -0.055452604490809174\n",
      "Step:273, train: -0.056136619910113046\n",
      "Step:274, train: -0.05695030274779904\n",
      "Step:275, train: -0.057786681378017866\n",
      "Step:276, train: -0.05814281787619487\n",
      "Step:277, train: -0.058941445980065885\n",
      "Step:278, train: -0.059210433145832714\n",
      "Step:279, train: -0.05996528155508703\n",
      "Step:280, train: -0.05993494329367777\n",
      "Step:281, train: -0.060384835517047854\n",
      "Step:282, train: -0.06093585039186473\n",
      "Step:283, train: -0.06184432172653167\n",
      "Step:284, train: -0.06296073307648146\n",
      "Step:285, train: -0.06358049086853391\n",
      "Step:286, train: -0.0640860256009656\n",
      "Step:287, train: -0.06487232249250312\n",
      "Step:288, train: -0.06449626669964065\n",
      "Step:289, train: -0.06537171069586678\n",
      "Step:290, train: -0.06749210434835155\n",
      "Step:291, train: -0.06824221625090078\n",
      "Step:292, train: -0.06792139696995002\n",
      "Step:293, train: -0.06891815404861809\n",
      "Step:294, train: -0.06993025182712605\n",
      "Step:295, train: -0.07069285580241697\n",
      "Step:296, train: -0.0711292292721066\n",
      "Step:297, train: -0.07179003749163233\n",
      "Step:298, train: -0.07252259920974313\n",
      "Step:299, train: -0.07330490505633903\n",
      "Step:300, train: -0.07387888075749433\n",
      "Step:301, train: -0.07465285547628445\n",
      "Step:302, train: -0.07568677823487886\n",
      "Step:303, train: -0.07675816017667429\n",
      "Step:304, train: -0.07766490394372\n",
      "Step:305, train: -0.07886629123088773\n",
      "Step:306, train: -0.07868618924765679\n",
      "Step:307, train: -0.08006527848481634\n",
      "Step:308, train: -0.07992450088960801\n",
      "Step:309, train: -0.08084386836889448\n",
      "Step:310, train: -0.0733662243824549\n",
      "Step:311, train: -0.08162175278372388\n",
      "Step:312, train: -0.08173562822583184\n",
      "Step:313, train: -0.08265484093411125\n",
      "Step:314, train: -0.08429605391216537\n",
      "Step:315, train: -0.08501348034753076\n",
      "Step:316, train: -0.0858611107078214\n",
      "Step:317, train: -0.08480199859024523\n",
      "Step:318, train: -0.0862389598167256\n",
      "Step:319, train: -0.0872617747176162\n",
      "Step:320, train: -0.08776618012419292\n",
      "Step:321, train: -0.08946218023513033\n",
      "Step:322, train: -0.08963938165544125\n",
      "Step:323, train: -0.090198404452483\n",
      "Step:324, train: -0.09101221658047934\n",
      "Step:325, train: -0.09204929504046662\n",
      "Step:326, train: -0.09260255459864633\n",
      "Step:327, train: -0.09328004303242565\n",
      "Step:328, train: -0.09421806352879694\n",
      "Step:329, train: -0.09485625539610157\n",
      "Step:330, train: -0.09548170937594663\n",
      "Step:331, train: -0.09644613907542386\n",
      "Step:332, train: -0.09752105454598077\n",
      "Step:333, train: -0.09867462941092542\n",
      "Step:334, train: -0.09993161664224699\n",
      "Step:335, train: -0.09943312070420193\n",
      "Step:336, train: -0.10167949079916988\n",
      "Step:337, train: -0.10268712127526988\n",
      "Step:338, train: -0.10065186995011237\n",
      "Step:339, train: -0.10123392061164113\n",
      "Step:340, train: -0.10414990876867702\n",
      "Step:341, train: -0.10476280966197181\n",
      "Step:342, train: -0.10549753956070454\n",
      "Step:343, train: -0.10578530069334645\n",
      "Step:344, train: -0.10648487909909195\n",
      "Step:345, train: -0.10767062227416918\n",
      "Step:346, train: -0.10797863041016884\n",
      "Step:347, train: -0.10931864018350196\n",
      "Step:348, train: -0.11026364509219154\n",
      "Step:349, train: -0.11093600703881198\n",
      "Step:350, train: -0.11239324769154557\n",
      "Step:351, train: -0.11469604838840913\n",
      "Step:352, train: -0.11600813305225738\n",
      "Step:353, train: -0.1171623669006658\n",
      "Step:354, train: -0.11870061236351727\n",
      "Step:355, train: -0.1200325730053085\n",
      "Step:356, train: -0.12095765234455452\n",
      "Step:357, train: -0.12069565243253536\n",
      "Step:358, train: -0.1222924074301091\n",
      "Step:359, train: -0.12459167905426918\n",
      "Step:360, train: -0.12623632106499763\n",
      "Step:361, train: -0.12789101093372707\n",
      "Step:362, train: -0.12856292174451592\n",
      "Step:363, train: -0.12980010834989605\n",
      "Step:364, train: -0.13143201953965158\n",
      "Step:365, train: -0.13315143863538692\n",
      "Step:366, train: -0.1321980592570164\n",
      "Step:367, train: -0.1342401012970746\n",
      "Step:368, train: -0.13694732204736468\n",
      "Step:369, train: -0.13814492151453006\n",
      "Step:370, train: -0.13954192655624992\n",
      "Step:371, train: -0.1414710707829228\n",
      "Step:372, train: -0.1434945500739929\n",
      "Step:373, train: -0.14568194970687467\n",
      "Step:374, train: -0.14771292301784988\n",
      "Step:375, train: -0.14998971744526157\n",
      "Step:376, train: -0.15236382405747717\n",
      "Step:377, train: -0.15481659248413626\n",
      "Step:378, train: -0.1564774405888015\n",
      "Step:379, train: -0.15855344692806767\n",
      "Step:380, train: -0.16073867116010188\n",
      "Step:381, train: -0.16265234163015826\n",
      "Step:382, train: -0.1654021274681842\n",
      "Step:383, train: -0.16724256671827012\n",
      "Step:384, train: -0.16954002068936802\n",
      "Step:385, train: -0.17274855038083953\n",
      "Step:386, train: -0.1748777887547229\n",
      "Step:387, train: -0.17711175670737092\n",
      "Step:388, train: -0.1796071305934742\n",
      "Step:389, train: -0.18193182747259043\n",
      "Step:390, train: -0.18452511249388936\n",
      "Step:391, train: -0.18678877459089588\n",
      "Step:392, train: -0.18980885992655033\n",
      "Step:393, train: -0.193470407109051\n",
      "Step:394, train: -0.1970922098552313\n",
      "Step:395, train: -0.19798137778142347\n",
      "Step:396, train: -0.2015784725956595\n",
      "Step:397, train: -0.2061009225798184\n",
      "Step:398, train: -0.20893916376275204\n",
      "Step:399, train: -0.2128260393800807\n",
      "Step:400, train: -0.21761249333233884\n",
      "Step:401, train: -0.22107936862771044\n",
      "Step:402, train: -0.22540075420210057\n",
      "Step:403, train: -0.2297267286202763\n",
      "Step:404, train: -0.23402749382855687\n",
      "Step:405, train: -0.2387717218097989\n",
      "Step:406, train: -0.24396770908499152\n",
      "Step:407, train: -0.24922861031111362\n",
      "Step:408, train: -0.2533279114389536\n",
      "Step:409, train: -0.2584071390786913\n",
      "Step:410, train: -0.2636531577623743\n",
      "Step:411, train: -0.26883488942339334\n",
      "Step:412, train: -0.27475822840975805\n",
      "Step:413, train: -0.28139318295133287\n",
      "Step:414, train: -0.2878689874070824\n",
      "Step:415, train: -0.29471990033952994\n",
      "Step:416, train: -0.3005110954107119\n",
      "Step:417, train: -0.3065996920126215\n",
      "Step:418, train: -0.3135566435838071\n",
      "Step:419, train: -0.3214223109165315\n",
      "Step:420, train: -0.32930006005606416\n",
      "Step:421, train: -0.3376200294265316\n",
      "Step:422, train: -0.34593957185802915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\optimization.py:81\u001b[0m, in \u001b[0;36mModelQuantumMap.train\u001b[1;34m(self, inputs, targets, inputs_val, targets_val, num_iter, N)\u001b[0m\n\u001b[0;32m     79\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loss_function \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function: \n\u001b[1;32m---> 81\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list))\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\loss_functions.py:221\u001b[0m, in \u001b[0;36mConj3.__call__\u001b[1;34m(self, channel, input, target)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, channel, \u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m    220\u001b[0m     d \u001b[38;5;241m=\u001b[39m channel\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m--> 221\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mchannel_spectrum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign\u001b[38;5;241m*\u001b[39mtf\u001b[38;5;241m.\u001b[39mabs(z[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mabs(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_prod(z))\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:89\u001b[0m, in \u001b[0;36mchannel_spectrum\u001b[1;34m(channel, real)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchannel_spectrum\u001b[39m(channel, real\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 89\u001b[0m     eig, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meig(reshuffle(\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoi\u001b[49m))\n\u001b[0;32m     90\u001b[0m     eig \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(eig, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m real:\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:201\u001b[0m, in \u001b[0;36mKrausMap.choi\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;129m@property\u001b[39m    \n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoi\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkraus_to_choi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:34\u001b[0m, in \u001b[0;36mkraus_to_choi\u001b[1;34m(kraus_channel, use_reshuffle)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rank):\n\u001b[0;32m     33\u001b[0m     K \u001b[38;5;241m=\u001b[39m kraus[\u001b[38;5;241m0\u001b[39m, i]\n\u001b[1;32m---> 34\u001b[0m     channel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reshuffle:\n\u001b[0;32m     37\u001b[0m     choi \u001b[38;5;241m=\u001b[39m reshuffle(channel)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:408\u001b[0m, in \u001b[0;36mkron\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    406\u001b[0m a_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(t_a)\n\u001b[0;32m    407\u001b[0m b_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(t_b)\n\u001b[1;32m--> 408\u001b[0m a_reshaped \u001b[38;5;241m=\u001b[39m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(t_a, \u001b[43m_make_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    409\u001b[0m b_reshaped \u001b[38;5;241m=\u001b[39m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(t_b, _make_shape(b_shape, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    410\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m a_shape \u001b[38;5;241m*\u001b[39m b_shape\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:399\u001b[0m, in \u001b[0;36mkron.<locals>._make_shape\u001b[1;34m(shape, prepend)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_shape\u001b[39m(shape, prepend):\n\u001b[1;32m--> 399\u001b[0m   ones \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m prepend:\n\u001b[0;32m    401\u001b[0m     shapes \u001b[38;5;241m=\u001b[39m [ones, shape]\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3185\u001b[0m, in \u001b[0;36mones_like\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   3154\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mones_like\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   3155\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_unary_elementwise_api\n\u001b[0;32m   3156\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   3157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mones_like\u001b[39m(tensor, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   3158\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a tensor with all elements set to 1.\u001b[39;00m\n\u001b[0;32m   3159\u001b[0m \n\u001b[0;32m   3160\u001b[0m \u001b[38;5;124;03m  See also `tf.ones`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;124;03m    A `Tensor` with all elements set to 1.\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3185\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mones_like_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3231\u001b[0m, in \u001b[0;36mones_like_impl\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3230\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m-> 3231\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mones_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m   3233\u001b[0m   ret\u001b[38;5;241m.\u001b[39mset_shape(tensor\u001b[38;5;241m.\u001b[39mget_shape())\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3288\u001b[0m, in \u001b[0;36mones\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   3286\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape\u001b[38;5;241m.\u001b[39m_shape_tuple():\n\u001b[0;32m   3287\u001b[0m     shape \u001b[38;5;241m=\u001b[39m reshape(shape, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Ensure it's a vector\u001b[39;00m\n\u001b[1;32m-> 3288\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3289\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype \u001b[38;5;241m==\u001b[39m dtype\n\u001b[0;32m   3290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:243\u001b[0m, in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill\u001b[39m(dims, value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a tensor filled with a scalar value.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m  See also `tf.ones`, `tf.zeros`, `tf.one_hot`, `tf.eye`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m   tensor_util\u001b[38;5;241m.\u001b[39mmaybe_set_static_shape(result, dims)\n\u001b[0;32m    245\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3500\u001b[0m, in \u001b[0;36mfill\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m   3498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3499\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3500\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3501\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3503\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_function = [Conj3(index = 1, sign = 1)]\n",
    "model.zero_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1063a5d93f4bec90a668ffd493a947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: 0.030713805290953137\n",
      "Step:1, train: 0.028699223894213064\n",
      "Step:2, train: 0.02678004630193693\n",
      "Step:3, train: 0.024962899238517564\n",
      "Step:4, train: 0.023251341638317893\n",
      "Step:5, train: 0.021646327481176137\n",
      "Step:6, train: 0.02014680975901247\n",
      "Step:7, train: 0.018750028862025432\n",
      "Step:8, train: 0.01745177296493724\n",
      "Step:9, train: 0.01624673617991047\n",
      "Step:10, train: 0.015128911364634198\n",
      "Step:11, train: 0.014092019432391927\n",
      "Step:12, train: 0.01312985724684231\n",
      "Step:13, train: 0.012236527562145365\n",
      "Step:14, train: 0.011406564196958898\n",
      "Step:15, train: 0.0106349645879232\n",
      "Step:16, train: 0.00991718385842708\n",
      "Step:17, train: 0.009249102735846101\n",
      "Step:18, train: 0.008626987677849236\n",
      "Step:19, train: 0.008047449991388967\n",
      "Step:20, train: 0.0075074000500233605\n",
      "Step:21, train: 0.00700401067240871\n",
      "Step:22, train: 0.006534687076137381\n",
      "Step:23, train: 0.0060970376976881785\n",
      "Step:24, train: 0.005688852428500489\n",
      "Step:25, train: 0.005308083379575189\n",
      "Step:26, train: 0.004952830995845784\n",
      "Step:27, train: 0.004621329932816632\n",
      "Step:28, train: 0.0043119383871786125\n",
      "Step:29, train: 0.00402312869939265\n",
      "Step:30, train: 0.003753479889237004\n",
      "Step:31, train: 0.0035016693489768637\n",
      "Step:32, train: 0.003266467286996281\n",
      "Step:33, train: 0.003046730105904819\n",
      "Step:34, train: 0.0028413958307183466\n",
      "Step:35, train: 0.002649479021940374\n",
      "Step:36, train: 0.0024700657961952595\n",
      "Step:37, train: 0.0023023093977028314\n",
      "Step:38, train: 0.002145427376560957\n",
      "Step:39, train: 0.0019986969241526337\n",
      "Step:40, train: 0.0018614512688793564\n",
      "Step:41, train: 0.001733076714844019\n",
      "Step:42, train: 0.0016130100134534033\n",
      "Step:43, train: 0.001500733960595566\n",
      "Step:44, train: 0.0013957752851221543\n",
      "Step:45, train: 0.0012977006856664285\n",
      "Step:46, train: 0.0012061142584906678\n",
      "Step:47, train: 0.0011206528938895294\n",
      "Step:48, train: 0.0010409830671553313\n",
      "Step:49, train: 0.0009667965548800567\n",
      "Step:50, train: 0.0008978057051592736\n",
      "Step:51, train: 0.0008337393958843265\n",
      "Step:52, train: 0.0007743386157182041\n",
      "Step:53, train: 0.0007193521939009627\n",
      "Step:54, train: 0.0006685341856160352\n",
      "Step:55, train: 0.0006216410747744864\n",
      "Step:56, train: 0.0005784307153850991\n",
      "Step:57, train: 0.0005386623780162794\n",
      "Step:58, train: 0.0005020977592770407\n",
      "Step:59, train: 0.00046850226833840074\n",
      "Step:60, train: 0.0004376473289350824\n",
      "Step:61, train: 0.0004093125750005385\n",
      "Step:62, train: 0.00038328759717597453\n",
      "Step:63, train: 0.00035937375129369427\n",
      "Step:64, train: 0.0003373852302424211\n",
      "Step:65, train: 0.00031714960050424694\n",
      "Step:66, train: 0.00029850811634024636\n",
      "Step:67, train: 0.00028131555141089126\n",
      "Step:68, train: 0.0002654395492664242\n",
      "Step:69, train: 0.000250760049512804\n",
      "Step:70, train: 0.00023716848552494526\n",
      "Step:71, train: 0.00022456674323540087\n",
      "Step:72, train: 0.00021286644651200354\n",
      "Step:73, train: 0.00020198795748999838\n",
      "Step:74, train: 0.00019185953134892396\n",
      "Step:75, train: 0.00018241655121514725\n",
      "Step:76, train: 0.00017360075564993959\n",
      "Step:77, train: 0.000165359636818057\n",
      "Step:78, train: 0.00015764581663049837\n",
      "Step:79, train: 0.00015041643041201335\n",
      "Step:80, train: 0.00014363272608031475\n",
      "Step:81, train: 0.00013725961757850224\n",
      "Step:82, train: 0.0001312652805223698\n",
      "Step:83, train: 0.00012562079883430863\n",
      "Step:84, train: 0.0001202999017818734\n",
      "Step:85, train: 0.00011527863426751205\n",
      "Step:86, train: 0.00011053520480043327\n",
      "Step:87, train: 0.00010604966416958416\n",
      "Step:88, train: 0.00010180380301283694\n",
      "Step:89, train: 9.778099194370692e-05\n",
      "Step:90, train: 9.396592685788242e-05\n",
      "Step:91, train: 9.034461932483473e-05\n",
      "Step:92, train: 8.690416576131901e-05\n",
      "Step:93, train: 8.363274981230876e-05\n",
      "Step:94, train: 8.051946505962271e-05\n",
      "Step:95, train: 7.755426755630052e-05\n",
      "Step:96, train: 7.472787045087755e-05\n",
      "Step:97, train: 7.20317247072991e-05\n",
      "Step:98, train: 6.945790324690538e-05\n",
      "Step:99, train: 6.699909773662161e-05\n",
      "Step:100, train: 6.464848343098148e-05\n",
      "Step:101, train: 6.239977570666548e-05\n",
      "Step:102, train: 6.024711390835152e-05\n",
      "Step:103, train: 5.8185059860699066e-05\n",
      "Step:104, train: 5.6208575525544256e-05\n",
      "Step:105, train: 5.431294216461655e-05\n",
      "Step:106, train: 5.249379062719081e-05\n",
      "Step:107, train: 5.0747014451999805e-05\n",
      "Step:108, train: 4.9068818494828567e-05\n",
      "Step:109, train: 4.7455628331078355e-05\n",
      "Step:110, train: 4.5904104008346464e-05\n",
      "Step:111, train: 4.441114173975708e-05\n",
      "Step:112, train: 4.2973808821055606e-05\n",
      "Step:113, train: 4.158938256335228e-05\n",
      "Step:114, train: 4.0255304389033435e-05\n",
      "Step:115, train: 3.896915091673743e-05\n",
      "Step:116, train: 3.7728688933663964e-05\n",
      "Step:117, train: 3.653176865391325e-05\n",
      "Step:118, train: 3.537640716963993e-05\n",
      "Step:119, train: 3.426072567548476e-05\n",
      "Step:120, train: 3.318294746841297e-05\n",
      "Step:121, train: 3.214141441745789e-05\n",
      "Step:122, train: 3.1134547283054145e-05\n",
      "Step:123, train: 3.016087637201947e-05\n",
      "Step:124, train: 2.9218993470185722e-05\n",
      "Step:125, train: 2.8307569536229893e-05\n",
      "Step:126, train: 2.7425359002719174e-05\n",
      "Step:127, train: 2.65711786206075e-05\n",
      "Step:128, train: 2.574390856322686e-05\n",
      "Step:129, train: 2.4942485345416776e-05\n",
      "Step:130, train: 2.4165911160168294e-05\n",
      "Step:131, train: 2.3413236658802185e-05\n",
      "Step:132, train: 2.2683540685818196e-05\n",
      "Step:133, train: 2.1975986755615713e-05\n",
      "Step:134, train: 2.1289747173976453e-05\n",
      "Step:135, train: 2.0624050185302987e-05\n",
      "Step:136, train: 1.997816058649545e-05\n",
      "Step:137, train: 1.935137218542159e-05\n",
      "Step:138, train: 1.8743024733980326e-05\n",
      "Step:139, train: 1.8152489468542745e-05\n",
      "Step:140, train: 1.7579156659227412e-05\n",
      "Step:141, train: 1.7022448879066008e-05\n",
      "Step:142, train: 1.648181950094543e-05\n",
      "Step:143, train: 1.595674502516692e-05\n",
      "Step:144, train: 1.5446719869725815e-05\n",
      "Step:145, train: 1.4951265852260494e-05\n",
      "Step:146, train: 1.4469932950541304e-05\n",
      "Step:147, train: 1.4002277372008778e-05\n",
      "Step:148, train: 1.3547880417520298e-05\n",
      "Step:149, train: 1.310634911127184e-05\n",
      "Step:150, train: 1.2677300187196016e-05\n",
      "Step:151, train: 1.2260361079366567e-05\n",
      "Step:152, train: 1.1855178931040243e-05\n",
      "Step:153, train: 1.1461420301086358e-05\n",
      "Step:154, train: 1.1078763198142038e-05\n",
      "Step:155, train: 1.0706895865492692e-05\n",
      "Step:156, train: 1.0345514496798531e-05\n",
      "Step:157, train: 9.994331919094135e-06\n",
      "Step:158, train: 9.653077352249974e-06\n",
      "Step:159, train: 9.321479635214827e-06\n",
      "Step:160, train: 8.999288685830207e-06\n",
      "Step:161, train: 8.686251797085303e-06\n",
      "Step:162, train: 8.382136507790454e-06\n",
      "Step:163, train: 8.08671196546151e-06\n",
      "Step:164, train: 7.799752606543007e-06\n",
      "Step:165, train: 7.5210538587919885e-06\n",
      "Step:166, train: 7.250399503416652e-06\n",
      "Step:167, train: 6.987596473839499e-06\n",
      "Step:168, train: 6.732448926274089e-06\n",
      "Step:169, train: 6.48476605310461e-06\n",
      "Step:170, train: 6.2443695286029785e-06\n",
      "Step:171, train: 6.0110856751695995e-06\n",
      "Step:172, train: 5.78474091066481e-06\n",
      "Step:173, train: 5.565169346823678e-06\n",
      "Step:174, train: 5.352208595238545e-06\n",
      "Step:175, train: 5.14570399908416e-06\n",
      "Step:176, train: 4.945502235718191e-06\n",
      "Step:177, train: 4.751455462473808e-06\n",
      "Step:178, train: 4.5634194548303005e-06\n",
      "Step:179, train: 4.381249109694085e-06\n",
      "Step:180, train: 4.204810704794921e-06\n",
      "Step:181, train: 4.033970410967853e-06\n",
      "Step:182, train: 3.868593556682219e-06\n",
      "Step:183, train: 3.7085527423513186e-06\n",
      "Step:184, train: 3.5537215896453136e-06\n",
      "Step:185, train: 3.403977696975431e-06\n",
      "Step:186, train: 3.259196953730578e-06\n",
      "Step:187, train: 3.1192634691007002e-06\n",
      "Step:188, train: 2.9840603079321323e-06\n",
      "Step:189, train: 2.85347489975187e-06\n",
      "Step:190, train: 2.7273921272539026e-06\n",
      "Step:191, train: 2.605708095877323e-06\n",
      "Step:192, train: 2.4883114036463682e-06\n",
      "Step:193, train: 2.375094646161942e-06\n",
      "Step:194, train: 2.265950714265397e-06\n",
      "Step:195, train: 2.1607794557338535e-06\n",
      "Step:196, train: 2.0594786307328564e-06\n",
      "Step:197, train: 1.9619489319635385e-06\n",
      "Step:198, train: 1.8680939669510605e-06\n",
      "Step:199, train: 1.7778142406806233e-06\n",
      "Step:200, train: 1.6910152468003576e-06\n",
      "Step:201, train: 1.6076005467769307e-06\n",
      "Step:202, train: 1.5274784909909283e-06\n",
      "Step:203, train: 1.4505573885392054e-06\n",
      "Step:204, train: 1.3767473123254622e-06\n",
      "Step:205, train: 1.3059560552704208e-06\n",
      "Step:206, train: 1.2381014600856874e-06\n",
      "Step:207, train: 1.173093773644298e-06\n",
      "Step:208, train: 1.1108490711509049e-06\n",
      "Step:209, train: 1.051282973691829e-06\n",
      "Step:210, train: 9.943125778677109e-07\n",
      "Step:211, train: 9.398545139765425e-07\n",
      "Step:212, train: 8.87831909974976e-07\n",
      "Step:213, train: 8.381638312667276e-07\n",
      "Step:214, train: 7.907735223221989e-07\n",
      "Step:215, train: 1.286795107756627e-06\n",
      "Step:216, train: 1.178010348901123e-06\n",
      "Step:217, train: 7.90997990232629e-07\n",
      "Step:218, train: 8.292536912039909e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:219, train: 8.605781996605048e-07\n",
      "Step:220, train: 8.853031601082331e-07\n",
      "Step:221, train: 9.038150691932643e-07\n",
      "Step:222, train: 9.165446190067548e-07\n",
      "Step:223, train: 9.239409585380534e-07\n",
      "Step:224, train: 9.264631408256592e-07\n",
      "Step:225, train: 9.245646092958362e-07\n",
      "Step:226, train: 9.186916131948694e-07\n",
      "Step:227, train: 9.092731917829219e-07\n",
      "Step:228, train: 8.967200847352295e-07\n",
      "Step:229, train: 8.814217789732307e-07\n",
      "Step:230, train: 8.637431594862668e-07\n",
      "Step:231, train: 8.440271806431791e-07\n",
      "Step:232, train: 8.225891932764927e-07\n",
      "Step:233, train: 7.997208169566883e-07\n",
      "Step:234, train: 7.756910699395151e-07\n",
      "Step:235, train: 7.507458994213094e-07\n",
      "Step:236, train: 7.251083061718589e-07\n",
      "Step:237, train: 6.989802546778814e-07\n",
      "Step:238, train: 6.725433052246984e-07\n",
      "Step:239, train: 6.459611075475691e-07\n",
      "Step:240, train: 9.16099524902623e-07\n",
      "Step:241, train: 6.390759680943775e-07\n",
      "Step:242, train: 6.539740624637857e-07\n",
      "Step:243, train: 6.643852663141969e-07\n",
      "Step:244, train: 6.706430544452204e-07\n",
      "Step:245, train: 6.73093972207132e-07\n",
      "Step:246, train: 6.720858757344272e-07\n",
      "Step:247, train: 6.679658150498919e-07\n",
      "Step:248, train: 6.610687383197075e-07\n",
      "Step:249, train: 6.517181243466958e-07\n",
      "Step:250, train: 6.402184206264717e-07\n",
      "Step:251, train: 6.268617920721607e-07\n",
      "Step:252, train: 6.119173555903982e-07\n",
      "Step:253, train: 5.956368687729747e-07\n",
      "Step:254, train: 5.78254466756485e-07\n",
      "Step:255, train: 7.918943427752886e-07\n",
      "Step:256, train: 5.878264826890622e-07\n",
      "Step:257, train: 6.105026721524957e-07\n",
      "Step:258, train: 6.28238070925262e-07\n",
      "Step:259, train: 6.413169037935155e-07\n",
      "Step:260, train: 6.500554573080429e-07\n",
      "Step:261, train: 6.547892394102586e-07\n",
      "Step:262, train: 6.558645273852922e-07\n",
      "Step:263, train: 6.53629983546997e-07\n",
      "Step:264, train: 6.484255759086807e-07\n",
      "Step:265, train: 6.405830745932754e-07\n",
      "Step:266, train: 6.304189046451144e-07\n",
      "Step:267, train: 6.182332517478289e-07\n",
      "Step:268, train: 6.04310291616171e-07\n",
      "Step:269, train: 5.889135108971168e-07\n",
      "Step:270, train: 5.722883879840075e-07\n",
      "Step:271, train: 5.54661407819781e-07\n",
      "Step:272, train: 5.362390261117396e-07\n",
      "Step:273, train: 5.17210754238746e-07\n",
      "Step:274, train: 6.037648601806565e-07\n",
      "Step:275, train: 5.456993311935435e-07\n",
      "Step:276, train: 5.877605034258792e-07\n",
      "Step:277, train: 6.238331824622992e-07\n",
      "Step:278, train: 6.539805427276836e-07\n",
      "Step:279, train: 6.783845975185457e-07\n",
      "Step:280, train: 6.973070668328277e-07\n",
      "Step:281, train: 7.110720360601955e-07\n",
      "Step:282, train: 7.200405745370818e-07\n",
      "Step:283, train: 7.245895125272402e-07\n",
      "Step:284, train: 7.251084242427516e-07\n",
      "Step:285, train: 7.219882586165242e-07\n",
      "Step:286, train: 7.156082657520693e-07\n",
      "Step:287, train: 7.063401056083484e-07\n",
      "Step:288, train: 6.945347081337814e-07\n",
      "Step:289, train: 6.805295605695188e-07\n",
      "Step:290, train: 6.646369778774744e-07\n",
      "Step:291, train: 6.471505775251013e-07\n",
      "Step:292, train: 6.283437982351368e-07\n",
      "Step:293, train: 6.084642221411193e-07\n",
      "Step:294, train: 5.877427149442789e-07\n",
      "Step:295, train: 5.663860988250045e-07\n",
      "Step:296, train: 5.445852677441428e-07\n",
      "Step:297, train: 5.225086186850909e-07\n",
      "Step:298, train: 5.003091365690912e-07\n",
      "Step:299, train: 4.781224223944217e-07\n",
      "Step:300, train: 4.5606785598603114e-07\n",
      "Step:301, train: 4.342497617459015e-07\n",
      "Step:302, train: 4.7135822253932977e-07\n",
      "Step:303, train: 4.728235090140143e-07\n",
      "Step:304, train: 5.274955869652269e-07\n",
      "Step:305, train: 5.762076227129367e-07\n",
      "Step:306, train: 6.186793554374387e-07\n",
      "Step:307, train: 6.548468647032588e-07\n",
      "Step:308, train: 6.848040954514059e-07\n",
      "Step:309, train: 7.087652349446434e-07\n",
      "Step:310, train: 7.270251205133239e-07\n",
      "Step:311, train: 7.399343016270143e-07\n",
      "Step:312, train: 7.478811849740693e-07\n",
      "Step:313, train: 7.512718860404442e-07\n",
      "Step:314, train: 7.505211462447522e-07\n",
      "Step:315, train: 7.460407189081728e-07\n",
      "Step:316, train: 7.382347725831639e-07\n",
      "Step:317, train: 7.274909887762239e-07\n",
      "Step:318, train: 7.1418101106642e-07\n",
      "Step:319, train: 6.986567666244296e-07\n",
      "Step:320, train: 6.812445987024947e-07\n",
      "Step:321, train: 6.622531696923844e-07\n",
      "Step:322, train: 6.419640221183812e-07\n",
      "Step:323, train: 6.206366833554322e-07\n",
      "Step:324, train: 5.985087731682227e-07\n",
      "Step:325, train: 5.757949885704335e-07\n",
      "Step:326, train: 5.526919091244667e-07\n",
      "Step:327, train: 5.293733442300487e-07\n",
      "Step:328, train: 5.059951799884634e-07\n",
      "Step:329, train: 4.826948505041262e-07\n",
      "Step:330, train: 4.595932338929162e-07\n",
      "Step:331, train: 4.367973939133625e-07\n",
      "Step:332, train: 4.143987962775691e-07\n",
      "Step:333, train: 3.924749402247905e-07\n",
      "Step:334, train: 3.71093274639184e-07\n",
      "Step:335, train: 3.503086784368572e-07\n",
      "Step:336, train: 3.301664950765181e-07\n",
      "Step:337, train: 5.912014290439317e-07\n",
      "Step:338, train: 5.473536381308505e-07\n",
      "Step:339, train: 3.334470005367221e-07\n",
      "Step:340, train: 3.514836620894071e-07\n",
      "Step:341, train: 3.661198099172103e-07\n",
      "Step:342, train: 3.7748250579335377e-07\n",
      "Step:343, train: 3.857459492533459e-07\n",
      "Step:344, train: 3.911131226035037e-07\n",
      "Step:345, train: 3.9380682197146416e-07\n",
      "Step:346, train: 3.9405966061568915e-07\n",
      "Step:347, train: 3.9210740856199043e-07\n",
      "Step:348, train: 3.881848141263607e-07\n",
      "Step:349, train: 3.825211058031427e-07\n",
      "Step:350, train: 3.7533554910031027e-07\n",
      "Step:351, train: 3.668389799443318e-07\n",
      "Step:352, train: 3.5722861512004345e-07\n",
      "Step:353, train: 3.4668973043542074e-07\n",
      "Step:354, train: 3.3539244406552235e-07\n",
      "Step:355, train: 3.234948477201479e-07\n",
      "Step:356, train: 3.1114008454602884e-07\n",
      "Step:357, train: 2.9845875088244047e-07\n",
      "Step:358, train: 3.1074342015648834e-07\n",
      "Step:359, train: 3.66080746527297e-07\n",
      "Step:360, train: 4.441869976038417e-07\n",
      "Step:361, train: 5.177837608040112e-07\n",
      "Step:362, train: 5.854507586190266e-07\n",
      "Step:363, train: 6.462888619308467e-07\n",
      "Step:364, train: 6.997998977033765e-07\n",
      "Step:365, train: 7.457855110072659e-07\n",
      "Step:366, train: 7.842728317988704e-07\n",
      "Step:367, train: 8.154572665747861e-07\n",
      "Step:368, train: 8.396519053958711e-07\n",
      "Step:369, train: 8.572530826871163e-07\n",
      "Step:370, train: 8.687107957756256e-07\n",
      "Step:371, train: 8.745102507190984e-07\n",
      "Step:372, train: 8.751503292021768e-07\n",
      "Step:373, train: 8.711301235470452e-07\n",
      "Step:374, train: 8.629469618424557e-07\n",
      "Step:375, train: 8.510783254151484e-07\n",
      "Step:376, train: 8.359839718718685e-07\n",
      "Step:377, train: 8.180970600113701e-07\n",
      "Step:378, train: 7.97830562714919e-07\n",
      "Step:379, train: 7.755633701433783e-07\n",
      "Step:380, train: 7.516503585674911e-07\n",
      "Step:381, train: 7.264184205328576e-07\n",
      "Step:382, train: 7.001635057078785e-07\n",
      "Step:383, train: 6.731568008450825e-07\n",
      "Step:384, train: 6.456425429414194e-07\n",
      "Step:385, train: 6.178384582761657e-07\n",
      "Step:386, train: 5.899412361142223e-07\n",
      "Step:387, train: 5.62124354841681e-07\n",
      "Step:388, train: 5.345409050220353e-07\n",
      "Step:389, train: 5.073225152600981e-07\n",
      "Step:390, train: 4.805851298419725e-07\n",
      "Step:391, train: 4.5442734726066093e-07\n",
      "Step:392, train: 4.2893288347613564e-07\n",
      "Step:393, train: 4.041691854287956e-07\n",
      "Step:394, train: 3.801934915856073e-07\n",
      "Step:395, train: 3.570502540776337e-07\n",
      "Step:396, train: 3.347723497410028e-07\n",
      "Step:397, train: 3.13385995010994e-07\n",
      "Step:398, train: 2.92906189719108e-07\n",
      "Step:399, train: 2.7334303434834244e-07\n",
      "Step:400, train: 2.546972658817581e-07\n",
      "Step:401, train: 2.3696617005397994e-07\n",
      "Step:402, train: 2.2014060588362846e-07\n",
      "Step:403, train: 2.761354466845523e-07\n",
      "Step:404, train: 2.118626981619156e-07\n",
      "Step:405, train: 2.175834856363759e-07\n",
      "Step:406, train: 2.2147982516511615e-07\n",
      "Step:407, train: 2.236801246003413e-07\n",
      "Step:408, train: 2.2432370098504052e-07\n",
      "Step:409, train: 2.2355564274055717e-07\n",
      "Step:410, train: 2.2152296541839306e-07\n",
      "Step:411, train: 2.183705450934071e-07\n",
      "Step:412, train: 2.1423975279641835e-07\n",
      "Step:413, train: 2.0926575457970887e-07\n",
      "Step:414, train: 2.0357618823240794e-07\n",
      "Step:415, train: 1.972916493865554e-07\n",
      "Step:416, train: 2.521820096299861e-07\n",
      "Step:417, train: 2.065621332900427e-07\n",
      "Step:418, train: 2.2023687114532835e-07\n",
      "Step:419, train: 2.3153738200047807e-07\n",
      "Step:420, train: 2.4051495230252485e-07\n",
      "Step:421, train: 2.472614660716095e-07\n",
      "Step:422, train: 2.5190386410539595e-07\n",
      "Step:423, train: 2.5458642126121336e-07\n",
      "Step:424, train: 2.554683047307372e-07\n",
      "Step:425, train: 2.547155921868023e-07\n",
      "Step:426, train: 2.5249599434838263e-07\n",
      "Step:427, train: 2.489777039222659e-07\n",
      "Step:428, train: 2.4432262529957205e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:429, train: 2.386872951411324e-07\n",
      "Step:430, train: 2.3221866561367886e-07\n",
      "Step:431, train: 2.2505675138025286e-07\n",
      "Step:432, train: 2.1733085263476447e-07\n",
      "Step:433, train: 2.0916066854762598e-07\n",
      "Step:434, train: 2.0065472951419992e-07\n",
      "Step:435, train: 1.919118999820072e-07\n",
      "Step:436, train: 1.8302208238395936e-07\n",
      "Step:437, train: 1.7406495278458124e-07\n",
      "Step:438, train: 3.071534892434456e-07\n",
      "Step:439, train: 2.508188371799382e-07\n",
      "Step:440, train: 1.8718125841044604e-07\n",
      "Step:441, train: 2.036301699773594e-07\n",
      "Step:442, train: 2.1769386120709272e-07\n",
      "Step:443, train: 2.2935269004823362e-07\n",
      "Step:444, train: 2.386480087662483e-07\n",
      "Step:445, train: 2.4566764241546556e-07\n",
      "Step:446, train: 2.505338819760762e-07\n",
      "Step:447, train: 2.5339311003395054e-07\n",
      "Step:448, train: 2.5440526766153797e-07\n",
      "Step:449, train: 2.5373957676051053e-07\n",
      "Step:450, train: 2.5156800386721793e-07\n",
      "Step:451, train: 2.480620775603205e-07\n",
      "Step:452, train: 2.433879425323428e-07\n",
      "Step:453, train: 2.3770663710173372e-07\n",
      "Step:454, train: 2.3117109533390915e-07\n",
      "Step:455, train: 2.2392489982031202e-07\n",
      "Step:456, train: 2.1610132901105588e-07\n",
      "Step:457, train: 2.078234550888685e-07\n",
      "Step:458, train: 1.992042550228787e-07\n",
      "Step:459, train: 1.9034592513617664e-07\n",
      "Step:460, train: 1.8134064895688598e-07\n",
      "Step:461, train: 1.7227101653099937e-07\n",
      "Step:462, train: 1.6320994264688077e-07\n",
      "Step:463, train: 1.5422026148732948e-07\n",
      "Step:464, train: 2.65444638238173e-07\n",
      "Step:465, train: 2.208170547751261e-07\n",
      "Step:466, train: 1.643842365615081e-07\n",
      "Step:467, train: 1.7881531879855214e-07\n",
      "Step:468, train: 1.9111804500655763e-07\n",
      "Step:469, train: 2.0127337196655872e-07\n",
      "Step:470, train: 2.0931818803253095e-07\n",
      "Step:471, train: 2.1533228589683993e-07\n",
      "Step:472, train: 2.1942674650679334e-07\n",
      "Step:473, train: 2.217331925628482e-07\n",
      "Step:474, train: 2.223971007304856e-07\n",
      "Step:475, train: 2.215710840875968e-07\n",
      "Step:476, train: 2.1941083760222602e-07\n",
      "Step:477, train: 2.160713118266703e-07\n",
      "Step:478, train: 2.117043340308352e-07\n",
      "Step:479, train: 2.0645468100926007e-07\n",
      "Step:480, train: 2.004599043129995e-07\n",
      "Step:481, train: 1.9384996662952665e-07\n",
      "Step:482, train: 1.86746040236928e-07\n",
      "Step:483, train: 1.7925740411162585e-07\n",
      "Step:484, train: 1.7148548078370723e-07\n",
      "Step:485, train: 1.6352255444169361e-07\n",
      "Step:486, train: 1.5545113656101525e-07\n",
      "Step:487, train: 1.473441409089536e-07\n",
      "Step:488, train: 1.3926592791672128e-07\n",
      "Step:489, train: 2.0185207398674084e-07\n",
      "Step:490, train: 1.3614626791422942e-07\n",
      "Step:491, train: 1.3968930915843668e-07\n",
      "Step:492, train: 1.419792135873384e-07\n",
      "Step:493, train: 1.431063998380581e-07\n",
      "Step:494, train: 1.431705137295763e-07\n",
      "Step:495, train: 1.4227384342366347e-07\n",
      "Step:496, train: 1.4052204011432987e-07\n",
      "Step:497, train: 1.380188130451452e-07\n",
      "Step:498, train: 1.348653189734303e-07\n",
      "Step:499, train: 1.3115918478888863e-07\n",
      "Step:500, train: 1.5538825963044056e-07\n",
      "Step:501, train: 1.4485010617734627e-07\n",
      "Step:502, train: 1.608706131694496e-07\n",
      "Step:503, train: 1.7486519938981187e-07\n",
      "Step:504, train: 1.8674265423100972e-07\n",
      "Step:505, train: 1.9648577330591905e-07\n",
      "Step:506, train: 2.0413351019303499e-07\n",
      "Step:507, train: 2.0976803647892341e-07\n",
      "Step:508, train: 2.1350285057115584e-07\n",
      "Step:509, train: 2.1547063983290557e-07\n",
      "Step:510, train: 2.1581968283705675e-07\n",
      "Step:511, train: 2.1470431287974277e-07\n",
      "Step:512, train: 2.1228086616343309e-07\n",
      "Step:513, train: 2.0870553190415408e-07\n",
      "Step:514, train: 2.0413021237161775e-07\n",
      "Step:515, train: 1.98701266564689e-07\n",
      "Step:516, train: 1.9255754186108792e-07\n",
      "Step:517, train: 1.8582821822201478e-07\n",
      "Step:518, train: 1.7863486257319647e-07\n",
      "Step:519, train: 1.710864385032625e-07\n",
      "Step:520, train: 1.6328449836079596e-07\n",
      "Step:521, train: 1.5531947078377883e-07\n",
      "Step:522, train: 1.4727316125970456e-07\n",
      "Step:523, train: 1.3921779230342037e-07\n",
      "Step:524, train: 1.3121667721675784e-07\n",
      "Step:525, train: 1.233244436766791e-07\n",
      "Step:526, train: 1.1558823265552568e-07\n",
      "Step:527, train: 1.973956985202301e-07\n",
      "Step:528, train: 1.6914129089513914e-07\n",
      "Step:529, train: 1.2180626122702764e-07\n",
      "Step:530, train: 1.3237415704649015e-07\n",
      "Step:531, train: 1.413081020125379e-07\n",
      "Step:532, train: 1.4859446108133048e-07\n",
      "Step:533, train: 1.542643521364059e-07\n",
      "Step:534, train: 1.5838057356561343e-07\n",
      "Step:535, train: 1.6103342725063244e-07\n",
      "Step:536, train: 1.6232605623635008e-07\n",
      "Step:537, train: 1.6237512780697682e-07\n",
      "Step:538, train: 1.6130210574832019e-07\n",
      "Step:539, train: 1.5923041256064895e-07\n",
      "Step:540, train: 1.5628345876518785e-07\n",
      "Step:541, train: 1.5257870003117055e-07\n",
      "Step:542, train: 1.482323385084863e-07\n",
      "Step:543, train: 1.4335240607063147e-07\n",
      "Step:544, train: 1.380394701285443e-07\n",
      "Step:545, train: 1.3238843600410223e-07\n",
      "Step:546, train: 1.2648552224095358e-07\n",
      "Step:547, train: 1.204089381187646e-07\n",
      "Step:548, train: 1.1422868025697012e-07\n",
      "Step:549, train: 1.0800775545604607e-07\n",
      "Step:550, train: 1.0180086546694614e-07\n",
      "Step:551, train: 1.9185104718707927e-07\n",
      "Step:552, train: 1.6825689206501412e-07\n",
      "Step:553, train: 1.0692955842012571e-07\n",
      "Step:554, train: 1.1527717452965358e-07\n",
      "Step:555, train: 1.2221168275518386e-07\n",
      "Step:556, train: 1.2773785288886496e-07\n",
      "Step:557, train: 1.3189598427752688e-07\n",
      "Step:558, train: 1.3475171190953784e-07\n",
      "Step:559, train: 1.3638769972626782e-07\n",
      "Step:560, train: 1.3690096097602042e-07\n",
      "Step:561, train: 1.3639381243890574e-07\n",
      "Step:562, train: 1.3497320657243186e-07\n",
      "Step:563, train: 1.3274694079193463e-07\n",
      "Step:564, train: 1.2982108035439375e-07\n",
      "Step:565, train: 1.262965223702252e-07\n",
      "Step:566, train: 1.2227175907938623e-07\n",
      "Step:567, train: 1.1783754361620531e-07\n",
      "Step:568, train: 1.1307975422504731e-07\n",
      "Step:569, train: 1.080769984439686e-07\n",
      "Step:570, train: 1.0290071388889352e-07\n",
      "Step:571, train: 9.761492508162267e-08\n",
      "Step:572, train: 9.227799160569892e-08\n",
      "Step:573, train: 1.7505971451562366e-07\n",
      "Step:574, train: 1.4961398077643528e-07\n",
      "Step:575, train: 9.830923566412157e-08\n",
      "Step:576, train: 1.0662818580503359e-07\n",
      "Step:577, train: 1.136011832597733e-07\n",
      "Step:578, train: 1.1921904421720757e-07\n",
      "Step:579, train: 1.2351201096962973e-07\n",
      "Step:580, train: 1.2653585673564806e-07\n",
      "Step:581, train: 1.2836621053059898e-07\n",
      "Step:582, train: 1.290926141005211e-07\n",
      "Step:583, train: 1.2881334243939194e-07\n",
      "Step:584, train: 1.276312726573836e-07\n",
      "Step:585, train: 1.2564940981224392e-07\n",
      "Step:586, train: 1.2297139233797085e-07\n",
      "Step:587, train: 1.196967282041386e-07\n",
      "Step:588, train: 1.1592194481151744e-07\n",
      "Step:589, train: 1.1173656238180454e-07\n",
      "Step:590, train: 1.0722552027420024e-07\n",
      "Step:591, train: 1.0246579724419856e-07\n",
      "Step:592, train: 9.752903384260044e-08\n",
      "Step:593, train: 9.247844321869846e-08\n",
      "Step:594, train: 8.737231149420734e-08\n",
      "Step:595, train: 1.221693855996775e-07\n",
      "Step:596, train: 8.665263417281263e-08\n",
      "Step:597, train: 9.004534446819407e-08\n",
      "Step:598, train: 9.247586692764554e-08\n",
      "Step:599, train: 9.399771668457593e-08\n",
      "Step:600, train: 9.467571692534785e-08\n",
      "Step:601, train: 9.458314969166735e-08\n",
      "Step:602, train: 9.379643194062684e-08\n",
      "Step:603, train: 9.239487614727624e-08\n",
      "Step:604, train: 9.045660203128929e-08\n",
      "Step:605, train: 8.805813520260497e-08\n",
      "Step:606, train: 8.527314727132751e-08\n",
      "Step:607, train: 8.217135176029364e-08\n",
      "Step:608, train: 1.0522078565015973e-07\n",
      "Step:609, train: 8.692052889262969e-08\n",
      "Step:610, train: 9.386718327098813e-08\n",
      "Step:611, train: 9.961821628135187e-08\n",
      "Step:612, train: 1.0417502331362142e-07\n",
      "Step:613, train: 1.0756938785998144e-07\n",
      "Step:614, train: 1.0985622459656097e-07\n",
      "Step:615, train: 1.1110707867656494e-07\n",
      "Step:616, train: 1.1140473470904007e-07\n",
      "Step:617, train: 1.1083807523109167e-07\n",
      "Step:618, train: 1.094998222008954e-07\n",
      "Step:619, train: 1.0748360148605642e-07\n",
      "Step:620, train: 1.048816593194735e-07\n",
      "Step:621, train: 1.017823369161072e-07\n",
      "Step:622, train: 9.827132930997125e-08\n",
      "Step:623, train: 9.442744898504573e-08\n",
      "Step:624, train: 9.032503125732127e-08\n",
      "Step:625, train: 8.60317548260681e-08\n",
      "Step:626, train: 8.160976830453969e-08\n",
      "Step:627, train: 7.711458589422298e-08\n",
      "Step:628, train: 1.0279913807056915e-07\n",
      "Step:629, train: 7.754829339038109e-08\n",
      "Step:630, train: 8.153434727110283e-08\n",
      "Step:631, train: 8.456970382627482e-08\n",
      "Step:632, train: 8.669170894577334e-08\n",
      "Step:633, train: 8.795414540099919e-08\n",
      "Step:634, train: 8.84210001386586e-08\n",
      "Step:635, train: 8.816330685347317e-08\n",
      "Step:636, train: 8.725614470278123e-08\n",
      "Step:637, train: 8.577537027093682e-08\n",
      "Step:638, train: 8.379718605629585e-08\n",
      "Step:639, train: 8.13950309628188e-08\n",
      "Step:640, train: 7.863976107895448e-08\n",
      "Step:641, train: 7.559796981371089e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:642, train: 7.233175931187e-08\n",
      "Step:643, train: 9.166819538843469e-08\n",
      "Step:644, train: 7.597029369357113e-08\n",
      "Step:645, train: 8.201122913717295e-08\n",
      "Step:646, train: 8.69870796370248e-08\n",
      "Step:647, train: 9.089940445717606e-08\n",
      "Step:648, train: 9.377770730922923e-08\n",
      "Step:649, train: 9.567196186380408e-08\n",
      "Step:650, train: 9.664706046473577e-08\n",
      "Step:651, train: 9.677792540849356e-08\n",
      "Step:652, train: 9.61451867286254e-08\n",
      "Step:653, train: 9.483301385151138e-08\n",
      "Step:654, train: 9.292658833766579e-08\n",
      "Step:655, train: 9.05087840065619e-08\n",
      "Step:656, train: 8.765977920348935e-08\n",
      "Step:657, train: 8.44559579697349e-08\n",
      "Step:658, train: 8.096923279522069e-08\n",
      "Step:659, train: 7.726578133149238e-08\n",
      "Step:660, train: 7.340686909365837e-08\n",
      "Step:661, train: 6.944724869878422e-08\n",
      "Step:662, train: 6.543594031406204e-08\n",
      "Step:663, train: 1.1855598704608595e-07\n",
      "Step:664, train: 9.50476888084453e-08\n",
      "Step:665, train: 7.185884497010421e-08\n",
      "Step:666, train: 7.969038027023309e-08\n",
      "Step:667, train: 8.642213610052395e-08\n",
      "Step:668, train: 9.200654693662907e-08\n",
      "Step:669, train: 9.643728203481358e-08\n",
      "Step:670, train: 9.974028609819638e-08\n",
      "Step:671, train: 1.0196576200754213e-07\n",
      "Step:672, train: 1.031813406567972e-07\n",
      "Step:673, train: 1.0346637013577729e-07\n",
      "Step:674, train: 1.0290834054120567e-07\n",
      "Step:675, train: 1.0159810196280803e-07\n",
      "Step:676, train: 9.96276658939965e-08\n",
      "Step:677, train: 9.708790697413345e-08\n",
      "Step:678, train: 9.406778792222652e-08\n",
      "Step:679, train: 9.065090033935911e-08\n",
      "Step:680, train: 8.691682974361725e-08\n",
      "Step:681, train: 8.29385609633805e-08\n",
      "Step:682, train: 7.878375658842835e-08\n",
      "Step:683, train: 7.451344920741096e-08\n",
      "Step:684, train: 7.018203654565165e-08\n",
      "Step:685, train: 6.583861138922146e-08\n",
      "Step:686, train: 6.152540289988016e-08\n",
      "Step:687, train: 9.110776058224734e-08\n",
      "Step:688, train: 5.960149126931092e-08\n",
      "Step:689, train: 6.124040387305187e-08\n",
      "Step:690, train: 6.22324739739496e-08\n",
      "Step:691, train: 6.262460316565206e-08\n",
      "Step:692, train: 6.246849557311252e-08\n",
      "Step:693, train: 6.182153610921515e-08\n",
      "Step:694, train: 6.074081711366727e-08\n",
      "Step:695, train: 5.9284590059470944e-08\n",
      "Step:696, train: 5.750921418406676e-08\n",
      "Step:697, train: 8.439780051756593e-08\n",
      "Step:698, train: 6.007528724137778e-08\n",
      "Step:699, train: 6.386937145162869e-08\n",
      "Step:700, train: 6.684831339958181e-08\n",
      "Step:701, train: 6.90317221004651e-08\n",
      "Step:702, train: 7.045618958714575e-08\n",
      "Step:703, train: 7.117134360898601e-08\n",
      "Step:704, train: 7.123513386940527e-08\n",
      "Step:705, train: 7.070985963519879e-08\n",
      "Step:706, train: 6.966219800438895e-08\n",
      "Step:707, train: 6.815805028564496e-08\n",
      "Step:708, train: 6.626308085890443e-08\n",
      "Step:709, train: 6.40404341401552e-08\n",
      "Step:710, train: 6.15510056544052e-08\n",
      "Step:711, train: 5.885096114621764e-08\n",
      "Step:712, train: 5.599232444100687e-08\n",
      "Step:713, train: 6.979716352547382e-08\n",
      "Step:714, train: 5.8985364633473156e-08\n",
      "Step:715, train: 6.41092479623126e-08\n",
      "Step:716, train: 6.835250431384023e-08\n",
      "Step:717, train: 7.170708746759729e-08\n",
      "Step:718, train: 7.419125645334158e-08\n",
      "Step:719, train: 7.584255259335198e-08\n",
      "Step:720, train: 7.671324600563421e-08\n",
      "Step:721, train: 7.686462955447033e-08\n",
      "Step:722, train: 7.636598417546786e-08\n",
      "Step:723, train: 7.528833937894825e-08\n",
      "Step:724, train: 7.370385846356287e-08\n",
      "Step:725, train: 7.168498412042807e-08\n",
      "Step:726, train: 6.930167374445994e-08\n",
      "Step:727, train: 6.662000550538433e-08\n",
      "Step:728, train: 6.370259314364342e-08\n",
      "Step:729, train: 6.06072353150976e-08\n",
      "Step:730, train: 5.738652568019035e-08\n",
      "Step:731, train: 5.408885867281651e-08\n",
      "Step:732, train: 5.075674286738909e-08\n",
      "Step:733, train: 9.159837380238736e-08\n",
      "Step:734, train: 7.269761653565447e-08\n",
      "Step:735, train: 5.614951532345089e-08\n",
      "Step:736, train: 6.275592064623122e-08\n",
      "Step:737, train: 6.845860521862495e-08\n",
      "Step:738, train: 7.32045103905625e-08\n",
      "Step:739, train: 7.697927681537943e-08\n",
      "Step:740, train: 7.97988345966166e-08\n",
      "Step:741, train: 8.170147558674557e-08\n",
      "Step:742, train: 8.274261217219523e-08\n",
      "Step:743, train: 8.298882400276243e-08\n",
      "Step:744, train: 8.251420855304861e-08\n",
      "Step:745, train: 8.139684429826457e-08\n",
      "Step:746, train: 7.971673458528518e-08\n",
      "Step:747, train: 7.755338483547523e-08\n",
      "Step:748, train: 7.498321855890193e-08\n",
      "Step:749, train: 7.208071680855027e-08\n",
      "Step:750, train: 6.891448943575712e-08\n",
      "Step:751, train: 6.554899822785446e-08\n",
      "Step:752, train: 6.20423760450727e-08\n",
      "Step:753, train: 5.8448550730794905e-08\n",
      "Step:754, train: 5.481472068260953e-08\n",
      "Step:755, train: 5.118285306275616e-08\n",
      "Step:756, train: 4.7589455307533383e-08\n",
      "Step:757, train: 7.587319192002548e-08\n",
      "Step:758, train: 4.545892584192969e-08\n",
      "Step:759, train: 4.633432423532198e-08\n",
      "Step:760, train: 4.672688326599494e-08\n",
      "Step:761, train: 4.6676714439861877e-08\n",
      "Step:762, train: 4.6228073841253154e-08\n",
      "Step:763, train: 4.5427628935760114e-08\n",
      "Step:764, train: 5.227679043899483e-08\n",
      "Step:765, train: 5.5291555561559706e-08\n",
      "Step:766, train: 6.565667838356696e-08\n",
      "Step:767, train: 7.51252262004711e-08\n",
      "Step:768, train: 8.350041822366799e-08\n",
      "Step:769, train: 9.066387505525273e-08\n",
      "Step:770, train: 9.655936068425558e-08\n",
      "Step:771, train: 1.0117828888591509e-07\n",
      "Step:772, train: 1.0454953983643855e-07\n",
      "Step:773, train: 1.0672981345446313e-07\n",
      "Step:774, train: 1.0779644728854344e-07\n",
      "Step:775, train: 1.0784110614652248e-07\n",
      "Step:776, train: 1.0696330812585481e-07\n",
      "Step:777, train: 1.0526740477648095e-07\n",
      "Step:778, train: 1.0285902641951453e-07\n",
      "Step:779, train: 9.984303778601036e-08\n",
      "Step:780, train: 9.631993223512704e-08\n",
      "Step:781, train: 9.23858330813572e-08\n",
      "Step:782, train: 8.81302106455236e-08\n",
      "Step:783, train: 8.363662390080544e-08\n",
      "Step:784, train: 7.898099270851874e-08\n",
      "Step:785, train: 7.423157589375749e-08\n",
      "Step:786, train: 6.944906027794803e-08\n",
      "Step:787, train: 6.468705716952418e-08\n",
      "Step:788, train: 5.999141605382933e-08\n",
      "Step:789, train: 5.540166774584508e-08\n",
      "Step:790, train: 5.0950481930813645e-08\n",
      "Step:791, train: 4.666486634948658e-08\n",
      "Step:792, train: 4.256568713328824e-08\n",
      "Step:793, train: 6.197566217231991e-08\n",
      "Step:794, train: 3.970060490504932e-08\n",
      "Step:795, train: 4.028359616487252e-08\n",
      "Step:796, train: 4.0451723474456113e-08\n",
      "Step:797, train: 4.0242689871287255e-08\n",
      "Step:798, train: 3.9697443257770946e-08\n",
      "Step:799, train: 3.885776080931884e-08\n",
      "Step:800, train: 5.975312182994974e-08\n",
      "Step:801, train: 4.1320543900156405e-08\n",
      "Step:802, train: 4.427310048240526e-08\n",
      "Step:803, train: 4.66106314973151e-08\n",
      "Step:804, train: 4.8341573764309853e-08\n",
      "Step:805, train: 4.9489159391504155e-08\n",
      "Step:806, train: 5.0088281906903385e-08\n",
      "Step:807, train: 5.018221766344951e-08\n",
      "Step:808, train: 4.981883519820153e-08\n",
      "Step:809, train: 4.904863919882596e-08\n",
      "Step:810, train: 4.792436181548347e-08\n",
      "Step:811, train: 4.649777828493196e-08\n",
      "Step:812, train: 4.481941651844363e-08\n",
      "Step:813, train: 4.293731262760308e-08\n",
      "Step:814, train: 4.0896834832031027e-08\n",
      "Step:815, train: 3.873953973888297e-08\n",
      "Step:816, train: 4.4827013662970473e-08\n",
      "Step:817, train: 4.2882978891791525e-08\n",
      "Step:818, train: 4.864278277151011e-08\n",
      "Step:819, train: 5.3671674314422496e-08\n",
      "Step:820, train: 5.790416891105152e-08\n",
      "Step:821, train: 6.131261565136661e-08\n",
      "Step:822, train: 6.390054297713556e-08\n",
      "Step:823, train: 6.569295977765492e-08\n",
      "Step:824, train: 6.67318019165684e-08\n",
      "Step:825, train: 6.707194302777512e-08\n",
      "Step:826, train: 6.677579177167665e-08\n",
      "Step:827, train: 6.591085231224933e-08\n",
      "Step:828, train: 6.454742146345594e-08\n",
      "Step:829, train: 6.275569030650386e-08\n",
      "Step:830, train: 6.06038477558523e-08\n",
      "Step:831, train: 5.8158014885914276e-08\n",
      "Step:832, train: 5.5480208858395905e-08\n",
      "Step:833, train: 5.262822117398603e-08\n",
      "Step:834, train: 4.965510492673415e-08\n",
      "Step:835, train: 4.6609043332180714e-08\n",
      "Step:836, train: 4.353262500284262e-08\n",
      "Step:837, train: 4.046308665444917e-08\n",
      "Step:838, train: 3.743336612920041e-08\n",
      "Step:839, train: 3.447084796217117e-08\n",
      "Step:840, train: 6.803870443061705e-08\n",
      "Step:841, train: 6.292066324546831e-08\n",
      "Step:842, train: 3.54639727240384e-08\n",
      "Step:843, train: 3.8435371604331726e-08\n",
      "Step:844, train: 4.084464390756117e-08\n",
      "Step:845, train: 4.2689360190827565e-08\n",
      "Step:846, train: 4.398407101530566e-08\n",
      "Step:847, train: 4.475572262189382e-08\n",
      "Step:848, train: 4.5040898606290626e-08\n",
      "Step:849, train: 4.488182361271887e-08\n",
      "Step:850, train: 4.432520528539204e-08\n",
      "Step:851, train: 4.341972251336954e-08\n",
      "Step:852, train: 4.2213457542957316e-08\n",
      "Step:853, train: 4.075499590095474e-08\n",
      "Step:854, train: 3.909063129783623e-08\n",
      "Step:855, train: 3.726397935950847e-08\n",
      "Step:856, train: 3.531532956013959e-08\n",
      "Step:857, train: 3.328272789850463e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:858, train: 4.722138286645633e-08\n",
      "Step:859, train: 3.353955260114587e-08\n",
      "Step:860, train: 3.5394415166302785e-08\n",
      "Step:861, train: 3.676771407421229e-08\n",
      "Step:862, train: 3.767698754034253e-08\n",
      "Step:863, train: 3.8148902976930044e-08\n",
      "Step:864, train: 3.821744548798054e-08\n",
      "Step:865, train: 3.7921098337456534e-08\n",
      "Step:866, train: 3.730074399922431e-08\n",
      "Step:867, train: 3.639847152055358e-08\n",
      "Step:868, train: 3.525665262562082e-08\n",
      "Step:869, train: 3.391609174470846e-08\n",
      "Step:870, train: 3.241644184007955e-08\n",
      "Step:871, train: 3.07942466227404e-08\n",
      "Step:872, train: 5.524906776212834e-08\n",
      "Step:873, train: 3.055907046320729e-08\n",
      "Step:874, train: 3.1623340424886146e-08\n",
      "Step:875, train: 3.229294579197414e-08\n",
      "Step:876, train: 3.259304997212268e-08\n",
      "Step:877, train: 3.255403604048631e-08\n",
      "Step:878, train: 3.2210114578636216e-08\n",
      "Step:879, train: 3.1597372547201324e-08\n",
      "Step:880, train: 3.0752298635818346e-08\n",
      "Step:881, train: 2.971125635154978e-08\n",
      "Step:882, train: 5.0421232294753384e-08\n",
      "Step:883, train: 3.0620664580458965e-08\n",
      "Step:884, train: 3.228378307617282e-08\n",
      "Step:885, train: 3.3503164384876536e-08\n",
      "Step:886, train: 3.4295554364591364e-08\n",
      "Step:887, train: 3.468643300281112e-08\n",
      "Step:888, train: 3.470721726213877e-08\n",
      "Step:889, train: 3.439449468145545e-08\n",
      "Step:890, train: 3.378610859623615e-08\n",
      "Step:891, train: 3.292181274699731e-08\n",
      "Step:892, train: 3.184085877756816e-08\n",
      "Step:893, train: 3.058085018099706e-08\n",
      "Step:894, train: 2.9178511263203364e-08\n",
      "Step:895, train: 4.575623646471381e-08\n",
      "Step:896, train: 2.9691779229315014e-08\n",
      "Step:897, train: 3.1279247988613506e-08\n",
      "Step:898, train: 3.243524296348752e-08\n",
      "Step:899, train: 3.317623703475938e-08\n",
      "Step:900, train: 3.352795350055542e-08\n",
      "Step:901, train: 3.3520947100581204e-08\n",
      "Step:902, train: 3.319114275315747e-08\n",
      "Step:903, train: 3.257607413414045e-08\n",
      "Step:904, train: 3.171453951397882e-08\n",
      "Step:905, train: 3.0644430839256984e-08\n",
      "Step:906, train: 2.9403413574906505e-08\n",
      "Step:907, train: 2.8026613619773884e-08\n",
      "Step:908, train: 4.524535029985646e-08\n",
      "Step:909, train: 2.8378759889051682e-08\n",
      "Step:910, train: 2.979440764649393e-08\n",
      "Step:911, train: 3.0802057430312426e-08\n",
      "Step:912, train: 3.14192026960466e-08\n",
      "Step:913, train: 3.167142218908908e-08\n",
      "Step:914, train: 3.1589461725253046e-08\n",
      "Step:915, train: 3.120767378450652e-08\n",
      "Step:916, train: 3.056269144762076e-08\n",
      "Step:917, train: 2.9691147927826088e-08\n",
      "Step:918, train: 2.8629978837196018e-08\n",
      "Step:919, train: 2.741381371469735e-08\n",
      "Step:920, train: 3.775681421574965e-08\n",
      "Step:921, train: 2.8917351374016556e-08\n",
      "Step:922, train: 3.1301763286292046e-08\n",
      "Step:923, train: 3.3210902536792804e-08\n",
      "Step:924, train: 3.464323881414797e-08\n",
      "Step:925, train: 3.561241528166588e-08\n",
      "Step:926, train: 3.6143137039621656e-08\n",
      "Step:927, train: 3.6267105221183695e-08\n",
      "Step:928, train: 3.602209168380092e-08\n",
      "Step:929, train: 3.544923304113803e-08\n",
      "Step:930, train: 3.459054489188421e-08\n",
      "Step:931, train: 3.34891542607264e-08\n",
      "Step:932, train: 3.218627919329896e-08\n",
      "Step:933, train: 3.072194136754656e-08\n",
      "Step:934, train: 2.9133728612115593e-08\n",
      "Step:935, train: 2.745683228648569e-08\n",
      "Step:936, train: 2.5722623820815048e-08\n",
      "Step:937, train: 4.6718899098497736e-08\n",
      "Step:938, train: 3.2174163722333766e-08\n",
      "Step:939, train: 3.13350579698429e-08\n",
      "Step:940, train: 3.7360084620071434e-08\n",
      "Step:941, train: 4.2831565828948615e-08\n",
      "Step:942, train: 4.762310814212726e-08\n",
      "Step:943, train: 5.1659145100381196e-08\n",
      "Step:944, train: 5.490415782799393e-08\n",
      "Step:945, train: 5.735440249434752e-08\n",
      "Step:946, train: 5.90307231082319e-08\n",
      "Step:947, train: 5.997295372667346e-08\n",
      "Step:948, train: 6.023333319159714e-08\n",
      "Step:949, train: 5.98743458102912e-08\n",
      "Step:950, train: 5.89640118976637e-08\n",
      "Step:951, train: 5.7571906238348e-08\n",
      "Step:952, train: 5.576923198399385e-08\n",
      "Step:953, train: 5.3625699425253506e-08\n",
      "Step:954, train: 5.120771231109025e-08\n",
      "Step:955, train: 4.8578761023751056e-08\n",
      "Step:956, train: 4.579710760476692e-08\n",
      "Step:957, train: 4.29157752645554e-08\n",
      "Step:958, train: 3.998268375638553e-08\n",
      "Step:959, train: 3.7040306885646404e-08\n",
      "Step:960, train: 3.41250869889845e-08\n",
      "Step:961, train: 3.1269107826304024e-08\n",
      "Step:962, train: 2.8498406745519644e-08\n",
      "Step:963, train: 2.5834827749630194e-08\n",
      "Step:964, train: 2.3295379473694407e-08\n",
      "Step:965, train: 5.1487296035429547e-08\n",
      "Step:966, train: 5.183396239879273e-08\n",
      "Step:967, train: 2.7597492255747166e-08\n",
      "Step:968, train: 3.094699557091422e-08\n",
      "Step:969, train: 3.9138867307667504e-08\n",
      "Step:970, train: 4.688453026097531e-08\n",
      "Step:971, train: 5.393491686263381e-08\n",
      "Step:972, train: 6.012224749032396e-08\n",
      "Step:973, train: 6.534399040275805e-08\n",
      "Step:974, train: 6.955126888653046e-08\n",
      "Step:975, train: 7.273703088213542e-08\n",
      "Step:976, train: 7.492606309787322e-08\n",
      "Step:977, train: 7.616825577372752e-08\n",
      "Step:978, train: 7.653044514572722e-08\n",
      "Step:979, train: 7.609240939145647e-08\n",
      "Step:980, train: 7.494128594888655e-08\n",
      "Step:981, train: 7.316797316831846e-08\n",
      "Step:982, train: 7.086395611437024e-08\n",
      "Step:983, train: 6.811921545369492e-08\n",
      "Step:984, train: 6.502027056580424e-08\n",
      "Step:985, train: 6.164878682074723e-08\n",
      "Step:986, train: 5.807996132610745e-08\n",
      "Step:987, train: 5.438331079499207e-08\n",
      "Step:988, train: 5.0620439235510394e-08\n",
      "Step:989, train: 4.684645445291936e-08\n",
      "Step:990, train: 4.3108855559684606e-08\n",
      "Step:991, train: 3.9449264997759273e-08\n",
      "Step:992, train: 3.590163616039749e-08\n",
      "Step:993, train: 3.2493628838554856e-08\n",
      "Step:994, train: 2.9247313574857393e-08\n",
      "Step:995, train: 2.6179776407073316e-08\n",
      "Step:996, train: 2.3302515178019847e-08\n",
      "Step:997, train: 2.0622865476269995e-08\n",
      "Step:998, train: 4.783826353271629e-08\n",
      "Step:999, train: 5.10964455473193e-08\n",
      "5.10964455473193e-08\n"
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting General Lindbladian to General Kraus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "d = 2**n\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "channel_target = KrausMap(d=d, rank=d**2, trainable=False)\n",
    "channel_model = LindbladMap(d=d, rank=d**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelQuantumMap(\n",
    "                        channel =channel_model,\n",
    "                        loss_function = channel_mse_loss,\n",
    "                        optimizer = tf.optimizers.Adam(learning_rate=0.01),\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcbe25b608f428bb7debcc507e63d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: 1.0126534226086128\n",
      "Step:1, train: 0.9844042464589062\n",
      "Step:2, train: 0.9588133817304019\n",
      "Step:3, train: 0.9365532489062811\n",
      "Step:4, train: 0.9180033382391217\n",
      "Step:5, train: 0.90277409335958\n",
      "Step:6, train: 0.8900186243815834\n",
      "Step:7, train: 0.8789752419070118\n",
      "Step:8, train: 0.8691530209672034\n",
      "Step:9, train: 0.8602809569577109\n",
      "Step:10, train: 0.8522146784377708\n",
      "Step:11, train: 0.8448695050345141\n",
      "Step:12, train: 0.8381809128667785\n",
      "Step:13, train: 0.8320844461060957\n",
      "Step:14, train: 0.8265096245907764\n",
      "Step:15, train: 0.8213839255317674\n",
      "Step:16, train: 0.8166423176452874\n",
      "Step:17, train: 0.8122365504099218\n",
      "Step:18, train: 0.8081394332242903\n",
      "Step:19, train: 0.8043428190811499\n",
      "Step:20, train: 0.8008507878868987\n",
      "Step:21, train: 0.7976704726219104\n",
      "Step:22, train: 0.7948027590282815\n",
      "Step:23, train: 0.7922350390046051\n",
      "Step:24, train: 0.789937767315096\n",
      "Step:25, train: 0.7878658755240193\n",
      "Step:26, train: 0.7859649315732367\n",
      "Step:27, train: 0.7841801715457983\n",
      "Step:28, train: 0.7824654072551673\n",
      "Step:29, train: 0.7807888458972689\n",
      "Step:30, train: 0.7791341069457465\n",
      "Step:31, train: 0.7774966737086692\n",
      "Step:32, train: 0.7758776572590441\n",
      "Step:33, train: 0.774277642681321\n",
      "Step:34, train: 0.7726929830078897\n",
      "Step:35, train: 0.7711154082526214\n",
      "Step:36, train: 0.7695340951072397\n",
      "Step:37, train: 0.7679383733090032\n",
      "Step:38, train: 0.76631948456814\n",
      "Step:39, train: 0.7646709293483458\n",
      "Step:40, train: 0.7629878267970247\n",
      "Step:41, train: 0.7612661061966799\n",
      "Step:42, train: 0.759502109330402\n",
      "Step:43, train: 0.7576926996314094\n",
      "Step:44, train: 0.7558356183313302\n",
      "Step:45, train: 0.7539297236785436\n",
      "Step:46, train: 0.7519749226624893\n",
      "Step:47, train: 0.7499717263580741\n",
      "Step:48, train: 0.7479206773199285\n",
      "Step:49, train: 0.7458218197386928\n",
      "Step:50, train: 0.7436744090090721\n",
      "Step:51, train: 0.7414769524604812\n",
      "Step:52, train: 0.7392275477091066\n",
      "Step:53, train: 0.7369243309816447\n",
      "Step:54, train: 0.7345658863664297\n",
      "Step:55, train: 0.732151406009607\n",
      "Step:56, train: 0.729680589294331\n",
      "Step:57, train: 0.7271533942962397\n",
      "Step:58, train: 0.7246472721267103\n",
      "Step:59, train: 0.7222966912728326\n",
      "Step:60, train: 0.7198748339461667\n",
      "Step:61, train: 0.7173821398781342\n",
      "Step:62, train: 0.7148204777509302\n",
      "Step:63, train: 0.7121938271282682\n",
      "Step:64, train: 0.7095082908837032\n",
      "Step:65, train: 0.7067715091216236\n",
      "Step:66, train: 0.7039919370686156\n",
      "Step:67, train: 0.7011782580198502\n",
      "Step:68, train: 0.698339055429692\n",
      "Step:69, train: 0.695505072959417\n",
      "Step:70, train: 0.6928439815754831\n",
      "Step:71, train: 0.6901581805577786\n",
      "Step:72, train: 0.6878269154310833\n",
      "Step:73, train: 0.6854583788082229\n",
      "Step:74, train: 0.6830469906456761\n",
      "Step:75, train: 0.6809627546258803\n",
      "Step:76, train: 0.6790269908176236\n",
      "Step:77, train: 0.6770807864261261\n",
      "Step:78, train: 0.6750549410025277\n",
      "Step:79, train: 0.6729580853620906\n",
      "Step:80, train: 0.6707950784523782\n",
      "Step:81, train: 0.6685986835813329\n",
      "Step:82, train: 0.6664230785271181\n",
      "Step:83, train: 0.6641841239730677\n",
      "Step:84, train: 0.6618925364841926\n",
      "Step:85, train: 0.6596253241439273\n",
      "Step:86, train: 0.6573397575935425\n",
      "Step:87, train: 0.6550859091558818\n",
      "Step:88, train: 0.6528859181843305\n",
      "Step:89, train: 0.6507021143433862\n",
      "Step:90, train: 0.6486211164537827\n",
      "Step:91, train: 0.6465802888015243\n",
      "Step:92, train: 0.6444511845193259\n",
      "Step:93, train: 0.6422690107521223\n",
      "Step:94, train: 0.6400663256149507\n",
      "Step:95, train: 0.6377827606230182\n",
      "Step:96, train: 0.6354244209486828\n",
      "Step:97, train: 0.6330398062313861\n",
      "Step:98, train: 0.6307279479890524\n",
      "Step:99, train: 0.6283438811784945\n",
      "Step:100, train: 0.6259361422589234\n",
      "Step:101, train: 0.623591432669606\n",
      "Step:102, train: 0.6211528667347848\n",
      "Step:103, train: 0.618642852118563\n",
      "Step:104, train: 0.6160821283003141\n",
      "Step:105, train: 0.613423055947337\n",
      "Step:106, train: 0.6106900065996355\n",
      "Step:107, train: 0.6081360780265307\n",
      "Step:108, train: 0.6055045011725295\n",
      "Step:109, train: 0.6027546517975357\n",
      "Step:110, train: 0.5998920351350308\n",
      "Step:111, train: 0.5969990109120084\n",
      "Step:112, train: 0.5941244877735876\n",
      "Step:113, train: 0.5912818361450056\n",
      "Step:114, train: 0.5883321485782176\n",
      "Step:115, train: 0.5852956521180601\n",
      "Step:116, train: 0.5822075948433687\n",
      "Step:117, train: 0.5790426775964791\n",
      "Step:118, train: 0.5758572393280665\n",
      "Step:119, train: 0.5727811090903705\n",
      "Step:120, train: 0.5696465289379675\n",
      "Step:121, train: 0.5664622865714146\n",
      "Step:122, train: 0.5632721393645281\n",
      "Step:123, train: 0.560012898021877\n",
      "Step:124, train: 0.5566989265737214\n",
      "Step:125, train: 0.5535311402964269\n",
      "Step:126, train: 0.5503812326399857\n",
      "Step:127, train: 0.5472034962585466\n",
      "Step:128, train: 0.5440223398654229\n",
      "Step:129, train: 0.5408170904302956\n",
      "Step:130, train: 0.5376180018911652\n",
      "Step:131, train: 0.5345620449532635\n",
      "Step:132, train: 0.5315167389538409\n",
      "Step:133, train: 0.5289740665148985\n",
      "Step:134, train: 0.5263717089193503\n",
      "Step:135, train: 0.5237521233508282\n",
      "Step:136, train: 0.5210508198077436\n",
      "Step:137, train: 0.5184648375438632\n",
      "Step:138, train: 0.5157904812531625\n",
      "Step:139, train: 0.513189357263966\n",
      "Step:140, train: 0.5104952006523671\n",
      "Step:141, train: 0.5076995525578282\n",
      "Step:142, train: 0.5050548321878996\n",
      "Step:143, train: 0.5026732044788\n",
      "Step:144, train: 0.5004366326312588\n",
      "Step:145, train: 0.4982043081312084\n",
      "Step:146, train: 0.4959039209592358\n",
      "Step:147, train: 0.49353344422020046\n",
      "Step:148, train: 0.49130109728234994\n",
      "Step:149, train: 0.4894067864545654\n",
      "Step:150, train: 0.48765591933705754\n",
      "Step:151, train: 0.4859477109267083\n",
      "Step:152, train: 0.4834762286243729\n",
      "Step:153, train: 0.48101905691030383\n",
      "Step:154, train: 0.47928492505071657\n",
      "Step:155, train: 0.4770379818822864\n",
      "Step:156, train: 0.4756016042209337\n",
      "Step:157, train: 0.473579131322832\n",
      "Step:158, train: 0.4723757482410409\n",
      "Step:159, train: 0.47077793816566416\n",
      "Step:160, train: 0.469246397636364\n",
      "Step:161, train: 0.4674941869929429\n",
      "Step:162, train: 0.4657256823521316\n",
      "Step:163, train: 0.464240417777878\n",
      "Step:164, train: 0.46256476874026975\n",
      "Step:165, train: 0.4610663433664325\n",
      "Step:166, train: 0.4596262675086137\n",
      "Step:167, train: 0.45831626748438703\n",
      "Step:168, train: 0.457126105727264\n",
      "Step:169, train: 0.45588551839102087\n",
      "Step:170, train: 0.45467288353527224\n",
      "Step:171, train: 0.45345763331792144\n",
      "Step:172, train: 0.4521363970068445\n",
      "Step:173, train: 0.4511517340828636\n",
      "Step:174, train: 0.4501495201820076\n",
      "Step:175, train: 0.4490098237058578\n",
      "Step:176, train: 0.44798027775232374\n",
      "Step:177, train: 0.44700775201103293\n",
      "Step:178, train: 0.44596314231386314\n",
      "Step:179, train: 0.4449700118973174\n",
      "Step:180, train: 0.4439829055569947\n",
      "Step:181, train: 0.4430499338537113\n",
      "Step:182, train: 0.4421097115217404\n",
      "Step:183, train: 0.4414080719125519\n",
      "Step:184, train: 0.44061049397149976\n",
      "Step:185, train: 0.43980766813248023\n",
      "Step:186, train: 0.4389867275550523\n",
      "Step:187, train: 0.43823358581278726\n",
      "Step:188, train: 0.4375007018072026\n",
      "Step:189, train: 0.4367896509296557\n",
      "Step:190, train: 0.4360570984983358\n",
      "Step:191, train: 0.4353733237711358\n",
      "Step:192, train: 0.4346604079098742\n",
      "Step:193, train: 0.4340917905207171\n",
      "Step:194, train: 0.43355800636981706\n",
      "Step:195, train: 0.4330545677969878\n",
      "Step:196, train: 0.4324563394267623\n",
      "Step:197, train: 0.4317697552467725\n",
      "Step:198, train: 0.43127015599780916\n",
      "Step:199, train: 0.4309003014684366\n",
      "Step:200, train: 0.4303341614767917\n",
      "Step:201, train: 0.42969613481645436\n",
      "Step:202, train: 0.42924566289989285\n",
      "Step:203, train: 0.4288762684633971\n",
      "Step:204, train: 0.42835680306178736\n",
      "Step:205, train: 0.4278003951040057\n",
      "Step:206, train: 0.42729527089113073\n",
      "Step:207, train: 0.4268572310345461\n",
      "Step:208, train: 0.4264253897888909\n",
      "Step:209, train: 0.42592998102449586\n",
      "Step:210, train: 0.4255629973986743\n",
      "Step:211, train: 0.4251215006979071\n",
      "Step:212, train: 0.42460620874778154\n",
      "Step:213, train: 0.42425630951636817\n",
      "Step:214, train: 0.42383496629052997\n",
      "Step:215, train: 0.4233430310933311\n",
      "Step:216, train: 0.4230166520870642\n",
      "Step:217, train: 0.4226205489628081\n",
      "Step:218, train: 0.4222015904488396\n",
      "Step:219, train: 0.42180774856457914\n",
      "Step:220, train: 0.4214454810901065\n",
      "Step:221, train: 0.42104082400628345\n",
      "Step:222, train: 0.420640677956297\n",
      "Step:223, train: 0.42038933267373335\n",
      "Step:224, train: 0.4200089892516339\n",
      "Step:225, train: 0.4197021756800693\n",
      "Step:226, train: 0.4193694006283294\n",
      "Step:227, train: 0.41889257074317954\n",
      "Step:228, train: 0.4185033849478951\n",
      "Step:229, train: 0.4181905757223245\n",
      "Step:230, train: 0.4178883889350504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:231, train: 0.417572169859164\n",
      "Step:232, train: 0.4171926443033998\n",
      "Step:233, train: 0.41683206999587014\n",
      "Step:234, train: 0.41667042418907885\n",
      "Step:235, train: 0.4164073538503671\n",
      "Step:236, train: 0.41605780802510095\n",
      "Step:237, train: 0.41568790970945146\n",
      "Step:238, train: 0.41556986716845545\n",
      "Step:239, train: 0.41537094319927537\n",
      "Step:240, train: 0.4150562339818574\n",
      "Step:241, train: 0.41470847361814134\n",
      "Step:242, train: 0.4142607984178385\n",
      "Step:243, train: 0.4141188036578436\n",
      "Step:244, train: 0.41386354017184834\n",
      "Step:245, train: 0.41357627124952984\n",
      "Step:246, train: 0.41340121151853043\n",
      "Step:247, train: 0.4130580637108946\n",
      "Step:248, train: 0.4128228331626406\n",
      "Step:249, train: 0.41269286860344123\n",
      "Step:250, train: 0.4124365776576283\n",
      "Step:251, train: 0.4120936997717535\n",
      "Step:252, train: 0.41180770911029896\n",
      "Step:253, train: 0.41159483359333826\n",
      "Step:254, train: 0.4114262385482001\n",
      "Step:255, train: 0.4111684789169252\n",
      "Step:256, train: 0.4109251155925787\n",
      "Step:257, train: 0.41062226542771924\n",
      "Step:258, train: 0.41040898590207164\n",
      "Step:259, train: 0.41022768166591916\n",
      "Step:260, train: 0.4100155494486566\n",
      "Step:261, train: 0.4098188538114836\n",
      "Step:262, train: 0.40953193517852426\n",
      "Step:263, train: 0.4093287055161826\n",
      "Step:264, train: 0.4092117752234879\n",
      "Step:265, train: 0.4090442263642643\n",
      "Step:266, train: 0.4087939742266262\n",
      "Step:267, train: 0.4085594724242152\n",
      "Step:268, train: 0.40841570064643407\n",
      "Step:269, train: 0.4084904674371711\n",
      "Step:270, train: 0.4084679515369799\n",
      "Step:271, train: 0.40810929285031516\n",
      "Step:272, train: 0.4075368129841834\n",
      "Step:273, train: 0.4075294412708965\n",
      "Step:274, train: 0.4076416095590905\n",
      "Step:275, train: 0.40735833167991153\n",
      "Step:276, train: 0.4069088137045821\n",
      "Step:277, train: 0.40674219240404996\n",
      "Step:278, train: 0.40677344340406074\n",
      "Step:279, train: 0.40660696813493513\n",
      "Step:280, train: 0.40630894891894465\n",
      "Step:281, train: 0.40618312839555437\n",
      "Step:282, train: 0.40606423479934173\n",
      "Step:283, train: 0.4059082320840781\n",
      "Step:284, train: 0.4056828349228017\n",
      "Step:285, train: 0.40564366600240825\n",
      "Step:286, train: 0.4054953124598797\n",
      "Step:287, train: 0.4051841487116384\n",
      "Step:288, train: 0.40506137740998216\n",
      "Step:289, train: 0.40501565603178974\n",
      "Step:290, train: 0.4048536422208783\n",
      "Step:291, train: 0.4047274564630579\n",
      "Step:292, train: 0.4045593038303785\n",
      "Step:293, train: 0.40448426753484296\n",
      "Step:294, train: 0.4043308913174439\n",
      "Step:295, train: 0.4042716905589942\n",
      "Step:296, train: 0.4040684725286041\n",
      "Step:297, train: 0.4038910945517973\n",
      "Step:298, train: 0.4037428359732571\n",
      "Step:299, train: 0.4036436160250956\n",
      "Step:300, train: 0.4036220016867978\n",
      "Step:301, train: 0.4034449722796762\n",
      "Step:302, train: 0.40325964470302883\n",
      "Step:303, train: 0.40318041671085136\n",
      "Step:304, train: 0.403169762364076\n",
      "Step:305, train: 0.4030769215926793\n",
      "Step:306, train: 0.402940573905371\n",
      "Step:307, train: 0.40270403474628214\n",
      "Step:308, train: 0.4025257290666734\n",
      "Step:309, train: 0.4025286065874745\n",
      "Step:310, train: 0.40247895358089314\n",
      "Step:311, train: 0.4023513379639784\n",
      "Step:312, train: 0.40216412819029107\n",
      "Step:313, train: 0.4020855989004489\n",
      "Step:314, train: 0.40203294015921487\n",
      "Step:315, train: 0.4020322974696128\n",
      "Step:316, train: 0.40188644912235616\n",
      "Step:317, train: 0.4017080549462114\n",
      "Step:318, train: 0.4015998442454288\n",
      "Step:319, train: 0.4016234168744536\n",
      "Step:320, train: 0.4015176497890963\n",
      "Step:321, train: 0.40139294552671406\n",
      "Step:322, train: 0.40131924095970195\n",
      "Step:323, train: 0.4012576573663521\n",
      "Step:324, train: 0.401219047869931\n",
      "Step:325, train: 0.4010038916499602\n",
      "Step:326, train: 0.4008422433907226\n",
      "Step:327, train: 0.40072710163329384\n",
      "Step:328, train: 0.40071483662075413\n",
      "Step:329, train: 0.40072842328680247\n",
      "Step:330, train: 0.40069018641611726\n",
      "Step:331, train: 0.40051515738821775\n",
      "Step:332, train: 0.40022829427180073\n",
      "Step:333, train: 0.4001295718388482\n",
      "Step:334, train: 0.40020272799170237\n",
      "Step:335, train: 0.4001775485905016\n",
      "Step:336, train: 0.40001244037451555\n",
      "Step:337, train: 0.399817000230193\n",
      "Step:338, train: 0.399868449795978\n",
      "Step:339, train: 0.39987679215843874\n",
      "Step:340, train: 0.3997444450625992\n",
      "Step:341, train: 0.3996612597445297\n",
      "Step:342, train: 0.3995557865284801\n",
      "Step:343, train: 0.3995125916346275\n",
      "Step:344, train: 0.3994019562281836\n",
      "Step:345, train: 0.3992889307132049\n",
      "Step:346, train: 0.39928441538861054\n",
      "Step:347, train: 0.3991840078857527\n",
      "Step:348, train: 0.3992313994696741\n",
      "Step:349, train: 0.3991701606906283\n",
      "Step:350, train: 0.39905074673746277\n",
      "Step:351, train: 0.3989038306685288\n",
      "Step:352, train: 0.39880970289749956\n",
      "Step:353, train: 0.3987000965139272\n",
      "Step:354, train: 0.39863773044195117\n",
      "Step:355, train: 0.3986156377616833\n",
      "Step:356, train: 0.39852359881659616\n",
      "Step:357, train: 0.3983839093555587\n",
      "Step:358, train: 0.3984140380486423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel_target\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\optimization.py:77\u001b[0m, in \u001b[0;36mModelQuantumMap.train\u001b[1;34m(self, inputs, targets, inputs_val, targets_val, num_iter, N)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(watch_accessed_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     76\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list)\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loss_function \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function: \n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:381\u001b[0m, in \u001b[0;36mLindbladMap.generate_channel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, L \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(L_list):\n\u001b[0;32m    380\u001b[0m     L2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(L, L, adjoint_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 381\u001b[0m     LB \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gamma[i]\u001b[38;5;241m*\u001b[39m(\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m((kron(L2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI) \u001b[38;5;241m+\u001b[39m kron(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI, tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mconj(L2)))))\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuper_operator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mexpm(LB)\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\utils.py:31\u001b[0m, in \u001b[0;36mkron\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     29\u001b[0m A \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, length):\n\u001b[1;32m---> 31\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:408\u001b[0m, in \u001b[0;36mkron\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    406\u001b[0m a_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(t_a)\n\u001b[0;32m    407\u001b[0m b_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(t_b)\n\u001b[1;32m--> 408\u001b[0m a_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mnp_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_make_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m b_reshaped \u001b[38;5;241m=\u001b[39m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(t_b, _make_shape(b_shape, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    410\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m a_shape \u001b[38;5;241m*\u001b[39m b_shape\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py:758\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    755\u001b[0m   r \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    756\u001b[0m       array_ops\u001b[38;5;241m.\u001b[39mreshape(array_ops\u001b[38;5;241m.\u001b[39mtranspose(a), newshape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:199\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanip.reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(tensor, shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m   tensor_util\u001b[38;5;241m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8537\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   8536\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 8537\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8538\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   8540\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [channel_target],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "d = 2**n\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "channel_model = KrausMap(d=d, rank=d**2)\n",
    "channel_target = LindbladMap(d=d, rank=d**2, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelQuantumMap(\n",
    "                        channel =channel_model,\n",
    "                        loss_function = channel_mse_loss,\n",
    "                        optimizer = tf.optimizers.Adam(learning_rate=0.01),\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92cde9f714244259a063a2b21cc7c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: -0.7262515546695187\n",
      "Step:1, train: -0.7275341116324222\n",
      "Step:2, train: -0.7287210593739565\n",
      "Step:3, train: -0.7298014922844818\n",
      "Step:4, train: -0.7307802383495227\n",
      "Step:5, train: -0.731658197116482\n",
      "Step:6, train: -0.7324374860730207\n",
      "Step:7, train: -0.7331210760998492\n",
      "Step:8, train: -0.7337121364750314\n",
      "Step:9, train: -0.7342139083008425\n",
      "Step:10, train: -0.7346298102204356\n",
      "Step:11, train: -0.7349634188202269\n",
      "Step:12, train: -0.7352183036684636\n",
      "Step:13, train: -0.7353978082227073\n",
      "Step:14, train: -0.7355048713135769\n",
      "Step:15, train: -0.7355419560831509\n",
      "Step:16, train: -0.7355110519685644\n",
      "Step:17, train: -0.7354137066111561\n",
      "Step:18, train: -0.7352510819175846\n",
      "Step:19, train: -0.7350239889662018\n",
      "Step:20, train: -0.7347329433195728\n",
      "Step:21, train: -0.7343781906166079\n",
      "Step:22, train: -0.7339597469602872\n",
      "Step:23, train: -0.7334774016692225\n",
      "Step:24, train: -0.7329307459113651\n",
      "Step:25, train: -0.7323191612110793\n",
      "Step:26, train: -0.7316418332121535\n",
      "Step:27, train: -0.7308977747938836\n",
      "Step:28, train: -0.730085843485828\n",
      "Step:29, train: -0.7292047630915838\n",
      "Step:30, train: -0.728253165852564\n",
      "Step:31, train: -0.7272296044052627\n",
      "Step:32, train: -0.7261325908698956\n",
      "Step:33, train: -0.7249606024547648\n",
      "Step:34, train: -0.7237121046151322\n",
      "Step:35, train: -0.7223855484410306\n",
      "Step:36, train: -0.7209793729926232\n",
      "Step:37, train: -0.7194919879454835\n",
      "Step:38, train: -0.7179217851777011\n",
      "Step:39, train: -0.716267120246736\n",
      "Step:40, train: -0.7145263239120976\n",
      "Step:41, train: -0.7126976964648242\n",
      "Step:42, train: -0.7107795233638804\n",
      "Step:43, train: -0.70877008599738\n",
      "Step:44, train: -0.7066676643543827\n",
      "Step:45, train: -0.7044705495441206\n",
      "Step:46, train: -0.7021770379588272\n",
      "Step:47, train: -0.6997854351573896\n",
      "Step:48, train: -0.697294037579898\n",
      "Step:49, train: -0.6947011604992213\n",
      "Step:50, train: -0.6920051222448849\n",
      "Step:51, train: -0.6892042565489512\n",
      "Step:52, train: -0.6862969526406704\n",
      "Step:53, train: -0.6832816737948381\n",
      "Step:54, train: -0.6801570093622146\n",
      "Step:55, train: -0.6769217100638208\n",
      "Step:56, train: -0.6735747398015454\n",
      "Step:57, train: -0.670115315457959\n",
      "Step:58, train: -0.666542924587432\n",
      "Step:59, train: -0.66285736240263\n",
      "Step:60, train: -0.6590587466139605\n",
      "Step:61, train: -0.6551475451617964\n",
      "Step:62, train: -0.6511246310597022\n",
      "Step:63, train: -0.6469913653954606\n",
      "Step:64, train: -0.6427497037223477\n",
      "Step:65, train: -0.638402354000732\n",
      "Step:66, train: -0.633952923567741\n",
      "Step:67, train: -0.6294060848663503\n",
      "Step:68, train: -0.6247677196588204\n",
      "Step:69, train: -0.6200450568450997\n",
      "Step:70, train: -0.6152467882322474\n",
      "Step:71, train: -0.6103831276514747\n",
      "Step:72, train: -0.6054657623209354\n",
      "Step:73, train: -0.6005076714018611\n",
      "Step:74, train: -0.5955228208233609\n",
      "Step:75, train: -0.5905257624756969\n",
      "Step:76, train: -0.5855312165876093\n",
      "Step:77, train: -0.5805536209058211\n",
      "Step:78, train: -0.575606700661081\n",
      "Step:79, train: -0.5707030490998443\n",
      "Step:80, train: -0.5658537389374133\n",
      "Step:81, train: -0.561067934311241\n",
      "Step:82, train: -0.5563525874774098\n",
      "Step:83, train: -0.5517121868823757\n",
      "Step:84, train: -0.5471485997014142\n",
      "Step:85, train: -0.5426609110246751\n",
      "Step:86, train: -0.5382452284762695\n",
      "Step:87, train: -0.5338944295640998\n",
      "Step:88, train: -0.529597932930571\n",
      "Step:89, train: -0.5253415621071443\n",
      "Step:90, train: -0.5211075281957688\n",
      "Step:91, train: -0.5168745428377717\n",
      "Step:92, train: -0.512618418197129\n",
      "Step:93, train: -0.5083140681406534\n",
      "Step:94, train: -0.5039427195647006\n",
      "Step:95, train: -0.4995210580852593\n",
      "Step:96, train: -0.4952059019424799\n",
      "Step:97, train: -0.49119904092235217\n",
      "Step:98, train: -0.48749033173336176\n",
      "Step:99, train: -0.4841002982406317\n",
      "Step:100, train: -0.4810372408639438\n",
      "Step:101, train: -0.4782935140321597\n",
      "Step:102, train: -0.4758524827999054\n",
      "Step:103, train: -0.47369324894531784\n",
      "Step:104, train: -0.47179293433092506\n",
      "Step:105, train: -0.4701272140136825\n",
      "Step:106, train: -0.468670507944267\n",
      "Step:107, train: -0.467396661269696\n",
      "Step:108, train: -0.4662801156275342\n",
      "Step:109, train: -0.46529714555436497\n",
      "Step:110, train: -0.4644267477912204\n",
      "Step:111, train: -0.4636511851149938\n",
      "Step:112, train: -0.4629562427997477\n",
      "Step:113, train: -0.4623313618325855\n",
      "Step:114, train: -0.4617698001213379\n",
      "Step:115, train: -0.4612688672226475\n",
      "Step:116, train: -0.46083030322871266\n",
      "Step:117, train: -0.4604608034432618\n",
      "Step:118, train: -0.46017269085215734\n",
      "Step:119, train: -0.4599846834306525\n",
      "Step:120, train: -0.45992276972190976\n",
      "Step:121, train: -0.46002120337392566\n",
      "Step:122, train: -0.46032220551656994\n",
      "Step:123, train: -0.46086909204695115\n",
      "Step:124, train: -0.4616838876509895\n",
      "Step:125, train: -0.46273008481562167\n",
      "Step:126, train: -0.4639313169864778\n",
      "Step:127, train: -0.4652192314225343\n",
      "Step:128, train: -0.46654449046596264\n",
      "Step:129, train: -0.46787227042269147\n",
      "Step:130, train: -0.4691775088131797\n",
      "Step:131, train: -0.4704422533676567\n",
      "Step:132, train: -0.4716546783409313\n",
      "Step:133, train: -0.4728086423883726\n",
      "Step:134, train: -0.4739029202155438\n",
      "Step:135, train: -0.4749398910955241\n",
      "Step:136, train: -0.47592396629821043\n",
      "Step:137, train: -0.4768600559306603\n",
      "Step:138, train: -0.4777522776576928\n",
      "Step:139, train: -0.47860302946123123\n",
      "Step:140, train: -0.47941251576395116\n",
      "Step:141, train: -0.4801786829273769\n",
      "Step:142, train: -0.4808975740505068\n",
      "Step:143, train: -0.4815639233276408\n",
      "Step:144, train: -0.4821718294538825\n",
      "Step:145, train: -0.48271539052096857\n",
      "Step:146, train: -0.48318918258931864\n",
      "Step:147, train: -0.4835885935518964\n",
      "Step:148, train: -0.4839100189403453\n",
      "Step:149, train: -0.48415093324125813\n",
      "Step:150, train: -0.48430988607363396\n",
      "Step:151, train: -0.48438644430085026\n",
      "Step:152, train: -0.4843810873223237\n",
      "Step:153, train: -0.484295088795516\n",
      "Step:154, train: -0.4841303893046318\n",
      "Step:155, train: -0.48388946187838333\n",
      "Step:156, train: -0.4835751971516429\n",
      "Step:157, train: -0.4831907686737744\n",
      "Step:158, train: -0.4827395360840817\n",
      "Step:159, train: -0.48222495587036573\n",
      "Step:160, train: -0.4816504888961856\n",
      "Step:161, train: -0.48101954610804387\n",
      "Step:162, train: -0.4803354245716729\n",
      "Step:163, train: -0.47960127191958696\n",
      "Step:164, train: -0.4788200447330761\n",
      "Step:165, train: -0.47799448915862397\n",
      "Step:166, train: -0.4771271232830713\n",
      "Step:167, train: -0.47622022522762203\n",
      "Step:168, train: -0.4752758351249658\n",
      "Step:169, train: -0.47429574521541396\n",
      "Step:170, train: -0.4732815107040321\n",
      "Step:171, train: -0.4722344470592998\n",
      "Step:172, train: -0.4711556434411385\n",
      "Step:173, train: -0.470045963430337\n",
      "Step:174, train: -0.46890606553786485\n",
      "Step:175, train: -0.46773640053564114\n",
      "Step:176, train: -0.46653722757749977\n",
      "Step:177, train: -0.4653086189132034\n",
      "Step:178, train: -0.464050480282359\n",
      "Step:179, train: -0.46276254548955653\n",
      "Step:180, train: -0.4614444005706835\n",
      "Step:181, train: -0.4600954908579259\n",
      "Step:182, train: -0.458715137017227\n",
      "Step:183, train: -0.4573025517696235\n",
      "Step:184, train: -0.45585685868128706\n",
      "Step:185, train: -0.45437712178330514\n",
      "Step:186, train: -0.45286238277947277\n",
      "Step:187, train: -0.4513116982675432\n",
      "Step:188, train: -0.449724200413888\n",
      "Step:189, train: -0.44809917187973347\n",
      "Step:190, train: -0.4464361101945808\n",
      "Step:191, train: -0.4447348419886332\n",
      "Step:192, train: -0.4429956137432367\n",
      "Step:193, train: -0.44121919990737807\n",
      "Step:194, train: -0.43940700409811156\n",
      "Step:195, train: -0.4375611240721561\n",
      "Step:196, train: -0.43568440489757326\n",
      "Step:197, train: -0.43378041689009406\n",
      "Step:198, train: -0.43185341027914753\n",
      "Step:199, train: -0.4299082283122798\n",
      "Step:200, train: -0.42795016140794306\n",
      "Step:201, train: -0.4259847903314328\n",
      "Step:202, train: -0.4240178422312807\n",
      "Step:203, train: -0.4220550171014141\n",
      "Step:204, train: -0.4201018626638351\n",
      "Step:205, train: -0.41816365769295705\n",
      "Step:206, train: -0.41624533346324344\n",
      "Step:207, train: -0.4143514230376777\n",
      "Step:208, train: -0.41248604077705986\n",
      "Step:209, train: -0.41065288115865195\n",
      "Step:210, train: -0.4088552401729039\n",
      "Step:211, train: -0.4070960437626165\n",
      "Step:212, train: -0.40537790519963063\n",
      "Step:213, train: -0.4037031717929135\n",
      "Step:214, train: -0.40207398640441805\n",
      "Step:215, train: -0.4004923616525399\n",
      "Step:216, train: -0.398960262955411\n",
      "Step:217, train: -0.3974797079104705\n",
      "Step:218, train: -0.39605289871567506\n",
      "Step:219, train: -0.39468241228629225\n",
      "Step:220, train: -0.39337147761723146\n",
      "Step:221, train: -0.3921244007453365\n",
      "Step:222, train: -0.390947206630785\n",
      "Step:223, train: -0.38984857950180873\n",
      "Step:224, train: -0.38884100042863307\n",
      "Step:225, train: -0.3879415247209667\n",
      "Step:226, train: -0.3871704328473002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:227, train: -0.3865454760813683\n",
      "Step:228, train: -0.38607369291428406\n",
      "Step:229, train: -0.3857481311180063\n",
      "Step:230, train: -0.38555127184590005\n",
      "Step:231, train: -0.3854597828578683\n",
      "Step:232, train: -0.38544775101893286\n",
      "Step:233, train: -0.3854887522528044\n",
      "Step:234, train: -0.3855573344291852\n",
      "Step:235, train: -0.38563005452282056\n",
      "Step:236, train: -0.3856860317131165\n",
      "Step:237, train: -0.3857071180277573\n",
      "Step:238, train: -0.3856778683584834\n",
      "Step:239, train: -0.38558542897531967\n",
      "Step:240, train: -0.3854195233337695\n",
      "Step:241, train: -0.3851725142020819\n",
      "Step:242, train: -0.38483956577145656\n",
      "Step:243, train: -0.3844187808814896\n",
      "Step:244, train: -0.3839111362417386\n",
      "Step:245, train: -0.38332005353360843\n",
      "Step:246, train: -0.38265063711641706\n",
      "Step:247, train: -0.3819087659678411\n",
      "Step:248, train: -0.38110043615194583\n",
      "Step:249, train: -0.38023157200706154\n",
      "Step:250, train: -0.37930830219788525\n",
      "Step:251, train: -0.3783374334812629\n",
      "Step:252, train: -0.37732684455179744\n",
      "Step:253, train: -0.37628561741094774\n",
      "Step:254, train: -0.3752238734344088\n",
      "Step:255, train: -0.3741522958645037\n",
      "Step:256, train: -0.3730812045637248\n",
      "Step:257, train: -0.3720190827122029\n",
      "Step:258, train: -0.3709706642692362\n",
      "Step:259, train: -0.369935594587989\n",
      "Step:260, train: -0.3689092139971504\n",
      "Step:261, train: -0.36788618447710403\n",
      "Step:262, train: -0.36686653740742137\n",
      "Step:263, train: -0.3658643083669691\n",
      "Step:264, train: -0.36491666054175564\n",
      "Step:265, train: -0.3640753840888362\n",
      "Step:266, train: -0.36337259769206565\n",
      "Step:267, train: -0.3628170524821827\n",
      "Step:268, train: -0.36242182977918025\n",
      "Step:269, train: -0.36220907668303337\n",
      "Step:270, train: -0.3621914810518875\n",
      "Step:271, train: -0.3623766761077561\n",
      "Step:272, train: -0.3627840175408448\n",
      "Step:273, train: -0.36333424267724057\n",
      "Step:274, train: -0.3638994170961061\n",
      "Step:275, train: -0.3644179047217948\n",
      "Step:276, train: -0.3648654681019459\n",
      "Step:277, train: -0.36523431185362276\n",
      "Step:278, train: -0.3655245653936796\n",
      "Step:279, train: -0.36574029761072613\n",
      "Step:280, train: -0.3658874288851632\n",
      "Step:281, train: -0.36597256208701073\n",
      "Step:282, train: -0.36600232607801736\n",
      "Step:283, train: -0.36598301685461204\n",
      "Step:284, train: -0.3659204021174667\n",
      "Step:285, train: -0.36581965554303697\n",
      "Step:286, train: -0.3656853385743179\n",
      "Step:287, train: -0.36552141337892374\n",
      "Step:288, train: -0.3653313005055854\n",
      "Step:289, train: -0.36511791481303774\n",
      "Step:290, train: -0.36488372964769084\n",
      "Step:291, train: -0.3646308216425254\n",
      "Step:292, train: -0.36436093038225\n",
      "Step:293, train: -0.36407549691070595\n",
      "Step:294, train: -0.36377570908944556\n",
      "Step:295, train: -0.36346253720813787\n",
      "Step:296, train: -0.36313676640178194\n",
      "Step:297, train: -0.3627990312664163\n",
      "Step:298, train: -0.36244982415354865\n",
      "Step:299, train: -0.3620895330644449\n",
      "Step:300, train: -0.36171845225015986\n",
      "Step:301, train: -0.3613368004940564\n",
      "Step:302, train: -0.360944733319736\n",
      "Step:303, train: -0.3605423515508271\n",
      "Step:304, train: -0.3601297179624629\n",
      "Step:305, train: -0.35970685238692934\n",
      "Step:306, train: -0.35927374813262547\n",
      "Step:307, train: -0.35883037480802277\n",
      "Step:308, train: -0.35837668021415175\n",
      "Step:309, train: -0.3579125913089052\n",
      "Step:310, train: -0.3574380232109434\n",
      "Step:311, train: -0.3569528660602702\n",
      "Step:312, train: -0.3564569957478741\n",
      "Step:313, train: -0.35595027210226604\n",
      "Step:314, train: -0.35543252540087683\n",
      "Step:315, train: -0.3549035664264996\n",
      "Step:316, train: -0.3543631716665217\n",
      "Step:317, train: -0.35381108000353756\n",
      "Step:318, train: -0.3532469831934438\n",
      "Step:319, train: -0.35267051491981416\n",
      "Step:320, train: -0.35208123666642804\n",
      "Step:321, train: -0.35147861900760924\n",
      "Step:322, train: -0.3508620213081828\n",
      "Step:323, train: -0.35023065760343924\n",
      "Step:324, train: -0.3495835601159231\n",
      "Step:325, train: -0.34891953454785923\n",
      "Step:326, train: -0.34823708881063037\n",
      "Step:327, train: -0.3475343419917615\n",
      "Step:328, train: -0.3468089082136211\n",
      "Step:329, train: -0.3460577147905618\n",
      "Step:330, train: -0.34527675717910095\n",
      "Step:331, train: -0.34446070677640706\n",
      "Step:332, train: -0.34360231624313015\n",
      "Step:333, train: -0.3426914095547857\n",
      "Step:334, train: -0.3417130996613105\n",
      "Step:335, train: -0.3406443021824386\n",
      "Step:336, train: -0.3394460654933992\n",
      "Step:337, train: -0.3380432303491783\n",
      "Step:338, train: -0.3362530925928722\n",
      "Step:339, train: -0.3339867210688934\n",
      "Step:340, train: -0.33194174238966423\n",
      "Step:341, train: -0.3301552664097587\n",
      "Step:342, train: -0.3286196635634278\n",
      "Step:343, train: -0.32730863248731323\n",
      "Step:344, train: -0.32619081831254504\n",
      "Step:345, train: -0.32523666369407944\n",
      "Step:346, train: -0.3244209998717433\n",
      "Step:347, train: -0.3237230013761385\n",
      "Step:348, train: -0.32312523224113743\n",
      "Step:349, train: -0.322612858702125\n",
      "Step:350, train: -0.3221733219684038\n",
      "Step:351, train: -0.32179628294020846\n",
      "Step:352, train: -0.32147358870747117\n",
      "Step:353, train: -0.32119915451452213\n",
      "Step:354, train: -0.32096873102365364\n",
      "Step:355, train: -0.32077962484969236\n",
      "Step:356, train: -0.3206302872898977\n",
      "Step:357, train: -0.3205197569422857\n",
      "Step:358, train: -0.3204469535562086\n",
      "Step:359, train: -0.32040992565285126\n",
      "Step:360, train: -0.3204053920839966\n",
      "Step:361, train: -0.32042881948579705\n",
      "Step:362, train: -0.3204749450423551\n",
      "Step:363, train: -0.3205384649338478\n",
      "Step:364, train: -0.3206145946634248\n",
      "Step:365, train: -0.320699401616653\n",
      "Step:366, train: -0.3207899492255841\n",
      "Step:367, train: -0.3208843283188681\n",
      "Step:368, train: -0.3209816226839372\n",
      "Step:369, train: -0.3210818372248952\n",
      "Step:370, train: -0.3211858248526874\n",
      "Step:371, train: -0.32129520756405255\n",
      "Step:372, train: -0.3214122667077207\n",
      "Step:373, train: -0.3215398186418489\n",
      "Step:374, train: -0.32168105953180404\n",
      "Step:375, train: -0.32183936690504483\n",
      "Step:376, train: -0.3220180704461301\n",
      "Step:377, train: -0.3222201859894004\n",
      "Step:378, train: -0.32244816955729333\n",
      "Step:379, train: -0.3227036936006661\n",
      "Step:380, train: -0.3229874967155896\n",
      "Step:381, train: -0.3232993001428714\n",
      "Step:382, train: -0.3236378340632804\n",
      "Step:383, train: -0.3240009233395665\n",
      "Step:384, train: -0.3243856514989163\n",
      "Step:385, train: -0.32478854413465774\n",
      "Step:386, train: -0.3252057851684406\n",
      "Step:387, train: -0.32563340916062716\n",
      "Step:388, train: -0.3260674760984773\n",
      "Step:389, train: -0.3265042230587045\n",
      "Step:390, train: -0.3269401619326148\n",
      "Step:391, train: -0.32737215252548146\n",
      "Step:392, train: -0.3277974464687324\n",
      "Step:393, train: -0.3282136915084168\n",
      "Step:394, train: -0.32861892849231533\n",
      "Step:395, train: -0.32901157121166613\n",
      "Step:396, train: -0.32939036860039445\n",
      "Step:397, train: -0.32975437499837534\n",
      "Step:398, train: -0.3301029111363101\n",
      "Step:399, train: -0.3304355238383139\n",
      "Step:400, train: -0.3307519548097018\n",
      "Step:401, train: -0.3310521138248194\n",
      "Step:402, train: -0.33133604018693\n",
      "Step:403, train: -0.33160388566450905\n",
      "Step:404, train: -0.3318558911204893\n",
      "Step:405, train: -0.3320923683653446\n",
      "Step:406, train: -0.33231368255240534\n",
      "Step:407, train: -0.3325202476128917\n",
      "Step:408, train: -0.3327125097164116\n",
      "Step:409, train: -0.3328909387857402\n",
      "Step:410, train: -0.3330560200138202\n",
      "Step:411, train: -0.3332082530936309\n",
      "Step:412, train: -0.33334814598899043\n",
      "Step:413, train: -0.3334762085045396\n",
      "Step:414, train: -0.3335929498879198\n",
      "Step:415, train: -0.3336988845946001\n",
      "Step:416, train: -0.33379451309950253\n",
      "Step:417, train: -0.33388033898980823\n",
      "Step:418, train: -0.33395685386002244\n",
      "Step:419, train: -0.3340245454856715\n",
      "Step:420, train: -0.3340838918582789\n",
      "Step:421, train: -0.3341353605237666\n",
      "Step:422, train: -0.33417940963754234\n",
      "Step:423, train: -0.3342164885312988\n",
      "Step:424, train: -0.33424703702299086\n",
      "Step:425, train: -0.3342714832813527\n",
      "Step:426, train: -0.3342902441951527\n",
      "Step:427, train: -0.33430373251695983\n",
      "Step:428, train: -0.33431233779464753\n",
      "Step:429, train: -0.3343164536080637\n",
      "Step:430, train: -0.3343164490856877\n",
      "Step:431, train: -0.3343126907933671\n",
      "Step:432, train: -0.3343055343497411\n",
      "Step:433, train: -0.33429531936973406\n",
      "Step:434, train: -0.3342823821455188\n",
      "Step:435, train: -0.33426704555167097\n",
      "Step:436, train: -0.3342496191867939\n",
      "Step:437, train: -0.3342304056214138\n",
      "Step:438, train: -0.33420969611145546\n",
      "Step:439, train: -0.3341877694072052\n",
      "Step:440, train: -0.3341648988869541\n",
      "Step:441, train: -0.3341413428236345\n",
      "Step:442, train: -0.3341173486747082\n",
      "Step:443, train: -0.3340931604527517\n",
      "Step:444, train: -0.33406900091262187\n",
      "Step:445, train: -0.33404509060268006\n",
      "Step:446, train: -0.3340216320998758\n",
      "Step:447, train: -0.33399882262106595\n",
      "Step:448, train: -0.3339768415657552\n",
      "Step:449, train: -0.3339558618712164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:450, train: -0.3339360380153029\n",
      "Step:451, train: -0.3339175172167661\n",
      "Step:452, train: -0.33390042312302703\n",
      "Step:453, train: -0.33388487628934005\n",
      "Step:454, train: -0.33387097449158265\n",
      "Step:455, train: -0.33385880348255004\n",
      "Step:456, train: -0.33384843238288897\n",
      "Step:457, train: -0.33383991191785295\n",
      "Step:458, train: -0.3338332789378312\n",
      "Step:459, train: -0.3338285519346091\n",
      "Step:460, train: -0.33382573327725384\n",
      "Step:461, train: -0.3338248110634478\n",
      "Step:462, train: -0.33382575192715336\n",
      "Step:463, train: -0.33382851303996863\n",
      "Step:464, train: -0.33383303289516225\n",
      "Step:465, train: -0.3338392371491418\n",
      "Step:466, train: -0.33384703789099734\n",
      "Step:467, train: -0.3338563365036262\n",
      "Step:468, train: -0.33386702579016203\n",
      "Step:469, train: -0.33387898592946186\n",
      "Step:470, train: -0.33389209598204267\n",
      "Step:471, train: -0.33390622770664374\n",
      "Step:472, train: -0.33392124287248803\n",
      "Step:473, train: -0.3339370118340047\n",
      "Step:474, train: -0.33395339495954524\n",
      "Step:475, train: -0.333970257769359\n",
      "Step:476, train: -0.33398746475237456\n",
      "Step:477, train: -0.33400488589349464\n",
      "Step:478, train: -0.3340223936406358\n",
      "Step:479, train: -0.334039864917889\n",
      "Step:480, train: -0.33405717874791124\n",
      "Step:481, train: -0.33407422840765516\n",
      "Step:482, train: -0.3340909050556478\n",
      "Step:483, train: -0.33410711288672457\n",
      "Step:484, train: -0.3341227586763809\n",
      "Step:485, train: -0.334137759649728\n",
      "Step:486, train: -0.33415203523832016\n",
      "Step:487, train: -0.3341655168327773\n",
      "Step:488, train: -0.33417814167029225\n",
      "Step:489, train: -0.33418985184624966\n",
      "Step:490, train: -0.334200597017485\n",
      "Step:491, train: -0.33421033153276997\n",
      "Step:492, train: -0.33421901938226867\n",
      "Step:493, train: -0.33422662645497364\n",
      "Step:494, train: -0.3342331255018519\n",
      "Step:495, train: -0.334238494295503\n",
      "Step:496, train: -0.33424271528428395\n",
      "Step:497, train: -0.3342457746235207\n",
      "Step:498, train: -0.3342476661186714\n",
      "Step:499, train: -0.3342483795237452\n",
      "Step:500, train: -0.33424791639630785\n",
      "Step:501, train: -0.3342462737522681\n",
      "Step:502, train: -0.33424345716828113\n",
      "Step:503, train: -0.3342394732528371\n",
      "Step:504, train: -0.33423432914315054\n",
      "Step:505, train: -0.33422803611025004\n",
      "Step:506, train: -0.3342206061263641\n",
      "Step:507, train: -0.3342120541245366\n",
      "Step:508, train: -0.33420239542622876\n",
      "Step:509, train: -0.3341916460281695\n",
      "Step:510, train: -0.33417982514691785\n",
      "Step:511, train: -0.3341669515846145\n",
      "Step:512, train: -0.3341530414672581\n",
      "Step:513, train: -0.33413812259405107\n",
      "Step:514, train: -0.3341222081766823\n",
      "Step:515, train: -0.3341053228517743\n",
      "Step:516, train: -0.33408748795490484\n",
      "Step:517, train: -0.33406872496321227\n",
      "Step:518, train: -0.33404905708513916\n",
      "Step:519, train: -0.33402850560850067\n",
      "Step:520, train: -0.33400709554980196\n",
      "Step:521, train: -0.33398484456634764\n",
      "Step:522, train: -0.33396178007956173\n",
      "Step:523, train: -0.33393792157409946\n",
      "Step:524, train: -0.33391329185088825\n",
      "Step:525, train: -0.33388791571317733\n",
      "Step:526, train: -0.33386181083770516\n",
      "Step:527, train: -0.33383500506501956\n",
      "Step:528, train: -0.3338075137894774\n",
      "Step:529, train: -0.33377936132158004\n",
      "Step:530, train: -0.33375057039050304\n",
      "Step:531, train: -0.3337211588962019\n",
      "Step:532, train: -0.33369115269811417\n",
      "Step:533, train: -0.3336605660842719\n",
      "Step:534, train: -0.3336294235499363\n",
      "Step:535, train: -0.3335977449736603\n",
      "Step:536, train: -0.3335655520645409\n",
      "Step:537, train: -0.333532860062461\n",
      "Step:538, train: -0.3334996908988126\n",
      "Step:539, train: -0.3334660653934291\n",
      "Step:540, train: -0.3334320016323418\n",
      "Step:541, train: -0.3333975195088803\n",
      "Step:542, train: -0.333362639009851\n",
      "Step:543, train: -0.33332738010997925\n",
      "Step:544, train: -0.3332917570546774\n",
      "Step:545, train: -0.3332557917215489\n",
      "Step:546, train: -0.3332195043390454\n",
      "Step:547, train: -0.3331829105230279\n",
      "Step:548, train: -0.33314603288787986\n",
      "Step:549, train: -0.3331088876172856\n",
      "Step:550, train: -0.3330714926401387\n",
      "Step:551, train: -0.3330338695525582\n",
      "Step:552, train: -0.3329960349103044\n",
      "Step:553, train: -0.3329580100684012\n",
      "Step:554, train: -0.3329198130450876\n",
      "Step:555, train: -0.33288146533367347\n",
      "Step:556, train: -0.33284298404763746\n",
      "Step:557, train: -0.33280438953202823\n",
      "Step:558, train: -0.3327657037250741\n",
      "Step:559, train: -0.33272694707535183\n",
      "Step:560, train: -0.3326881397561814\n",
      "Step:561, train: -0.33264930290485606\n",
      "Step:562, train: -0.33261045918644344\n",
      "Step:563, train: -0.3325716292261843\n",
      "Step:564, train: -0.332532838381429\n",
      "Step:565, train: -0.3324941066785501\n",
      "Step:566, train: -0.3324554610295586\n",
      "Step:567, train: -0.33241692083746244\n",
      "Step:568, train: -0.3323785140207999\n",
      "Step:569, train: -0.3323402640026253\n",
      "Step:570, train: -0.3323021962272634\n",
      "Step:571, train: -0.33226433647867415\n",
      "Step:572, train: -0.33222671148042204\n",
      "Step:573, train: -0.33218934632522534\n",
      "Step:574, train: -0.33215227104502015\n",
      "Step:575, train: -0.3321155127613005\n",
      "Step:576, train: -0.3320790981130707\n",
      "Step:577, train: -0.3320430560441222\n",
      "Step:578, train: -0.3320074160230527\n",
      "Step:579, train: -0.3319722038869463\n",
      "Step:580, train: -0.33193745209419595\n",
      "Step:581, train: -0.33190318625182363\n",
      "Step:582, train: -0.3318694359110573\n",
      "Step:583, train: -0.3318362308787623\n",
      "Step:584, train: -0.33180359817581206\n",
      "Step:585, train: -0.331771566908305\n",
      "Step:586, train: -0.3317401630300048\n",
      "Step:587, train: -0.33170941462839937\n",
      "Step:588, train: -0.3316793470446\n",
      "Step:589, train: -0.33164998560332953\n",
      "Step:590, train: -0.3316213545016483\n",
      "Step:591, train: -0.3315934764475394\n",
      "Step:592, train: -0.33156637325419797\n",
      "Step:593, train: -0.3315400651301081\n",
      "Step:594, train: -0.3315145750084592\n",
      "Step:595, train: -0.33148991376522463\n",
      "Step:596, train: -0.3314661030722513\n",
      "Step:597, train: -0.3314431591677571\n",
      "Step:598, train: -0.3314210902935502\n",
      "Step:599, train: -0.3313999082065344\n",
      "Step:600, train: -0.33137962369350443\n",
      "Step:601, train: -0.3313602449458919\n",
      "Step:602, train: -0.33134177914533725\n",
      "Step:603, train: -0.331324228708231\n",
      "Step:604, train: -0.33130760128706044\n",
      "Step:605, train: -0.33129189430938827\n",
      "Step:606, train: -0.33127711508982655\n",
      "Step:607, train: -0.3312632588379215\n",
      "Step:608, train: -0.33125032414158123\n",
      "Step:609, train: -0.331238309613167\n",
      "Step:610, train: -0.3312272112815431\n",
      "Step:611, train: -0.3312170285687147\n",
      "Step:612, train: -0.33120775500106303\n",
      "Step:613, train: -0.3311993865566408\n",
      "Step:614, train: -0.3311919191117868\n",
      "Step:615, train: -0.33118534784541204\n",
      "Step:616, train: -0.3311796672894174\n",
      "Step:617, train: -0.33117487484224745\n",
      "Step:618, train: -0.33117096544059793\n",
      "Step:619, train: -0.3311679356448147\n",
      "Step:620, train: -0.3311657818829093\n",
      "Step:621, train: -0.33116450467624986\n",
      "Step:622, train: -0.3311641012006364\n",
      "Step:623, train: -0.3311645707854288\n",
      "Step:624, train: -0.33116591185651617\n",
      "Step:625, train: -0.3311681233444277\n",
      "Step:626, train: -0.33117121277668277\n",
      "Step:627, train: -0.3311751810545209\n",
      "Step:628, train: -0.33118003237291627\n",
      "Step:629, train: -0.33118577294305085\n",
      "Step:630, train: -0.3311924087689935\n",
      "Step:631, train: -0.3311999465927437\n",
      "Step:632, train: -0.3312083982927706\n",
      "Step:633, train: -0.33121776890471244\n",
      "Step:634, train: -0.33122807470163906\n",
      "Step:635, train: -0.33123932615447976\n",
      "Step:636, train: -0.33125154261684553\n",
      "Step:637, train: -0.3312647358147252\n",
      "Step:638, train: -0.3312789232882181\n",
      "Step:639, train: -0.3312941242407169\n",
      "Step:640, train: -0.331310359828739\n",
      "Step:641, train: -0.3313276500521441\n",
      "Step:642, train: -0.33134601861667573\n",
      "Step:643, train: -0.33136549049775005\n",
      "Step:644, train: -0.33138609432125316\n",
      "Step:645, train: -0.3314078594089271\n",
      "Step:646, train: -0.3314308095457439\n",
      "Step:647, train: -0.3314549802387232\n",
      "Step:648, train: -0.3314804038871221\n",
      "Step:649, train: -0.33150711299331104\n",
      "Step:650, train: -0.3315351454722555\n",
      "Step:651, train: -0.3315645358524973\n",
      "Step:652, train: -0.3315953256253141\n",
      "Step:653, train: -0.3316275558231922\n",
      "Step:654, train: -0.33166126752095537\n",
      "Step:655, train: -0.33169650098119513\n",
      "Step:656, train: -0.3317333032295423\n",
      "Step:657, train: -0.3317717173917305\n",
      "Step:658, train: -0.33181179137790545\n",
      "Step:659, train: -0.3318535700938962\n",
      "Step:660, train: -0.3318971034255577\n",
      "Step:661, train: -0.331942438538071\n",
      "Step:662, train: -0.3319896251215417\n",
      "Step:663, train: -0.3320387070419329\n",
      "Step:664, train: -0.3320897371670222\n",
      "Step:665, train: -0.33214276201902315\n",
      "Step:666, train: -0.33219782617772947\n",
      "Step:667, train: -0.33225497725549047\n",
      "Step:668, train: -0.33231425833658307\n",
      "Step:669, train: -0.3323757106737577\n",
      "Step:670, train: -0.3324393733941079\n",
      "Step:671, train: -0.3325052859906576\n",
      "Step:672, train: -0.3325734795116151\n",
      "Step:673, train: -0.332643980716633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:674, train: -0.33271681422514254\n",
      "Step:675, train: -0.3327919982158172\n",
      "Step:676, train: -0.3328695428086821\n",
      "Step:677, train: -0.33294945428846245\n",
      "Step:678, train: -0.3330317284189269\n",
      "Step:679, train: -0.33311635002816514\n",
      "Step:680, train: -0.3332032997361061\n",
      "Step:681, train: -0.33329254027916516\n",
      "Step:682, train: -0.3333840266667991\n",
      "Step:683, train: -0.3334776998053232\n",
      "Step:684, train: -0.33357349802495606\n",
      "Step:685, train: -0.33367133390892856\n",
      "Step:686, train: -0.3337711089331396\n",
      "Step:687, train: -0.33387271686320846\n",
      "Step:688, train: -0.3339760350831943\n",
      "Step:689, train: -0.3340809233496983\n",
      "Step:690, train: -0.3341872393158142\n",
      "Step:691, train: -0.3342948249225504\n",
      "Step:692, train: -0.3344035127825803\n",
      "Step:693, train: -0.33451313107257796\n",
      "Step:694, train: -0.3346234987844495\n",
      "Step:695, train: -0.33473443168177075\n",
      "Step:696, train: -0.33484574892377617\n",
      "Step:697, train: -0.3349572689481641\n",
      "Step:698, train: -0.3350688157974931\n",
      "Step:699, train: -0.335180219860297\n",
      "Step:700, train: -0.33529132012425367\n",
      "Step:701, train: -0.3354019668878103\n",
      "Step:702, train: -0.335512027018274\n",
      "Step:703, train: -0.3356213792551507\n",
      "Step:704, train: -0.33572992526190576\n",
      "Step:705, train: -0.3358375761533226\n",
      "Step:706, train: -0.3359442712673197\n",
      "Step:707, train: -0.33604996920836244\n",
      "Step:708, train: -0.3361546479486519\n",
      "Step:709, train: -0.33625830766287634\n",
      "Step:710, train: -0.33636097265352444\n",
      "Step:711, train: -0.33646268394568674\n",
      "Step:712, train: -0.33656350655157063\n",
      "Step:713, train: -0.3366635234894463\n",
      "Step:714, train: -0.33676283950690356\n",
      "Step:715, train: -0.33686157983065307\n",
      "Step:716, train: -0.33695988991875986\n",
      "Step:717, train: -0.33705792669109086\n",
      "Step:718, train: -0.3371558667464984\n",
      "Step:719, train: -0.3372539017306446\n",
      "Step:720, train: -0.33735223708868806\n",
      "Step:721, train: -0.3374510946177132\n",
      "Step:722, train: -0.3375507046498359\n",
      "Step:723, train: -0.3376513113056207\n",
      "Step:724, train: -0.33775317193109367\n",
      "Step:725, train: -0.3378565511016991\n",
      "Step:726, train: -0.33796172129833774\n",
      "Step:727, train: -0.3380689646383495\n",
      "Step:728, train: -0.33817857690945086\n",
      "Step:729, train: -0.3382908532834482\n",
      "Step:730, train: -0.33840610034881885\n",
      "Step:731, train: -0.3385246240912051\n",
      "Step:732, train: -0.3386467472770657\n",
      "Step:733, train: -0.33877278956288037\n",
      "Step:734, train: -0.3389030783798406\n",
      "Step:735, train: -0.33903794857868086\n",
      "Step:736, train: -0.3391777324680119\n",
      "Step:737, train: -0.339322770439346\n",
      "Step:738, train: -0.3394734039640696\n",
      "Step:739, train: -0.33962997731270456\n",
      "Step:740, train: -0.3397928338469453\n",
      "Step:741, train: -0.3399623192015425\n",
      "Step:742, train: -0.3401387755888701\n",
      "Step:743, train: -0.34032253671244733\n",
      "Step:744, train: -0.3405139355970438\n",
      "Step:745, train: -0.3407132893059094\n",
      "Step:746, train: -0.3409209061861694\n",
      "Step:747, train: -0.34113707600998805\n",
      "Step:748, train: -0.3413620660078484\n",
      "Step:749, train: -0.34159611421063646\n",
      "Step:750, train: -0.3418394251297969\n",
      "Step:751, train: -0.34209215962581624\n",
      "Step:752, train: -0.34235443308715147\n",
      "Step:753, train: -0.3426263060320034\n",
      "Step:754, train: -0.3429077704870259\n",
      "Step:755, train: -0.34319874800428135\n",
      "Step:756, train: -0.343499077462451\n",
      "Step:757, train: -0.3438085096609678\n",
      "Step:758, train: -0.3441267048428947\n",
      "Step:759, train: -0.3444532057641875\n",
      "Step:760, train: -0.3447874589583051\n",
      "Step:761, train: -0.3451287863786175\n",
      "Step:762, train: -0.34547639601250385\n",
      "Step:763, train: -0.3458293677721088\n",
      "Step:764, train: -0.3461866628743468\n",
      "Step:765, train: -0.3465471192321638\n",
      "Step:766, train: -0.34690945547628177\n",
      "Step:767, train: -0.3472722763526682\n",
      "Step:768, train: -0.34763409375778437\n",
      "Step:769, train: -0.34799331761370467\n",
      "Step:770, train: -0.3483482974933591\n",
      "Step:771, train: -0.3486973277665892\n",
      "Step:772, train: -0.3490386711276005\n",
      "Step:773, train: -0.3493705802838266\n",
      "Step:774, train: -0.34969132907877964\n",
      "Step:775, train: -0.3499992192832004\n",
      "Step:776, train: -0.3502926139730069\n",
      "Step:777, train: -0.35056994696448746\n",
      "Step:778, train: -0.3508297302057705\n",
      "Step:779, train: -0.35107057520445856\n",
      "Step:780, train: -0.3512911934965004\n",
      "Step:781, train: -0.35149039353262646\n",
      "Step:782, train: -0.35166708997164325\n",
      "Step:783, train: -0.3518202907029862\n",
      "Step:784, train: -0.3519491017940194\n",
      "Step:785, train: -0.3520527067242806\n",
      "Step:786, train: -0.3521303698231447\n",
      "Step:787, train: -0.35218142508972383\n",
      "Step:788, train: -0.35220525300769806\n",
      "Step:789, train: -0.3522012868316654\n",
      "Step:790, train: -0.35216898815564335\n",
      "Step:791, train: -0.3521078372212036\n",
      "Step:792, train: -0.3520173208432881\n",
      "Step:793, train: -0.3518969055220057\n",
      "Step:794, train: -0.35174602293791246\n",
      "Step:795, train: -0.35156404307041556\n",
      "Step:796, train: -0.3513502390849966\n",
      "Step:797, train: -0.35110374465233357\n",
      "Step:798, train: -0.35082349856240885\n",
      "Step:799, train: -0.3505081614428501\n",
      "Step:800, train: -0.3501560293410263\n",
      "Step:801, train: -0.34976490652186126\n",
      "Step:802, train: -0.3493320309002131\n",
      "Step:803, train: -0.34885421067001227\n",
      "Step:804, train: -0.348328938975269\n",
      "Step:805, train: -0.3477583064520658\n",
      "Step:806, train: -0.3471557969583801\n",
      "Step:807, train: -0.34654413073310264\n",
      "Step:808, train: -0.3459415208288241\n",
      "Step:809, train: -0.3453595241353428\n",
      "Step:810, train: -0.34480603882847805\n",
      "Step:811, train: -0.3442858145997196\n",
      "Step:812, train: -0.3438010651384714\n",
      "Step:813, train: -0.3433522606455613\n",
      "Step:814, train: -0.34293880564828766\n",
      "Step:815, train: -0.3425595216369032\n",
      "Step:816, train: -0.3422130398309172\n",
      "Step:817, train: -0.34189804272266683\n",
      "Step:818, train: -0.341613460424209\n",
      "Step:819, train: -0.3413586419740564\n",
      "Step:820, train: -0.3411334650325898\n",
      "Step:821, train: -0.3409384846697558\n",
      "Step:822, train: -0.3407750639853819\n",
      "Step:823, train: -0.34064551349700123\n",
      "Step:824, train: -0.34055322474640204\n",
      "Step:825, train: -0.34050271930386744\n",
      "Step:826, train: -0.3404995611247915\n",
      "Step:827, train: -0.3405500278172289\n",
      "Step:828, train: -0.3406605584577831\n",
      "Step:829, train: -0.34083715818521065\n",
      "Step:830, train: -0.3410849721501344\n",
      "Step:831, train: -0.341407834603215\n",
      "Step:832, train: -0.3418068860395894\n",
      "Step:833, train: -0.3422778670373514\n",
      "Step:834, train: -0.3428092128988552\n",
      "Step:835, train: -0.34338380912162464\n",
      "Step:836, train: -0.3439833334771612\n",
      "Step:837, train: -0.3445916821016149\n",
      "Step:838, train: -0.3451962368091681\n",
      "Step:839, train: -0.3457877646329052\n",
      "Step:840, train: -0.3463598164767558\n",
      "Step:841, train: -0.3469081183673971\n",
      "Step:842, train: -0.34743001944055274\n",
      "Step:843, train: -0.34792406502147766\n",
      "Step:844, train: -0.34838968560526373\n",
      "Step:845, train: -0.3488269462793205\n",
      "Step:846, train: -0.3492363637265529\n",
      "Step:847, train: -0.3496187759375958\n",
      "Step:848, train: -0.3499752391599689\n",
      "Step:849, train: -0.3503069539678143\n",
      "Step:850, train: -0.35061520240526606\n",
      "Step:851, train: -0.3509013206514983\n",
      "Step:852, train: -0.3511666597588277\n",
      "Step:853, train: -0.3514125673656884\n",
      "Step:854, train: -0.35164037063117837\n",
      "Step:855, train: -0.3518513626087411\n",
      "Step:856, train: -0.35204680691975065\n",
      "Step:857, train: -0.35222791829115796\n",
      "Step:858, train: -0.35239586688435365\n",
      "Step:859, train: -0.3525517749078964\n",
      "Step:860, train: -0.3526967157901125\n",
      "Step:861, train: -0.3528317131053779\n",
      "Step:862, train: -0.35295773354368454\n",
      "Step:863, train: -0.353075702348572\n",
      "Step:864, train: -0.3531864822904574\n",
      "Step:865, train: -0.3532908970722179\n",
      "Step:866, train: -0.35338970738943004\n",
      "Step:867, train: -0.35348362544962486\n",
      "Step:868, train: -0.3535733156943164\n",
      "Step:869, train: -0.353659389500277\n",
      "Step:870, train: -0.3537424070579866\n",
      "Step:871, train: -0.35382287609125285\n",
      "Step:872, train: -0.3539012583986077\n",
      "Step:873, train: -0.35397796437099444\n",
      "Step:874, train: -0.35405335679381567\n",
      "Step:875, train: -0.354127755637583\n",
      "Step:876, train: -0.3542014352595405\n",
      "Step:877, train: -0.35427463039876533\n",
      "Step:878, train: -0.3543475385143883\n",
      "Step:879, train: -0.3544203252935815\n",
      "Step:880, train: -0.3544931174740643\n",
      "Step:881, train: -0.35456602708558094\n",
      "Step:882, train: -0.35463914133403435\n",
      "Step:883, train: -0.3547125287070622\n",
      "Step:884, train: -0.3547862443785624\n",
      "Step:885, train: -0.35486033985807397\n",
      "Step:886, train: -0.3549348593574465\n",
      "Step:887, train: -0.3550098565838926\n",
      "Step:888, train: -0.35508538459875816\n",
      "Step:889, train: -0.35516150928695855\n",
      "Step:890, train: -0.35523830696624353\n",
      "Step:891, train: -0.3553158698748549\n",
      "Step:892, train: -0.35539430626085106\n",
      "Step:893, train: -0.35547374478460475\n",
      "Step:894, train: -0.3555543305504074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:895, train: -0.3556362213214699\n",
      "Step:896, train: -0.3557196010920431\n",
      "Step:897, train: -0.3558046569512253\n",
      "Step:898, train: -0.3558915860383243\n",
      "Step:899, train: -0.35598059393272347\n",
      "Step:900, train: -0.3560718839893796\n",
      "Step:901, train: -0.35616564957605795\n",
      "Step:902, train: -0.35626207597046344\n",
      "Step:903, train: -0.3563613244157707\n",
      "Step:904, train: -0.3564635301541961\n",
      "Step:905, train: -0.356568794119469\n",
      "Step:906, train: -0.3566771733205616\n",
      "Step:907, train: -0.35678868048370516\n",
      "Step:908, train: -0.35690327564667534\n",
      "Step:909, train: -0.3570208624177565\n",
      "Step:910, train: -0.35714128270008927\n",
      "Step:911, train: -0.35726431852724394\n",
      "Step:912, train: -0.35738969882227334\n",
      "Step:913, train: -0.3575170875477535\n",
      "Step:914, train: -0.35764609933625946\n",
      "Step:915, train: -0.35777629485908097\n",
      "Step:916, train: -0.3579071905677208\n",
      "Step:917, train: -0.358038261768097\n",
      "Step:918, train: -0.35816894891454204\n",
      "Step:919, train: -0.358298659895677\n",
      "Step:920, train: -0.35842678758196994\n",
      "Step:921, train: -0.3585526990180228\n",
      "Step:922, train: -0.3586757535258785\n",
      "Step:923, train: -0.3587952968816522\n",
      "Step:924, train: -0.35891067421164996\n",
      "Step:925, train: -0.3590212304069693\n",
      "Step:926, train: -0.35912631031977843\n",
      "Step:927, train: -0.35922527240065527\n",
      "Step:928, train: -0.35931747953417464\n",
      "Step:929, train: -0.3594023062449477\n",
      "Step:930, train: -0.35947914756612404\n",
      "Step:931, train: -0.3595474170784925\n",
      "Step:932, train: -0.3596065433989081\n",
      "Step:933, train: -0.3596559840911968\n",
      "Step:934, train: -0.35969521538383276\n",
      "Step:935, train: -0.35972374515102357\n",
      "Step:936, train: -0.3597410984860867\n",
      "Step:937, train: -0.3597468292403088\n",
      "Step:938, train: -0.3597405188676477\n",
      "Step:939, train: -0.3597217772112366\n",
      "Step:940, train: -0.35969023562202973\n",
      "Step:941, train: -0.3596455456326627\n",
      "Step:942, train: -0.35958738517382416\n",
      "Step:943, train: -0.3595154445320421\n",
      "Step:944, train: -0.35942942439859427\n",
      "Step:945, train: -0.359329040132499\n",
      "Step:946, train: -0.3592140052188565\n",
      "Step:947, train: -0.35908403084840645\n",
      "Step:948, train: -0.3589388206012842\n",
      "Step:949, train: -0.3587780570987811\n",
      "Step:950, train: -0.35860140603222657\n",
      "Step:951, train: -0.3584084919213881\n",
      "Step:952, train: -0.3581989086025947\n",
      "Step:953, train: -0.3579721922985537\n",
      "Step:954, train: -0.3577278259460138\n",
      "Step:955, train: -0.3574652253378154\n",
      "Step:956, train: -0.35718372535840953\n",
      "Step:957, train: -0.35688257370860405\n",
      "Step:958, train: -0.35656092453017185\n",
      "Step:959, train: -0.3562178152718579\n",
      "Step:960, train: -0.35585217337217906\n",
      "Step:961, train: -0.35546279522910706\n",
      "Step:962, train: -0.35504834786349493\n",
      "Step:963, train: -0.3546073694975928\n",
      "Step:964, train: -0.3541382667485487\n",
      "Step:965, train: -0.35363933948104637\n",
      "Step:966, train: -0.3531087956456963\n",
      "Step:967, train: -0.3525447923709291\n",
      "Step:968, train: -0.3519454889256678\n",
      "Step:969, train: -0.35130912830511823\n",
      "Step:970, train: -0.35063413869545046\n",
      "Step:971, train: -0.34991928137834083\n",
      "Step:972, train: -0.3491639040298754\n",
      "Step:973, train: -0.3483683426584997\n",
      "Step:974, train: -0.3475346172463941\n",
      "Step:975, train: -0.34666746121691555\n",
      "Step:976, train: -0.3457753891217571\n",
      "Step:977, train: -0.34487082897470295\n",
      "Step:978, train: -0.3439681450337966\n",
      "Step:979, train: -0.3430802607257934\n",
      "Step:980, train: -0.34221643808203267\n",
      "Step:981, train: -0.3413821691986672\n",
      "Step:982, train: -0.3405801135015295\n",
      "Step:983, train: -0.3398109928646481\n",
      "Step:984, train: -0.3390743102302835\n",
      "Step:985, train: -0.338368801843331\n",
      "Step:986, train: -0.3376927901321938\n",
      "Step:987, train: -0.33704438753246485\n",
      "Step:988, train: -0.3364216645195224\n",
      "Step:989, train: -0.33582272591360535\n",
      "Step:990, train: -0.33524578024142687\n",
      "Step:991, train: -0.33468917002999726\n",
      "Step:992, train: -0.3341513811092762\n",
      "Step:993, train: -0.3336310503257495\n",
      "Step:994, train: -0.33312697101659355\n",
      "Step:995, train: -0.33263807765223924\n",
      "Step:996, train: -0.33216344081505966\n",
      "Step:997, train: -0.3317022640317214\n",
      "Step:998, train: -0.3312538618900729\n",
      "Step:999, train: -0.33081765873543156\n",
      "-0.33081765873543156\n"
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [channel_target],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
