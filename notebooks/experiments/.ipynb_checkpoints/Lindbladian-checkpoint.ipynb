{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lindbladian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src_tf/')\n",
    "\n",
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from qiskit.quantum_info import DensityMatrix, random_unitary\n",
    "from qiskit.quantum_info import Operator\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from loss_functions import *\n",
    "from optimization import *\n",
    "from quantum_channel import *\n",
    "from quantum_tools import *\n",
    "from experimental import *\n",
    "from spam import *\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "n = 2\n",
    "d = 2**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelQuantumMap(\n",
    "                       # channel = LindbladMap(d, rank=d**2),\n",
    "                        channel = KrausMap(d, rank=d**2),\n",
    "                        loss_function = Conj3(index = 1, sign =-1),\n",
    "                        optimizer = tf.optimizers.Adam(learning_rate=0.005),\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b805251f084009b5c2a1bdcd7ae0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: -0.005319056835806454\n",
      "Step:1, train: -0.005409890271258896\n",
      "Step:2, train: -0.005500319852732862\n",
      "Step:3, train: -0.005603604406599859\n",
      "Step:4, train: -0.005693484766028796\n",
      "Step:5, train: -0.005754782371642226\n",
      "Step:6, train: -0.005843753424728983\n",
      "Step:7, train: -0.005928551654983521\n",
      "Step:8, train: -0.006068893155486398\n",
      "Step:9, train: -0.0061655893462217145\n",
      "Step:10, train: -0.006259854875841803\n",
      "Step:11, train: -0.006336246679045931\n",
      "Step:12, train: -0.00648572354629201\n",
      "Step:13, train: -0.006561972326282462\n",
      "Step:14, train: -0.006658321167529744\n",
      "Step:15, train: -0.006751662530903383\n",
      "Step:16, train: -0.006900385907406421\n",
      "Step:17, train: -0.007016097792571957\n",
      "Step:18, train: -0.007090400351173105\n",
      "Step:19, train: -0.007113954773772011\n",
      "Step:20, train: -0.007184285027697104\n",
      "Step:21, train: -0.007360819159746508\n",
      "Step:22, train: -0.007497102736818089\n",
      "Step:23, train: -0.007614880915839808\n",
      "Step:24, train: -0.007758303349895402\n",
      "Step:25, train: -0.007799744052806835\n",
      "Step:26, train: -0.007929868300956346\n",
      "Step:27, train: -0.007983117499552116\n",
      "Step:28, train: -0.008163792696007257\n",
      "Step:29, train: -0.008332251010893047\n",
      "Step:30, train: -0.008510665050124714\n",
      "Step:31, train: -0.008561503041499241\n",
      "Step:32, train: -0.008767501019193769\n",
      "Step:33, train: -0.008938236237394068\n",
      "Step:34, train: -0.009106144054915558\n",
      "Step:35, train: -0.0091048578956835\n",
      "Step:36, train: -0.00926185893039243\n",
      "Step:37, train: -0.009545749905640367\n",
      "Step:38, train: -0.009680646762626364\n",
      "Step:39, train: -0.009845849340169468\n",
      "Step:40, train: -0.010071474617982943\n",
      "Step:41, train: -0.010274347189691757\n",
      "Step:42, train: -0.010388319824705802\n",
      "Step:43, train: -0.010552454343003136\n",
      "Step:44, train: -0.010763101165243774\n",
      "Step:45, train: -0.01085048391053629\n",
      "Step:46, train: -0.011041024694001462\n",
      "Step:47, train: -0.011282373851742845\n",
      "Step:48, train: -0.011461819079233533\n",
      "Step:49, train: -0.01154050034183709\n",
      "Step:50, train: -0.011754817051803817\n",
      "Step:51, train: -0.012015556347534\n",
      "Step:52, train: -0.012090094478110117\n",
      "Step:53, train: -0.01231270407611371\n",
      "Step:54, train: -0.012514384716840332\n",
      "Step:55, train: -0.012692251450768657\n",
      "Step:56, train: -0.012993863319270613\n",
      "Step:57, train: -0.013204864341622493\n",
      "Step:58, train: -0.013532853955877455\n",
      "Step:59, train: -0.013778948359281985\n",
      "Step:60, train: -0.013929112391697201\n",
      "Step:61, train: -0.014058941228034727\n",
      "Step:62, train: -0.014308888900640323\n",
      "Step:63, train: -0.014616331618431152\n",
      "Step:64, train: -0.014891607107761076\n",
      "Step:65, train: -0.01513924861846188\n",
      "Step:66, train: -0.015275340116939023\n",
      "Step:67, train: -0.015531342241845643\n",
      "Step:68, train: -0.01597129705013071\n",
      "Step:69, train: -0.016270800107221154\n",
      "Step:70, train: -0.016484074390634953\n",
      "Step:71, train: -0.016778485557531704\n",
      "Step:72, train: -0.01688657707738375\n",
      "Step:73, train: -0.017294399624676382\n",
      "Step:74, train: -0.017517759887589263\n",
      "Step:75, train: -0.017621566241146986\n",
      "Step:76, train: -0.018005130962267826\n",
      "Step:77, train: -0.018311471278681945\n",
      "Step:78, train: -0.01864353954299663\n",
      "Step:79, train: -0.01909060197192187\n",
      "Step:80, train: -0.01938236691042366\n",
      "Step:81, train: -0.019804540244567\n",
      "Step:82, train: -0.020051504822427582\n",
      "Step:83, train: -0.020419317607170653\n",
      "Step:84, train: -0.02067448959551376\n",
      "Step:85, train: -0.021104359972513274\n",
      "Step:86, train: -0.021173936310655797\n",
      "Step:87, train: -0.021387571476042876\n",
      "Step:88, train: -0.02147571566655803\n",
      "Step:89, train: -0.02168878132516597\n",
      "Step:90, train: -0.022017079556673712\n",
      "Step:91, train: -0.022185141222238133\n",
      "Step:92, train: -0.02251457159095545\n",
      "Step:93, train: -0.02268445887726462\n",
      "Step:94, train: -0.02287192279520366\n",
      "Step:95, train: -0.023097456616972944\n",
      "Step:96, train: -0.023269487575287788\n",
      "Step:97, train: -0.02368926168115759\n",
      "Step:98, train: -0.02406502843708583\n",
      "Step:99, train: -0.024283334328704154\n",
      "Step:100, train: -0.024432163863659484\n",
      "Step:101, train: -0.024475548480067204\n",
      "Step:102, train: -0.024705754858002842\n",
      "Step:103, train: -0.025117038622857184\n",
      "Step:104, train: -0.025655169092564125\n",
      "Step:105, train: -0.026005546330518044\n",
      "Step:106, train: -0.02612934656451312\n",
      "Step:107, train: -0.02658029583576499\n",
      "Step:108, train: -0.026844834269236804\n",
      "Step:109, train: -0.02712166893430618\n",
      "Step:110, train: -0.027265473778748373\n",
      "Step:111, train: -0.027637449567868563\n",
      "Step:112, train: -0.028116806714675893\n",
      "Step:113, train: -0.028477684797354227\n",
      "Step:114, train: -0.028742956794638532\n",
      "Step:115, train: -0.028953220572798654\n",
      "Step:116, train: -0.029339126054703738\n",
      "Step:117, train: -0.02990565398656004\n",
      "Step:118, train: -0.03008038272272897\n",
      "Step:119, train: -0.030418632593750765\n",
      "Step:120, train: -0.030714262578470364\n",
      "Step:121, train: -0.03106129314280373\n",
      "Step:122, train: -0.0316590306144999\n",
      "Step:123, train: -0.032080619257954184\n",
      "Step:124, train: -0.0323416861501549\n",
      "Step:125, train: -0.0326434500627988\n",
      "Step:126, train: -0.03335410852529727\n",
      "Step:127, train: -0.03377075229283513\n",
      "Step:128, train: -0.03409989626294574\n",
      "Step:129, train: -0.034480983979264815\n",
      "Step:130, train: -0.03498593927660917\n",
      "Step:131, train: -0.03551546303893327\n",
      "Step:132, train: -0.03602851673066498\n",
      "Step:133, train: -0.03641602133720351\n",
      "Step:134, train: -0.03672478407224903\n",
      "Step:135, train: -0.0372200861669417\n",
      "Step:136, train: -0.03788824517029659\n",
      "Step:137, train: -0.03826958911202121\n",
      "Step:138, train: -0.03870887100351776\n",
      "Step:139, train: -0.039175629512331604\n",
      "Step:140, train: -0.039534265814662996\n",
      "Step:141, train: -0.03985609222929136\n",
      "Step:142, train: -0.04024720207661672\n",
      "Step:143, train: -0.04099997722956027\n",
      "Step:144, train: -0.04149447390201445\n",
      "Step:145, train: -0.042108584057362286\n",
      "Step:146, train: -0.042576098208557775\n",
      "Step:147, train: -0.04295992044298917\n",
      "Step:148, train: -0.043658821046522876\n",
      "Step:149, train: -0.04398841332517518\n",
      "Step:150, train: -0.04445986712880668\n",
      "Step:151, train: -0.04527078870215152\n",
      "Step:152, train: -0.0458339672240113\n",
      "Step:153, train: -0.046480952199866615\n",
      "Step:154, train: -0.04707845302922274\n",
      "Step:155, train: -0.04750157547018559\n",
      "Step:156, train: -0.04779962289962779\n",
      "Step:157, train: -0.048165719538440246\n",
      "Step:158, train: -0.04912173398655189\n",
      "Step:159, train: -0.04940043274597946\n",
      "Step:160, train: -0.047829690929348814\n",
      "Step:161, train: -0.05067401803749693\n",
      "Step:162, train: -0.05113574919090183\n",
      "Step:163, train: -0.05148075231516115\n",
      "Step:164, train: -0.051890523912220377\n",
      "Step:165, train: -0.051756595345561325\n",
      "Step:166, train: -0.05263776195411853\n",
      "Step:167, train: -0.052997331475490066\n",
      "Step:168, train: -0.05335320074053059\n",
      "Step:169, train: -0.05377107348933577\n",
      "Step:170, train: -0.05420868400391308\n",
      "Step:171, train: -0.05497038142995739\n",
      "Step:172, train: -0.055452604490809174\n",
      "Step:173, train: -0.056136619910113046\n",
      "Step:174, train: -0.05695030274779904\n",
      "Step:175, train: -0.057786681378017866\n",
      "Step:176, train: -0.05814281787619487\n",
      "Step:177, train: -0.058941445980065885\n",
      "Step:178, train: -0.059210433145832714\n",
      "Step:179, train: -0.05996528155508703\n",
      "Step:180, train: -0.05993494329367777\n",
      "Step:181, train: -0.060384835517047854\n",
      "Step:182, train: -0.06093585039186473\n",
      "Step:183, train: -0.06184432172653167\n",
      "Step:184, train: -0.06296073307648146\n",
      "Step:185, train: -0.06358049086853391\n",
      "Step:186, train: -0.0640860256009656\n",
      "Step:187, train: -0.06487232249250312\n",
      "Step:188, train: -0.06449626669964065\n",
      "Step:189, train: -0.06537171069586678\n",
      "Step:190, train: -0.06749210434835155\n",
      "Step:191, train: -0.06824221625090078\n",
      "Step:192, train: -0.06792139696995002\n",
      "Step:193, train: -0.06891815404861809\n",
      "Step:194, train: -0.06993025182712605\n",
      "Step:195, train: -0.07069285580241697\n",
      "Step:196, train: -0.0711292292721066\n",
      "Step:197, train: -0.07179003749163233\n",
      "Step:198, train: -0.07252259920974313\n",
      "Step:199, train: -0.07330490505633903\n",
      "Step:200, train: -0.07387888075749433\n",
      "Step:201, train: -0.07465285547628445\n",
      "Step:202, train: -0.07568677823487886\n",
      "Step:203, train: -0.07675816017667429\n",
      "Step:204, train: -0.07766490394372\n",
      "Step:205, train: -0.07886629123088773\n",
      "Step:206, train: -0.07868618924765679\n",
      "Step:207, train: -0.08006527848481634\n",
      "Step:208, train: -0.07992450088960801\n",
      "Step:209, train: -0.08084386836889448\n",
      "Step:210, train: -0.0733662243824549\n",
      "Step:211, train: -0.08162175278372388\n",
      "Step:212, train: -0.08173562822583184\n",
      "Step:213, train: -0.08265484093411125\n",
      "Step:214, train: -0.08429605391216537\n",
      "Step:215, train: -0.08501348034753076\n",
      "Step:216, train: -0.0858611107078214\n",
      "Step:217, train: -0.08480199859024523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:218, train: -0.0862389598167256\n",
      "Step:219, train: -0.0872617747176162\n",
      "Step:220, train: -0.08776618012419292\n",
      "Step:221, train: -0.08946218023513033\n",
      "Step:222, train: -0.08963938165544125\n",
      "Step:223, train: -0.090198404452483\n",
      "Step:224, train: -0.09101221658047934\n",
      "Step:225, train: -0.09204929504046662\n",
      "Step:226, train: -0.09260255459864633\n",
      "Step:227, train: -0.09328004303242565\n",
      "Step:228, train: -0.09421806352879694\n",
      "Step:229, train: -0.09485625539610157\n",
      "Step:230, train: -0.09548170937594663\n",
      "Step:231, train: -0.09644613907542386\n",
      "Step:232, train: -0.09752105454598077\n",
      "Step:233, train: -0.09867462941092542\n",
      "Step:234, train: -0.09993161664224699\n",
      "Step:235, train: -0.09943312070420193\n",
      "Step:236, train: -0.10167949079916988\n",
      "Step:237, train: -0.10268712127526988\n",
      "Step:238, train: -0.10065186995011237\n",
      "Step:239, train: -0.10123392061164113\n",
      "Step:240, train: -0.10414990876867702\n",
      "Step:241, train: -0.10476280966197181\n",
      "Step:242, train: -0.10549753956070454\n",
      "Step:243, train: -0.10578530069334645\n",
      "Step:244, train: -0.10648487909909195\n",
      "Step:245, train: -0.10767062227416918\n",
      "Step:246, train: -0.10797863041016884\n",
      "Step:247, train: -0.10931864018350196\n",
      "Step:248, train: -0.11026364509219154\n",
      "Step:249, train: -0.11093600703881198\n",
      "Step:250, train: -0.11239324769154557\n",
      "Step:251, train: -0.11469604838840913\n",
      "Step:252, train: -0.11600813305225738\n",
      "Step:253, train: -0.1171623669006658\n",
      "Step:254, train: -0.11870061236351727\n",
      "Step:255, train: -0.1200325730053085\n",
      "Step:256, train: -0.12095765234455452\n",
      "Step:257, train: -0.12069565243253536\n",
      "Step:258, train: -0.1222924074301091\n",
      "Step:259, train: -0.12459167905426918\n",
      "Step:260, train: -0.12623632106499763\n",
      "Step:261, train: -0.12789101093372707\n",
      "Step:262, train: -0.12856292174451592\n",
      "Step:263, train: -0.12980010834989605\n",
      "Step:264, train: -0.13143201953965158\n",
      "Step:265, train: -0.13315143863538692\n",
      "Step:266, train: -0.1321980592570164\n",
      "Step:267, train: -0.1342401012970746\n",
      "Step:268, train: -0.13694732204736468\n",
      "Step:269, train: -0.13814492151453006\n",
      "Step:270, train: -0.13954192655624992\n",
      "Step:271, train: -0.1414710707829228\n",
      "Step:272, train: -0.1434945500739929\n",
      "Step:273, train: -0.14568194970687467\n",
      "Step:274, train: -0.14771292301784988\n",
      "Step:275, train: -0.14998971744526157\n",
      "Step:276, train: -0.15236382405747717\n",
      "Step:277, train: -0.15481659248413626\n",
      "Step:278, train: -0.1564774405888015\n",
      "Step:279, train: -0.15855344692806767\n",
      "Step:280, train: -0.16073867116010188\n",
      "Step:281, train: -0.16265234163015826\n",
      "Step:282, train: -0.1654021274681842\n",
      "Step:283, train: -0.16724256671827012\n",
      "Step:284, train: -0.16954002068936802\n",
      "Step:285, train: -0.17274855038083953\n",
      "Step:286, train: -0.1748777887547229\n",
      "Step:287, train: -0.17711175670737092\n",
      "Step:288, train: -0.1796071305934742\n",
      "Step:289, train: -0.18193182747259043\n",
      "Step:290, train: -0.18452511249388936\n",
      "Step:291, train: -0.18678877459089588\n",
      "Step:292, train: -0.18980885992655033\n",
      "Step:293, train: -0.193470407109051\n",
      "Step:294, train: -0.1970922098552313\n",
      "Step:295, train: -0.19798137778142347\n",
      "Step:296, train: -0.2015784725956595\n",
      "Step:297, train: -0.2061009225798184\n",
      "Step:298, train: -0.20893916376275204\n",
      "Step:299, train: -0.2128260393800807\n",
      "Step:300, train: -0.21761249333233884\n",
      "Step:301, train: -0.22107936862771044\n",
      "Step:302, train: -0.22540075420210057\n",
      "Step:303, train: -0.2297267286202763\n",
      "Step:304, train: -0.23402749382855687\n",
      "Step:305, train: -0.2387717218097989\n",
      "Step:306, train: -0.24396770908499152\n",
      "Step:307, train: -0.24922861031111362\n",
      "Step:308, train: -0.2533279114389536\n",
      "Step:309, train: -0.2584071390786913\n",
      "Step:310, train: -0.2636531577623743\n",
      "Step:311, train: -0.26883488942339334\n",
      "Step:312, train: -0.27475822840975805\n",
      "Step:313, train: -0.28139318295133287\n",
      "Step:314, train: -0.2878689874070824\n",
      "Step:315, train: -0.29471990033952994\n",
      "Step:316, train: -0.3005110954107119\n",
      "Step:317, train: -0.3065996920126215\n",
      "Step:318, train: -0.3135566435838071\n",
      "Step:319, train: -0.3214223109165315\n",
      "Step:320, train: -0.32930006005606416\n",
      "Step:321, train: -0.3376200294265316\n",
      "Step:322, train: -0.34593957185802915\n",
      "Step:323, train: -0.3543947865757362\n",
      "Step:324, train: -0.3618194092696188\n",
      "Step:325, train: -0.3713557775582678\n",
      "Step:326, train: -0.38114943623879877\n",
      "Step:327, train: -0.3900236269287516\n",
      "Step:328, train: -0.4002752203743511\n",
      "Step:329, train: -0.4104632611302682\n",
      "Step:330, train: -0.42160145409927635\n",
      "Step:331, train: -0.4338646778325681\n",
      "Step:332, train: -0.4454404011756402\n",
      "Step:333, train: -0.45824963835161586\n",
      "Step:334, train: -0.4695702246014356\n",
      "Step:335, train: -0.4829901927298156\n",
      "Step:336, train: -0.4965735647032521\n",
      "Step:337, train: -0.5100539118409411\n",
      "Step:338, train: -0.524122052544411\n",
      "Step:339, train: -0.5410193040674545\n",
      "Step:340, train: -0.5573310519468959\n",
      "Step:341, train: -0.5734482332196604\n",
      "Step:342, train: -0.5906017280335101\n",
      "Step:343, train: -0.6079404816911835\n",
      "Step:344, train: -0.6269920185661468\n",
      "Step:345, train: -0.6460491650803178\n",
      "Step:346, train: -0.6658626035268043\n",
      "Step:347, train: -0.6869754291655943\n",
      "Step:348, train: -0.7087121623921595\n",
      "Step:349, train: -0.7303163851322819\n",
      "Step:350, train: -0.7537462708222423\n",
      "Step:351, train: -0.7790159099116699\n",
      "Step:352, train: -0.8036295626881923\n",
      "Step:353, train: -0.829771569684979\n",
      "Step:354, train: -0.8566121487826712\n",
      "Step:355, train: -0.8842773653329875\n",
      "Step:356, train: -0.9128778536545805\n",
      "Step:357, train: -0.9424787730696349\n",
      "Step:358, train: -0.9722983928275034\n",
      "Step:359, train: -1.0034801830587605\n",
      "Step:360, train: -1.0345606040182316\n",
      "Step:361, train: -1.0666923054301751\n",
      "Step:362, train: -1.0998033900386812\n",
      "Step:363, train: -1.1337057607069327\n",
      "Step:364, train: -1.1670096940661354\n",
      "Step:365, train: -1.1999643074237842\n",
      "Step:366, train: -1.2339015340330812\n",
      "Step:367, train: -1.2680274969617606\n",
      "Step:368, train: -1.3019031385301068\n",
      "Step:369, train: -1.335431243891271\n",
      "Step:370, train: -1.3691142901292548\n",
      "Step:371, train: -1.402069511613579\n",
      "Step:372, train: -1.43452073756737\n",
      "Step:373, train: -1.4667771948013688\n",
      "Step:374, train: -1.49739991195977\n",
      "Step:375, train: -1.5273732915064993\n",
      "Step:376, train: -1.55597656351132\n",
      "Step:377, train: -1.5839666165450466\n",
      "Step:378, train: -1.6108337491777631\n",
      "Step:379, train: -1.6366273566869025\n",
      "Step:380, train: -1.6608178136274894\n",
      "Step:381, train: -1.6838234648196977\n",
      "Step:382, train: -1.7057374768598077\n",
      "Step:383, train: -1.7265595014008144\n",
      "Step:384, train: -1.746131484262036\n",
      "Step:385, train: -1.7646239728139852\n",
      "Step:386, train: -1.7820022649850062\n",
      "Step:387, train: -1.7980742050766414\n",
      "Step:388, train: -1.8130997475299453\n",
      "Step:389, train: -1.8272715390660705\n",
      "Step:390, train: -1.8402084509346448\n",
      "Step:391, train: -1.8524151446681048\n",
      "Step:392, train: -1.86391559052486\n",
      "Step:393, train: -1.8744436867788632\n",
      "Step:394, train: -1.8841148111845278\n",
      "Step:395, train: -1.8931357694141124\n",
      "Step:396, train: -1.9015397634443278\n",
      "Step:397, train: -1.9092766891866026\n",
      "Step:398, train: -1.9165092499727072\n",
      "Step:399, train: -1.9230411393260483\n",
      "Step:400, train: -1.9291332255869307\n",
      "Step:401, train: -1.934804896312425\n",
      "Step:402, train: -1.9400365551680157\n",
      "Step:403, train: -1.9447822078855057\n",
      "Step:404, train: -1.9492225720580885\n",
      "Step:405, train: -1.9533134318866812\n",
      "Step:406, train: -1.9570793302834955\n",
      "Step:407, train: -1.9605276827577789\n",
      "Step:408, train: -1.963727007449631\n",
      "Step:409, train: -1.9666687124572624\n",
      "Step:410, train: -1.9693429274102379\n",
      "Step:411, train: -1.9718219613158152\n",
      "Step:412, train: -1.9740931029014406\n",
      "Step:413, train: -1.9762016554549842\n",
      "Step:414, train: -1.978167521930925\n",
      "Step:415, train: -1.9799294440517985\n",
      "Step:416, train: -1.9815626631720433\n",
      "Step:417, train: -1.9830684121260462\n",
      "Step:418, train: -1.984450658644771\n",
      "Step:419, train: -1.9857287937727963\n",
      "Step:420, train: -1.9869040201875163\n",
      "Step:421, train: -1.9879786083347717\n",
      "Step:422, train: -1.9889733872286945\n",
      "Step:423, train: -1.9898944797363725\n",
      "Step:424, train: -1.9907268761554548\n",
      "Step:425, train: -1.991495762202109\n",
      "Step:426, train: -1.9921863749604634\n",
      "Step:427, train: -1.9928323447263594\n",
      "Step:428, train: -1.9934393441130547\n",
      "Step:429, train: -1.9939854291250318\n",
      "Step:430, train: -1.994484045357257\n",
      "Step:431, train: -1.9949444596921984\n",
      "Step:432, train: -1.9953690958092485\n",
      "Step:433, train: -1.995751183646305\n",
      "Step:434, train: -1.9960988924569758\n",
      "Step:435, train: -1.9964207315185234\n",
      "Step:436, train: -1.9967184402999578\n",
      "Step:437, train: -1.9969935081712025\n",
      "Step:438, train: -1.9972472561859904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:439, train: -1.9974716376464579\n",
      "Step:440, train: -1.9976779642660154\n",
      "Step:441, train: -1.9978687034125655\n",
      "Step:442, train: -1.9980449940826777\n",
      "Step:443, train: -1.9982053693134176\n",
      "Step:444, train: -1.9983541121141575\n",
      "Step:445, train: -1.9984932481896793\n",
      "Step:446, train: -1.9986183444533505\n",
      "Step:447, train: -1.9987329556751439\n",
      "Step:448, train: -1.9988367469368833\n",
      "Step:449, train: -1.9989313525917165\n",
      "Step:450, train: -1.9990188378091878\n",
      "Step:451, train: -1.9990996799281728\n",
      "Step:452, train: -1.999172451899828\n",
      "Step:453, train: -1.9992396174841582\n",
      "Step:454, train: -1.999301159977324\n",
      "Step:455, train: -1.9993581642027365\n",
      "Step:456, train: -1.9994113387868793\n",
      "Step:457, train: -1.9994600168225616\n",
      "Step:458, train: -1.9995038733786341\n",
      "Step:459, train: -1.9995439988649395\n",
      "Step:460, train: -1.9995811129952559\n",
      "Step:461, train: -1.9996154164253848\n",
      "Step:462, train: -1.9996470591119095\n",
      "Step:463, train: -1.9996756020416186\n",
      "Step:464, train: -1.9997019982686988\n",
      "Step:465, train: -1.9997261113744944\n",
      "Step:466, train: -1.999748001685386\n",
      "Step:467, train: -1.9997683629284102\n",
      "Step:468, train: -1.9997870517221816\n",
      "Step:469, train: -1.9998043353862789\n",
      "Step:470, train: -1.9998199129293517\n",
      "Step:471, train: -1.999834272863786\n",
      "Step:472, train: -1.9998475890825003\n",
      "Step:473, train: -1.9998599265688233\n",
      "Step:474, train: -1.9998713430193025\n",
      "Step:475, train: -1.9998814831542315\n",
      "Step:476, train: -1.9998907871360003\n",
      "Step:477, train: -1.999899502530373\n",
      "Step:478, train: -1.99990750143377\n",
      "Step:479, train: -1.9999147563485282\n",
      "Step:480, train: -1.999921480883027\n",
      "Step:481, train: -1.9999277670786895\n",
      "Step:482, train: -1.999933542698626\n",
      "Step:483, train: -1.9999387988080464\n",
      "Step:484, train: -1.9999436699817534\n",
      "Step:485, train: -1.999948179634731\n",
      "Step:486, train: -1.999952348641316\n",
      "Step:487, train: -1.9999560958789167\n",
      "Step:488, train: -1.9999595615552967\n",
      "Step:489, train: -1.9999627012163548\n",
      "Step:490, train: -1.9999656225616134\n",
      "Step:491, train: -1.9999683391306085\n",
      "Step:492, train: -1.9999708622458967\n",
      "Step:493, train: -1.9999732017206595\n",
      "Step:494, train: -1.9999752584446195\n",
      "Step:495, train: -1.9999771660026657\n",
      "Step:496, train: -1.9999789387974332\n",
      "Step:497, train: -1.9999805863571025\n",
      "Step:498, train: -1.9999820837486406\n",
      "Step:499, train: -1.999983509634113\n",
      "Step:500, train: -1.9999848232605764\n",
      "Step:501, train: -1.999986025060323\n",
      "Step:502, train: -1.9999870967308246\n",
      "Step:503, train: -1.9999880987175576\n",
      "Step:504, train: -1.99998902506828\n",
      "Step:505, train: -1.9999898736456223\n",
      "Step:506, train: -1.999990664157007\n",
      "Step:507, train: -1.9999913997428047\n",
      "Step:508, train: -1.999992079878671\n",
      "Step:509, train: -1.999992692125271\n",
      "Step:510, train: -1.9999932622800094\n",
      "Step:511, train: -1.9999937929429548\n",
      "Step:512, train: -1.9999942863115523\n",
      "Step:513, train: -1.9999947356947139\n",
      "Step:514, train: -1.999995147304903\n",
      "Step:515, train: -1.9999955299517307\n",
      "Step:516, train: -1.9999958838881016\n",
      "Step:517, train: -1.9999962132254618\n",
      "Step:518, train: -1.9999965182966877\n",
      "Step:519, train: -1.9999967940117775\n",
      "Step:520, train: -1.999997050225442\n",
      "Step:521, train: -1.9999972836105662\n",
      "Step:522, train: -1.9999975009647635\n",
      "Step:523, train: -1.999997703969827\n",
      "Step:524, train: -1.9999978857048073\n",
      "Step:525, train: -1.9999980544816345\n",
      "Step:526, train: -1.9999982112806542\n",
      "Step:527, train: -1.999998356758271\n",
      "Step:528, train: -1.9999984887163191\n",
      "Step:529, train: -1.999998611536494\n",
      "Step:530, train: -1.999998721849031\n",
      "Step:531, train: -1.9999988262617063\n",
      "Step:532, train: -1.9999989222547008\n",
      "Step:533, train: -1.9999990106179046\n",
      "Step:534, train: -1.9999990925457545\n",
      "Step:535, train: -1.999999168352216\n",
      "Step:536, train: -1.999999235889783\n",
      "Step:537, train: -1.9999992984789245\n",
      "Step:538, train: -1.9999993552520854\n",
      "Step:539, train: -1.9999994081767354\n",
      "Step:540, train: -1.9999994576059366\n",
      "Step:541, train: -1.9999995036524267\n",
      "Step:542, train: -1.99999954635304\n",
      "Step:543, train: -1.9999995834042843\n",
      "Step:544, train: -1.9999996177248591\n",
      "Step:545, train: -1.999999649581105\n",
      "Step:546, train: -1.9999996791282084\n",
      "Step:547, train: -1.9999997065063995\n",
      "Step:548, train: -1.9999997318310383\n",
      "Step:549, train: -1.9999997552029334\n",
      "Step:550, train: -1.9999997757208154\n",
      "Step:551, train: -1.9999997947009318\n",
      "Step:552, train: -1.9999998122916112\n",
      "Step:553, train: -1.9999998285864247\n",
      "Step:554, train: -1.9999998436547188\n",
      "Step:555, train: -1.9999998571336048\n",
      "Step:556, train: -1.9999998696019134\n",
      "Step:557, train: -1.9999998811690192\n",
      "Step:558, train: -1.9999998918708637\n",
      "Step:559, train: -1.9999999010656617\n",
      "Step:560, train: -1.999999909537269\n",
      "Step:561, train: -1.999999917367557\n",
      "Step:562, train: -1.9999999246124824\n",
      "Step:563, train: -1.9999999313090315\n",
      "Step:564, train: -1.9999999374926731\n",
      "Step:565, train: -1.9999999431320017\n",
      "Step:566, train: -1.9999999483128375\n",
      "Step:567, train: -1.9999999531438901\n",
      "Step:568, train: -1.9999999574934901\n",
      "Step:569, train: -1.9999999613589963\n",
      "Step:570, train: -1.9999999647907192\n",
      "Step:571, train: -1.99999996795462\n",
      "Step:572, train: -1.9999999708710148\n",
      "Step:573, train: -1.9999999735583303\n",
      "Step:574, train: -1.9999999759710425\n",
      "Step:575, train: -1.9999999781962665\n",
      "Step:576, train: -1.9999999802508892\n",
      "Step:577, train: -1.9999999821408685\n",
      "Step:578, train: -1.9999999838071911\n",
      "Step:579, train: -1.9999999852888601\n",
      "Step:580, train: -1.9999999866485625\n",
      "Step:581, train: -1.999999987897863\n",
      "Step:582, train: -1.999999989044437\n",
      "Step:583, train: -1.9999999900946803\n",
      "Step:584, train: -1.999999991056529\n",
      "Step:585, train: -1.9999999919334273\n",
      "Step:586, train: -1.999999992716658\n",
      "Step:587, train: -1.9999999934090233\n",
      "Step:588, train: -1.9999999940429691\n",
      "Step:589, train: -1.9999999946230789\n",
      "Step:590, train: -1.9999999951531173\n",
      "Step:591, train: -1.9999999956372887\n",
      "Step:592, train: -1.9999999960775627\n",
      "Step:593, train: -1.9999999964724018\n",
      "Step:594, train: -1.9999999968192292\n",
      "Step:595, train: -1.9999999971311804\n",
      "Step:596, train: -1.9999999974150509\n",
      "Step:597, train: -1.9999999976732408\n",
      "Step:598, train: -1.99999999790067\n",
      "Step:599, train: -1.999999998111345\n",
      "Step:600, train: -1.9999999983037322\n",
      "Step:601, train: -1.9999999984771062\n",
      "Step:602, train: -1.9999999986353183\n",
      "Step:603, train: -1.999999998776191\n",
      "Step:604, train: -1.9999999989016786\n",
      "Step:605, train: -1.999999999015178\n",
      "Step:606, train: -1.9999999991182325\n",
      "Step:607, train: -1.9999999992111632\n",
      "Step:608, train: -1.9999999992953632\n",
      "Step:609, train: -1.9999999993697137\n",
      "Step:610, train: -1.9999999994343152\n",
      "Step:611, train: -1.9999999994929643\n",
      "Step:612, train: -1.999999999546272\n",
      "Step:613, train: -1.9999999995944946\n",
      "Step:614, train: -1.9999999996383409\n",
      "Step:615, train: -1.9999999996779902\n",
      "Step:616, train: -1.9999999997136504\n",
      "Step:617, train: -1.9999999997455704\n",
      "Step:618, train: -1.9999999997729165\n",
      "Step:619, train: -1.9999999997976485\n",
      "Step:620, train: -1.999999999819871\n",
      "Step:621, train: -1.999999999839589\n",
      "Step:622, train: -1.9999999998574332\n",
      "Step:623, train: -1.9999999998734523\n",
      "Step:624, train: -1.9999999998875062\n",
      "Step:625, train: -1.9999999999000826\n",
      "Step:626, train: -1.99999999991143\n",
      "Step:627, train: -1.9999999999216251\n",
      "Step:628, train: -1.9999999999307496\n",
      "Step:629, train: -1.999999999938884\n",
      "Step:630, train: -1.999999999945848\n",
      "Step:631, train: -1.9999999999521711\n",
      "Step:632, train: -1.9999999999578384\n",
      "Step:633, train: -1.9999999999628577\n",
      "Step:634, train: -1.9999999999671814\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\optimization.py:83\u001b[0m, in \u001b[0;36mModelQuantumMap.train\u001b[1;34m(self, inputs, targets, inputs_val, targets_val, num_iter, N)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loss_function \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function: \n\u001b[0;32m     81\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel, inputs_batch, targets_batch)\n\u001b[1;32m---> 83\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameter_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#self.logger.log(self)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#if targets_val is None:\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#    loss_val = 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#if verbose:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1094\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1095\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1096\u001b[0m           output_gradients))\n\u001b[0;32m   1097\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1098\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1100\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1109\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:285\u001b[0m, in \u001b[0;36m_StridedSliceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    282\u001b[0m strides_static \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconstant_value(strides)\n\u001b[0;32m    283\u001b[0m strides \u001b[38;5;241m=\u001b[39m strides_static \u001b[38;5;28;01mif\u001b[39;00m strides_static \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m strides\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrided_slice_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbegin_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mellipsis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshrink_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10858\u001b[0m, in \u001b[0;36mstrided_slice_grad\u001b[1;34m(shape, begin, end, strides, dy, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  10857\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10858\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10859\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStridedSliceGrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10860\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbegin_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mellipsis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshrink_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10862\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  10864\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_function = [Conj3(index = 1, sign = 1)]\n",
    "model.zero_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baacf7da2bfc4d56964960045fb467de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: 1.5956347354517675e-11\n",
      "Step:1, train: 3.5477874082978644e-08\n",
      "Step:2, train: 4.986695638864447e-06\n",
      "Step:3, train: 4.9461809634387954e-05\n",
      "Step:4, train: 1.863231614307459e-05\n",
      "Step:5, train: 3.2233213293819674e-05\n",
      "Step:6, train: 2.1688487378046517e-05\n",
      "Step:7, train: 1.4553265682337724e-05\n",
      "Step:8, train: 1.749138571116582e-05\n",
      "Step:9, train: 1.6263342935252112e-05\n",
      "Step:10, train: 1.1425563637867953e-05\n",
      "Step:11, train: 9.641057365339378e-06\n",
      "Step:12, train: 1.022320058352566e-05\n",
      "Step:13, train: 9.71317015374673e-06\n",
      "Step:14, train: 7.828698879275287e-06\n",
      "Step:15, train: 6.52072746509802e-06\n",
      "Step:16, train: 6.35101494361745e-06\n",
      "Step:17, train: 6.163004220627322e-06\n",
      "Step:18, train: 5.2605032087038595e-06\n",
      "Step:19, train: 4.308388331653035e-06\n",
      "Step:20, train: 3.942045614047096e-06\n",
      "Step:21, train: 3.968899534689818e-06\n",
      "Step:22, train: 3.7461686382966874e-06\n",
      "Step:23, train: 3.097651466776874e-06\n",
      "Step:24, train: 2.5161635629489254e-06\n",
      "Step:25, train: 2.381775867643654e-06\n",
      "Step:26, train: 2.4593171448206164e-06\n",
      "Step:27, train: 2.230415514281603e-06\n",
      "Step:28, train: 1.7774893603661468e-06\n",
      "Step:29, train: 1.5943209993318774e-06\n",
      "Step:30, train: 1.5735143844564092e-06\n",
      "Step:31, train: 1.4143201673144645e-06\n",
      "Step:32, train: 1.2253685094387734e-06\n",
      "Step:33, train: 1.1388607853435317e-06\n",
      "Step:34, train: 1.0679413121605918e-06\n",
      "Step:35, train: 9.281280427542171e-07\n",
      "Step:36, train: 8.061459035868523e-07\n",
      "Step:37, train: 7.624065626599119e-07\n",
      "Step:38, train: 7.073431228521443e-07\n",
      "Step:39, train: 6.265625459578317e-07\n",
      "Step:40, train: 5.75426973958848e-07\n",
      "Step:41, train: 5.278752741588733e-07\n",
      "Step:42, train: 4.435037184169488e-07\n",
      "Step:43, train: 3.918389331891703e-07\n",
      "Step:44, train: 3.9869101242651084e-07\n",
      "Step:45, train: 3.8557858950127866e-07\n",
      "Step:46, train: 2.984450109266845e-07\n",
      "Step:47, train: 2.4333367976137055e-07\n",
      "Step:48, train: 2.7073264619037474e-07\n",
      "Step:49, train: 2.602169312249103e-07\n",
      "Step:50, train: 1.9186667810089375e-07\n",
      "Step:51, train: 1.7300972510181367e-07\n",
      "Step:52, train: 1.8916141986125012e-07\n",
      "Step:53, train: 1.635355156848206e-07\n",
      "Step:54, train: 1.284232229448179e-07\n",
      "Step:55, train: 1.2245001590471816e-07\n",
      "Step:56, train: 1.192434931907016e-07\n",
      "Step:57, train: 1.0820524320998004e-07\n",
      "Step:58, train: 9.556387570164304e-08\n",
      "Step:59, train: 8.066785228688644e-08\n",
      "Step:60, train: 7.425512049685778e-08\n",
      "Step:61, train: 7.291712111889836e-08\n",
      "Step:62, train: 6.476821778900188e-08\n",
      "Step:63, train: 5.42700975270094e-08\n",
      "Step:64, train: 5.2157627994375844e-08\n",
      "Step:65, train: 4.80606562325292e-08\n",
      "Step:66, train: 3.8943508973154906e-08\n",
      "Step:67, train: 3.872523701708985e-08\n",
      "Step:68, train: 3.721623120789275e-08\n",
      "Step:69, train: 2.8816388497610035e-08\n",
      "Step:70, train: 2.673976318146032e-08\n",
      "Step:71, train: 2.7929678347859976e-08\n",
      "Step:72, train: 2.2586873704710797e-08\n",
      "Step:73, train: 1.841655283829624e-08\n",
      "Step:74, train: 2.0170223269388998e-08\n",
      "Step:75, train: 1.8136907531385305e-08\n",
      "Step:76, train: 1.3251900465860444e-08\n",
      "Step:77, train: 1.3690291122969711e-08\n",
      "Step:78, train: 1.4174708073966258e-08\n",
      "Step:79, train: 1.0075354328620278e-08\n",
      "Step:80, train: 9.184707105980294e-09\n",
      "Step:81, train: 1.0713448128640835e-08\n",
      "Step:82, train: 8.181980315491444e-09\n",
      "Step:83, train: 5.964147065107284e-09\n",
      "Step:84, train: 7.422585435179485e-09\n",
      "Step:85, train: 6.753524961489177e-09\n",
      "Step:86, train: 4.5766024303617314e-09\n",
      "Step:87, train: 4.74576367093249e-09\n",
      "Step:88, train: 4.961525856828075e-09\n",
      "Step:89, train: 3.822047900392533e-09\n",
      "Step:90, train: 3.2636562341537e-09\n",
      "Step:91, train: 3.3220866058059073e-09\n",
      "Step:92, train: 3.025308892290468e-09\n",
      "Step:93, train: 2.4631616746262353e-09\n",
      "Step:94, train: 2.2058822546355827e-09\n",
      "Step:95, train: 2.1700703456417614e-09\n",
      "Step:96, train: 1.9358087355314524e-09\n",
      "Step:97, train: 1.532409754645414e-09\n",
      "Step:98, train: 1.5355573479425288e-09\n",
      "Step:99, train: 1.5447957357750397e-09\n",
      "Step:100, train: 1.3894415618054268e-09\n",
      "Step:101, train: 1.6547889725160303e-09\n",
      "Step:102, train: 2.594131132127586e-09\n",
      "Step:103, train: 4.670315578714224e-09\n",
      "Step:104, train: 8.1168171073287e-09\n",
      "Step:105, train: 1.0226967939175324e-08\n",
      "Step:106, train: 8.007607688043095e-09\n",
      "Step:107, train: 7.274241986543473e-09\n",
      "Step:108, train: 1.5635003181024842e-08\n",
      "Step:109, train: 2.511706576058259e-08\n",
      "Step:110, train: 1.978874364372274e-08\n",
      "Step:111, train: 5.754965393123257e-09\n",
      "Step:112, train: 4.404416054093474e-09\n",
      "Step:113, train: 1.1764543672576622e-08\n",
      "Step:114, train: 1.071578603628609e-08\n",
      "Step:115, train: 4.689884369746267e-09\n",
      "Step:116, train: 5.0893945724439504e-09\n",
      "Step:117, train: 7.23741877539652e-09\n",
      "Step:118, train: 4.945355125407502e-09\n",
      "Step:119, train: 3.354241551178916e-09\n",
      "Step:120, train: 4.705236866797691e-09\n",
      "Step:121, train: 4.507575868117897e-09\n",
      "Step:122, train: 3.327697228883153e-09\n",
      "Step:123, train: 3.907601131558636e-09\n",
      "Step:124, train: 3.952004390406216e-09\n",
      "Step:125, train: 2.500607831912305e-09\n",
      "Step:126, train: 2.9192163131241955e-09\n",
      "Step:127, train: 4.9695216830514255e-09\n",
      "Step:128, train: 5.068744091119015e-09\n",
      "Step:129, train: 4.344184900695325e-09\n",
      "Step:130, train: 6.7110560442174005e-09\n",
      "Step:131, train: 1.1621709816722614e-08\n",
      "Step:132, train: 1.599926746642666e-08\n",
      "Step:133, train: 1.907541335999241e-08\n",
      "Step:134, train: 2.0344573248287645e-08\n",
      "Step:135, train: 1.7728169043884634e-08\n",
      "Step:136, train: 1.1801728483362695e-08\n",
      "Step:137, train: 7.432262583151328e-09\n",
      "Step:138, train: 8.04688182753921e-09\n",
      "Step:139, train: 1.1526913867854205e-08\n",
      "Step:140, train: 1.3536016751913849e-08\n",
      "Step:141, train: 1.2202428512075869e-08\n",
      "Step:142, train: 8.989126887293253e-09\n",
      "Step:143, train: 6.383885531313638e-09\n",
      "Step:144, train: 5.565575000865408e-09\n",
      "Step:145, train: 6.117247375314605e-09\n",
      "Step:146, train: 6.859626977551159e-09\n",
      "Step:147, train: 6.800781937599254e-09\n",
      "Step:148, train: 5.792683333005755e-09\n",
      "Step:149, train: 4.775195017181488e-09\n",
      "Step:150, train: 5.218602217027524e-09\n",
      "Step:151, train: 7.563529136334068e-09\n",
      "Step:152, train: 1.0193018207260707e-08\n",
      "Step:153, train: 1.0461166488617835e-08\n",
      "Step:154, train: 7.622678044327813e-09\n",
      "Step:155, train: 4.508112438905698e-09\n",
      "Step:156, train: 4.204290360299012e-09\n",
      "Step:157, train: 6.27616558723787e-09\n",
      "Step:158, train: 7.798680701220917e-09\n",
      "Step:159, train: 6.963253640357436e-09\n",
      "Step:160, train: 5.007277481539063e-09\n",
      "Step:161, train: 4.0353187458208595e-09\n",
      "Step:162, train: 4.207899029218254e-09\n",
      "Step:163, train: 4.2640992958808965e-09\n",
      "Step:164, train: 3.7241497663487166e-09\n",
      "Step:165, train: 3.4735336829072594e-09\n",
      "Step:166, train: 4.075258019931027e-09\n",
      "Step:167, train: 4.800696506990221e-09\n",
      "Step:168, train: 4.808398901268163e-09\n",
      "Step:169, train: 4.377419871026689e-09\n",
      "Step:170, train: 4.45751779931669e-09\n",
      "Step:171, train: 5.360927035624741e-09\n",
      "Step:172, train: 6.519283002326404e-09\n",
      "Step:173, train: 7.384362121776178e-09\n",
      "Step:174, train: 8.095215942027778e-09\n",
      "Step:175, train: 9.197466233068496e-09\n",
      "Step:176, train: 1.1064435812002671e-08\n",
      "Step:177, train: 1.395670867765375e-08\n",
      "Step:178, train: 1.8614978114506187e-08\n",
      "Step:179, train: 2.652078223697174e-08\n",
      "Step:180, train: 3.906730927649704e-08\n",
      "Step:181, train: 5.541963088440127e-08\n",
      "Step:182, train: 7.039496252581046e-08\n",
      "Step:183, train: 7.624093312230684e-08\n",
      "Step:184, train: 6.962417165023993e-08\n",
      "Step:185, train: 5.552213799830952e-08\n",
      "Step:186, train: 4.253576801893644e-08\n",
      "Step:187, train: 3.8882796205008674e-08\n",
      "Step:188, train: 4.800270614335744e-08\n",
      "Step:189, train: 5.9501642390991094e-08\n",
      "Step:190, train: 5.7139316500531834e-08\n",
      "Step:191, train: 3.968114115071586e-08\n",
      "Step:192, train: 2.4611960358633667e-08\n",
      "Step:193, train: 2.7028738802670205e-08\n",
      "Step:194, train: 3.922363245312255e-08\n",
      "Step:195, train: 4.1604062084310556e-08\n",
      "Step:196, train: 3.011153348886353e-08\n",
      "Step:197, train: 2.1059136656731425e-08\n",
      "Step:198, train: 2.4183272606670414e-08\n",
      "Step:199, train: 3.1197850636210944e-08\n",
      "Step:200, train: 3.333286369144872e-08\n",
      "Step:201, train: 3.179169727474118e-08\n",
      "Step:202, train: 2.8791644512971004e-08\n",
      "Step:203, train: 2.331546500311532e-08\n",
      "Step:204, train: 1.6691733994989022e-08\n",
      "Step:205, train: 1.327132914674678e-08\n",
      "Step:206, train: 1.5135721564973892e-08\n",
      "Step:207, train: 2.0367530884080054e-08\n",
      "Step:208, train: 2.4932974462821278e-08\n",
      "Step:209, train: 2.539126398026781e-08\n",
      "Step:210, train: 2.2139688526934265e-08\n",
      "Step:211, train: 1.8675964441605686e-08\n",
      "Step:212, train: 1.6966638871629414e-08\n",
      "Step:213, train: 1.6262046820259002e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:214, train: 1.6014201631442404e-08\n",
      "Step:215, train: 1.6643022737738988e-08\n",
      "Step:216, train: 1.7790837136821835e-08\n",
      "Step:217, train: 1.8187180428341776e-08\n",
      "Step:218, train: 1.7353881776038804e-08\n",
      "Step:219, train: 1.6122455148703807e-08\n",
      "Step:220, train: 1.5386493523728006e-08\n"
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
