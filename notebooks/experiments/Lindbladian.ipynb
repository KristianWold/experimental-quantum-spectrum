{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lindbladian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src_tf/')\n",
    "\n",
    "import numpy as np\n",
    "import qiskit as qk\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from qiskit.quantum_info import DensityMatrix, random_unitary\n",
    "from qiskit.quantum_info import Operator\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from loss_functions import *\n",
    "from optimization import *\n",
    "from quantum_channel import *\n",
    "from quantum_tools import *\n",
    "from experimental import *\n",
    "from spam import *\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "n = 2\n",
    "d = 2**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelQuantumMap(\n",
    "                       # channel = LindbladMap(d, rank=d**2),\n",
    "                        channel = KrausMap(d, rank=d**2),\n",
    "                        loss_function = Conj3(index = 1, sign =-1),\n",
    "                        optimizer = tf.optimizers.Adam(learning_rate=0.005),\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f807b0e15104930ab1f034dfc004187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: -0.0001418699213523943\n",
      "Step:1, train: -0.0002077159466784163\n",
      "Step:2, train: -0.00022946448184104647\n",
      "Step:3, train: -0.0002419461213667737\n",
      "Step:4, train: -0.00027960750766922916\n",
      "Step:5, train: -0.00033719193123189247\n",
      "Step:6, train: -0.0004122398173001311\n",
      "Step:7, train: -0.0004964359189398318\n",
      "Step:8, train: -0.000534773728923086\n",
      "Step:9, train: -0.0005707515274779834\n",
      "Step:10, train: -0.0006388650374267328\n",
      "Step:11, train: -0.0007069936235092311\n",
      "Step:12, train: -0.0007091592662494515\n",
      "Step:13, train: -0.0007455670309957648\n",
      "Step:14, train: -0.0007797938391596686\n",
      "Step:15, train: -0.000810824109518607\n",
      "Step:16, train: -0.0008619416872459445\n",
      "Step:17, train: -0.0009297996405620402\n",
      "Step:18, train: -0.0009586206955581254\n",
      "Step:19, train: -0.0009965588603652704\n",
      "Step:20, train: -0.0010139270697791134\n",
      "Step:21, train: -0.0010517809594427475\n",
      "Step:22, train: -0.0010770183639882353\n",
      "Step:23, train: -0.0011008859963854804\n",
      "Step:24, train: -0.0011282249813351687\n",
      "Step:25, train: -0.001176932277799206\n",
      "Step:26, train: -0.001200458986769809\n",
      "Step:27, train: -0.0012535225235357087\n",
      "Step:28, train: -0.001291080701437264\n",
      "Step:29, train: -0.0013367238680149214\n",
      "Step:30, train: -0.0013688021023975947\n",
      "Step:31, train: -0.0014271327128102598\n",
      "Step:32, train: -0.0014584462536531247\n",
      "Step:33, train: -0.0014825604153410035\n",
      "Step:34, train: -0.0015145478469271397\n",
      "Step:35, train: -0.0015698569001940295\n",
      "Step:36, train: -0.001592041686436136\n",
      "Step:37, train: -0.0016457551572760913\n",
      "Step:38, train: -0.0016919638147430758\n",
      "Step:39, train: -0.001726829974500416\n",
      "Step:40, train: -0.0017500233441275024\n",
      "Step:41, train: -0.00179968529317905\n",
      "Step:42, train: -0.00187137338323628\n",
      "Step:43, train: -0.0018963577111319263\n",
      "Step:44, train: -0.001958918717769078\n",
      "Step:45, train: -0.0019933076790089833\n",
      "Step:46, train: -0.0020416766200220928\n",
      "Step:47, train: -0.0020588711078735637\n",
      "Step:48, train: -0.002099382319618583\n",
      "Step:49, train: -0.002148955110854485\n",
      "Step:50, train: -0.0021921227537959513\n",
      "Step:51, train: -0.002245324497617138\n",
      "Step:52, train: -0.002264556598448367\n",
      "Step:53, train: -0.002298260345844148\n",
      "Step:54, train: -0.0023614885599449486\n",
      "Step:55, train: -0.002427261474745949\n",
      "Step:56, train: -0.0024603162082799854\n",
      "Step:57, train: -0.0024928307690612887\n",
      "Step:58, train: -0.0025405940403306604\n",
      "Step:59, train: -0.0025974797911281033\n",
      "Step:60, train: -0.002636708430600606\n",
      "Step:61, train: -0.002694233662390167\n",
      "Step:62, train: -0.00276394530308653\n",
      "Step:63, train: -0.0028092951078520093\n",
      "Step:64, train: -0.0028830023621935746\n",
      "Step:65, train: -0.002907344278850407\n",
      "Step:66, train: -0.002983359070716991\n",
      "Step:67, train: -0.003055083621114199\n",
      "Step:68, train: -0.0030742597600927936\n",
      "Step:69, train: -0.003134908210599821\n",
      "Step:70, train: -0.00313945744355249\n",
      "Step:71, train: -0.0032063914389513296\n",
      "Step:72, train: -0.0032899540955748465\n",
      "Step:73, train: -0.0033167357751979604\n",
      "Step:74, train: -0.003396102275331444\n",
      "Step:75, train: -0.003460436345549546\n",
      "Step:76, train: -0.003538651738330655\n",
      "Step:77, train: -0.0036364647051594524\n",
      "Step:78, train: -0.003700062073985042\n",
      "Step:79, train: -0.0037184796884625997\n",
      "Step:80, train: -0.0037789516027758423\n",
      "Step:81, train: -0.0038731296154024203\n",
      "Step:82, train: -0.003946462726149897\n",
      "Step:83, train: -0.0039900592520111175\n",
      "Step:84, train: -0.004088648812196477\n",
      "Step:85, train: -0.004165097046323329\n",
      "Step:86, train: -0.004216702088797415\n",
      "Step:87, train: -0.004294932084269407\n",
      "Step:88, train: -0.004344863919591297\n",
      "Step:89, train: -0.0044025702069122995\n",
      "Step:90, train: -0.00451043491501542\n",
      "Step:91, train: -0.004571163836780305\n",
      "Step:92, train: -0.004631104139293687\n",
      "Step:93, train: -0.004740615016786106\n",
      "Step:94, train: -0.004844733962185196\n",
      "Step:95, train: -0.004923877231144088\n",
      "Step:96, train: -0.004981282035172674\n",
      "Step:97, train: -0.00507855017306234\n",
      "Step:98, train: -0.005149638113044225\n",
      "Step:99, train: -0.0051954259914864485\n",
      "Step:100, train: -0.005319056835806454\n",
      "Step:101, train: -0.005409890271258896\n",
      "Step:102, train: -0.005500319852732862\n",
      "Step:103, train: -0.005603604406599859\n",
      "Step:104, train: -0.005693484766028796\n",
      "Step:105, train: -0.005754782371642226\n",
      "Step:106, train: -0.005843753424728983\n",
      "Step:107, train: -0.005928551654983521\n",
      "Step:108, train: -0.006068893155486398\n",
      "Step:109, train: -0.0061655893462217145\n",
      "Step:110, train: -0.006259854875841803\n",
      "Step:111, train: -0.006336246679045931\n",
      "Step:112, train: -0.00648572354629201\n",
      "Step:113, train: -0.006561972326282462\n",
      "Step:114, train: -0.006658321167529744\n",
      "Step:115, train: -0.006751662530903383\n",
      "Step:116, train: -0.006900385907406421\n",
      "Step:117, train: -0.007016097792571957\n",
      "Step:118, train: -0.007090400351173105\n",
      "Step:119, train: -0.007113954773772011\n",
      "Step:120, train: -0.007184285027697104\n",
      "Step:121, train: -0.007360819159746508\n",
      "Step:122, train: -0.007497102736818089\n",
      "Step:123, train: -0.007614880915839808\n",
      "Step:124, train: -0.007758303349895402\n",
      "Step:125, train: -0.007799744052806835\n",
      "Step:126, train: -0.007929868300956346\n",
      "Step:127, train: -0.007983117499552116\n",
      "Step:128, train: -0.008163792696007257\n",
      "Step:129, train: -0.008332251010893047\n",
      "Step:130, train: -0.008510665050124714\n",
      "Step:131, train: -0.008561503041499241\n",
      "Step:132, train: -0.008767501019193769\n",
      "Step:133, train: -0.008938236237394068\n",
      "Step:134, train: -0.009106144054915558\n",
      "Step:135, train: -0.0091048578956835\n",
      "Step:136, train: -0.00926185893039243\n",
      "Step:137, train: -0.009545749905640367\n",
      "Step:138, train: -0.009680646762626364\n",
      "Step:139, train: -0.009845849340169468\n",
      "Step:140, train: -0.010071474617982943\n",
      "Step:141, train: -0.010274347189691757\n",
      "Step:142, train: -0.010388319824705802\n",
      "Step:143, train: -0.010552454343003136\n",
      "Step:144, train: -0.010763101165243774\n",
      "Step:145, train: -0.01085048391053629\n",
      "Step:146, train: -0.011041024694001462\n",
      "Step:147, train: -0.011282373851742845\n",
      "Step:148, train: -0.011461819079233533\n",
      "Step:149, train: -0.01154050034183709\n",
      "Step:150, train: -0.011754817051803817\n",
      "Step:151, train: -0.012015556347534\n",
      "Step:152, train: -0.012090094478110117\n",
      "Step:153, train: -0.01231270407611371\n",
      "Step:154, train: -0.012514384716840332\n",
      "Step:155, train: -0.012692251450768657\n",
      "Step:156, train: -0.012993863319270613\n",
      "Step:157, train: -0.013204864341622493\n",
      "Step:158, train: -0.013532853955877455\n",
      "Step:159, train: -0.013778948359281985\n",
      "Step:160, train: -0.013929112391697201\n",
      "Step:161, train: -0.014058941228034727\n",
      "Step:162, train: -0.014308888900640323\n",
      "Step:163, train: -0.014616331618431152\n",
      "Step:164, train: -0.014891607107761076\n",
      "Step:165, train: -0.01513924861846188\n",
      "Step:166, train: -0.015275340116939023\n",
      "Step:167, train: -0.015531342241845643\n",
      "Step:168, train: -0.01597129705013071\n",
      "Step:169, train: -0.016270800107221154\n",
      "Step:170, train: -0.016484074390634953\n",
      "Step:171, train: -0.016778485557531704\n",
      "Step:172, train: -0.01688657707738375\n",
      "Step:173, train: -0.017294399624676382\n",
      "Step:174, train: -0.017517759887589263\n",
      "Step:175, train: -0.017621566241146986\n",
      "Step:176, train: -0.018005130962267826\n",
      "Step:177, train: -0.018311471278681945\n",
      "Step:178, train: -0.01864353954299663\n",
      "Step:179, train: -0.01909060197192187\n",
      "Step:180, train: -0.01938236691042366\n",
      "Step:181, train: -0.019804540244567\n",
      "Step:182, train: -0.020051504822427582\n",
      "Step:183, train: -0.020419317607170653\n",
      "Step:184, train: -0.02067448959551376\n",
      "Step:185, train: -0.021104359972513274\n",
      "Step:186, train: -0.021173936310655797\n",
      "Step:187, train: -0.021387571476042876\n",
      "Step:188, train: -0.02147571566655803\n",
      "Step:189, train: -0.02168878132516597\n",
      "Step:190, train: -0.022017079556673712\n",
      "Step:191, train: -0.022185141222238133\n",
      "Step:192, train: -0.02251457159095545\n",
      "Step:193, train: -0.02268445887726462\n",
      "Step:194, train: -0.02287192279520366\n",
      "Step:195, train: -0.023097456616972944\n",
      "Step:196, train: -0.023269487575287788\n",
      "Step:197, train: -0.02368926168115759\n",
      "Step:198, train: -0.02406502843708583\n",
      "Step:199, train: -0.024283334328704154\n",
      "Step:200, train: -0.024432163863659484\n",
      "Step:201, train: -0.024475548480067204\n",
      "Step:202, train: -0.024705754858002842\n",
      "Step:203, train: -0.025117038622857184\n",
      "Step:204, train: -0.025655169092564125\n",
      "Step:205, train: -0.026005546330518044\n",
      "Step:206, train: -0.02612934656451312\n",
      "Step:207, train: -0.02658029583576499\n",
      "Step:208, train: -0.026844834269236804\n",
      "Step:209, train: -0.02712166893430618\n",
      "Step:210, train: -0.027265473778748373\n",
      "Step:211, train: -0.027637449567868563\n",
      "Step:212, train: -0.028116806714675893\n",
      "Step:213, train: -0.028477684797354227\n",
      "Step:214, train: -0.028742956794638532\n",
      "Step:215, train: -0.028953220572798654\n",
      "Step:216, train: -0.029339126054703738\n",
      "Step:217, train: -0.02990565398656004\n",
      "Step:218, train: -0.03008038272272897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:219, train: -0.030418632593750765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\optimization.py:81\u001b[0m, in \u001b[0;36mModelQuantumMap.train\u001b[1;34m(self, inputs, targets, inputs_val, targets_val, num_iter, N)\u001b[0m\n\u001b[0;32m     79\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loss_function \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function: \n\u001b[1;32m---> 81\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel\u001b[38;5;241m.\u001b[39mparameter_list))\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\loss_functions.py:221\u001b[0m, in \u001b[0;36mConj3.__call__\u001b[1;34m(self, channel, input, target)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, channel, \u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m    220\u001b[0m     d \u001b[38;5;241m=\u001b[39m channel\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m--> 221\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mchannel_spectrum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign\u001b[38;5;241m*\u001b[39mtf\u001b[38;5;241m.\u001b[39mabs(z[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mabs(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_prod(z))\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:89\u001b[0m, in \u001b[0;36mchannel_spectrum\u001b[1;34m(channel, real)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchannel_spectrum\u001b[39m(channel, real\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 89\u001b[0m     eig, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meig(reshuffle(\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoi\u001b[49m))\n\u001b[0;32m     90\u001b[0m     eig \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(eig, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m real:\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:201\u001b[0m, in \u001b[0;36mKrausMap.choi\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;129m@property\u001b[39m    \n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoi\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkraus_to_choi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\ModelNISQ\\notebooks\\experiments\\../../src_tf\\quantum_channel.py:34\u001b[0m, in \u001b[0;36mkraus_to_choi\u001b[1;34m(kraus_channel, use_reshuffle)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rank):\n\u001b[0;32m     33\u001b[0m     K \u001b[38;5;241m=\u001b[39m kraus[\u001b[38;5;241m0\u001b[39m, i]\n\u001b[1;32m---> 34\u001b[0m     channel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reshuffle:\n\u001b[0;32m     37\u001b[0m     choi \u001b[38;5;241m=\u001b[39m reshuffle(channel)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:409\u001b[0m, in \u001b[0;36mkron\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    407\u001b[0m b_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(t_b)\n\u001b[0;32m    408\u001b[0m a_reshaped \u001b[38;5;241m=\u001b[39m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(t_a, _make_shape(a_shape, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 409\u001b[0m b_reshaped \u001b[38;5;241m=\u001b[39m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(t_b, \u001b[43m_make_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    410\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m a_shape \u001b[38;5;241m*\u001b[39m b_shape\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np_array_ops\u001b[38;5;241m.\u001b[39mreshape(a_reshaped \u001b[38;5;241m*\u001b[39m b_reshaped, out_shape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:404\u001b[0m, in \u001b[0;36mkron.<locals>._make_shape\u001b[1;34m(shape, prepend)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m   shapes \u001b[38;5;241m=\u001b[39m [shape, ones]\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39mreshape(\u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1480\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1476\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mexpanded_num_dims \u001b[38;5;129;01mor\u001b[39;00m axis \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m expanded_num_dims:\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `axis` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in range \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1478\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m-\u001b[39mexpanded_num_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_num_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_qiskit\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:6543\u001b[0m, in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   6542\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6543\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6544\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maxis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6546\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_function = [Conj3(index = 1, sign = 1)]\n",
    "model.zero_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1063a5d93f4bec90a668ffd493a947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, train: 0.030713805290953137\n",
      "Step:1, train: 0.028699223894213064\n",
      "Step:2, train: 0.02678004630193693\n",
      "Step:3, train: 0.024962899238517564\n",
      "Step:4, train: 0.023251341638317893\n",
      "Step:5, train: 0.021646327481176137\n",
      "Step:6, train: 0.02014680975901247\n",
      "Step:7, train: 0.018750028862025432\n",
      "Step:8, train: 0.01745177296493724\n",
      "Step:9, train: 0.01624673617991047\n",
      "Step:10, train: 0.015128911364634198\n",
      "Step:11, train: 0.014092019432391927\n",
      "Step:12, train: 0.01312985724684231\n",
      "Step:13, train: 0.012236527562145365\n",
      "Step:14, train: 0.011406564196958898\n",
      "Step:15, train: 0.0106349645879232\n",
      "Step:16, train: 0.00991718385842708\n",
      "Step:17, train: 0.009249102735846101\n",
      "Step:18, train: 0.008626987677849236\n",
      "Step:19, train: 0.008047449991388967\n",
      "Step:20, train: 0.0075074000500233605\n",
      "Step:21, train: 0.00700401067240871\n",
      "Step:22, train: 0.006534687076137381\n",
      "Step:23, train: 0.0060970376976881785\n",
      "Step:24, train: 0.005688852428500489\n",
      "Step:25, train: 0.005308083379575189\n",
      "Step:26, train: 0.004952830995845784\n",
      "Step:27, train: 0.004621329932816632\n",
      "Step:28, train: 0.0043119383871786125\n",
      "Step:29, train: 0.00402312869939265\n",
      "Step:30, train: 0.003753479889237004\n",
      "Step:31, train: 0.0035016693489768637\n",
      "Step:32, train: 0.003266467286996281\n",
      "Step:33, train: 0.003046730105904819\n",
      "Step:34, train: 0.0028413958307183466\n",
      "Step:35, train: 0.002649479021940374\n",
      "Step:36, train: 0.0024700657961952595\n",
      "Step:37, train: 0.0023023093977028314\n",
      "Step:38, train: 0.002145427376560957\n",
      "Step:39, train: 0.0019986969241526337\n",
      "Step:40, train: 0.0018614512688793564\n",
      "Step:41, train: 0.001733076714844019\n",
      "Step:42, train: 0.0016130100134534033\n",
      "Step:43, train: 0.001500733960595566\n",
      "Step:44, train: 0.0013957752851221543\n",
      "Step:45, train: 0.0012977006856664285\n",
      "Step:46, train: 0.0012061142584906678\n",
      "Step:47, train: 0.0011206528938895294\n",
      "Step:48, train: 0.0010409830671553313\n",
      "Step:49, train: 0.0009667965548800567\n",
      "Step:50, train: 0.0008978057051592736\n",
      "Step:51, train: 0.0008337393958843265\n",
      "Step:52, train: 0.0007743386157182041\n",
      "Step:53, train: 0.0007193521939009627\n",
      "Step:54, train: 0.0006685341856160352\n",
      "Step:55, train: 0.0006216410747744864\n",
      "Step:56, train: 0.0005784307153850991\n",
      "Step:57, train: 0.0005386623780162794\n",
      "Step:58, train: 0.0005020977592770407\n",
      "Step:59, train: 0.00046850226833840074\n",
      "Step:60, train: 0.0004376473289350824\n",
      "Step:61, train: 0.0004093125750005385\n",
      "Step:62, train: 0.00038328759717597453\n",
      "Step:63, train: 0.00035937375129369427\n",
      "Step:64, train: 0.0003373852302424211\n",
      "Step:65, train: 0.00031714960050424694\n",
      "Step:66, train: 0.00029850811634024636\n",
      "Step:67, train: 0.00028131555141089126\n",
      "Step:68, train: 0.0002654395492664242\n",
      "Step:69, train: 0.000250760049512804\n",
      "Step:70, train: 0.00023716848552494526\n",
      "Step:71, train: 0.00022456674323540087\n",
      "Step:72, train: 0.00021286644651200354\n",
      "Step:73, train: 0.00020198795748999838\n",
      "Step:74, train: 0.00019185953134892396\n",
      "Step:75, train: 0.00018241655121514725\n",
      "Step:76, train: 0.00017360075564993959\n",
      "Step:77, train: 0.000165359636818057\n",
      "Step:78, train: 0.00015764581663049837\n",
      "Step:79, train: 0.00015041643041201335\n",
      "Step:80, train: 0.00014363272608031475\n",
      "Step:81, train: 0.00013725961757850224\n",
      "Step:82, train: 0.0001312652805223698\n",
      "Step:83, train: 0.00012562079883430863\n",
      "Step:84, train: 0.0001202999017818734\n",
      "Step:85, train: 0.00011527863426751205\n",
      "Step:86, train: 0.00011053520480043327\n",
      "Step:87, train: 0.00010604966416958416\n",
      "Step:88, train: 0.00010180380301283694\n",
      "Step:89, train: 9.778099194370692e-05\n",
      "Step:90, train: 9.396592685788242e-05\n",
      "Step:91, train: 9.034461932483473e-05\n",
      "Step:92, train: 8.690416576131901e-05\n",
      "Step:93, train: 8.363274981230876e-05\n",
      "Step:94, train: 8.051946505962271e-05\n",
      "Step:95, train: 7.755426755630052e-05\n",
      "Step:96, train: 7.472787045087755e-05\n",
      "Step:97, train: 7.20317247072991e-05\n",
      "Step:98, train: 6.945790324690538e-05\n",
      "Step:99, train: 6.699909773662161e-05\n",
      "Step:100, train: 6.464848343098148e-05\n",
      "Step:101, train: 6.239977570666548e-05\n",
      "Step:102, train: 6.024711390835152e-05\n",
      "Step:103, train: 5.8185059860699066e-05\n",
      "Step:104, train: 5.6208575525544256e-05\n",
      "Step:105, train: 5.431294216461655e-05\n",
      "Step:106, train: 5.249379062719081e-05\n",
      "Step:107, train: 5.0747014451999805e-05\n",
      "Step:108, train: 4.9068818494828567e-05\n",
      "Step:109, train: 4.7455628331078355e-05\n",
      "Step:110, train: 4.5904104008346464e-05\n",
      "Step:111, train: 4.441114173975708e-05\n",
      "Step:112, train: 4.2973808821055606e-05\n",
      "Step:113, train: 4.158938256335228e-05\n",
      "Step:114, train: 4.0255304389033435e-05\n",
      "Step:115, train: 3.896915091673743e-05\n",
      "Step:116, train: 3.7728688933663964e-05\n",
      "Step:117, train: 3.653176865391325e-05\n",
      "Step:118, train: 3.537640716963993e-05\n",
      "Step:119, train: 3.426072567548476e-05\n",
      "Step:120, train: 3.318294746841297e-05\n",
      "Step:121, train: 3.214141441745789e-05\n",
      "Step:122, train: 3.1134547283054145e-05\n",
      "Step:123, train: 3.016087637201947e-05\n",
      "Step:124, train: 2.9218993470185722e-05\n",
      "Step:125, train: 2.8307569536229893e-05\n",
      "Step:126, train: 2.7425359002719174e-05\n",
      "Step:127, train: 2.65711786206075e-05\n",
      "Step:128, train: 2.574390856322686e-05\n",
      "Step:129, train: 2.4942485345416776e-05\n",
      "Step:130, train: 2.4165911160168294e-05\n",
      "Step:131, train: 2.3413236658802185e-05\n",
      "Step:132, train: 2.2683540685818196e-05\n",
      "Step:133, train: 2.1975986755615713e-05\n",
      "Step:134, train: 2.1289747173976453e-05\n",
      "Step:135, train: 2.0624050185302987e-05\n",
      "Step:136, train: 1.997816058649545e-05\n",
      "Step:137, train: 1.935137218542159e-05\n",
      "Step:138, train: 1.8743024733980326e-05\n",
      "Step:139, train: 1.8152489468542745e-05\n",
      "Step:140, train: 1.7579156659227412e-05\n",
      "Step:141, train: 1.7022448879066008e-05\n",
      "Step:142, train: 1.648181950094543e-05\n",
      "Step:143, train: 1.595674502516692e-05\n",
      "Step:144, train: 1.5446719869725815e-05\n",
      "Step:145, train: 1.4951265852260494e-05\n",
      "Step:146, train: 1.4469932950541304e-05\n",
      "Step:147, train: 1.4002277372008778e-05\n",
      "Step:148, train: 1.3547880417520298e-05\n",
      "Step:149, train: 1.310634911127184e-05\n",
      "Step:150, train: 1.2677300187196016e-05\n",
      "Step:151, train: 1.2260361079366567e-05\n",
      "Step:152, train: 1.1855178931040243e-05\n",
      "Step:153, train: 1.1461420301086358e-05\n",
      "Step:154, train: 1.1078763198142038e-05\n",
      "Step:155, train: 1.0706895865492692e-05\n",
      "Step:156, train: 1.0345514496798531e-05\n",
      "Step:157, train: 9.994331919094135e-06\n",
      "Step:158, train: 9.653077352249974e-06\n",
      "Step:159, train: 9.321479635214827e-06\n",
      "Step:160, train: 8.999288685830207e-06\n",
      "Step:161, train: 8.686251797085303e-06\n",
      "Step:162, train: 8.382136507790454e-06\n",
      "Step:163, train: 8.08671196546151e-06\n",
      "Step:164, train: 7.799752606543007e-06\n",
      "Step:165, train: 7.5210538587919885e-06\n",
      "Step:166, train: 7.250399503416652e-06\n",
      "Step:167, train: 6.987596473839499e-06\n",
      "Step:168, train: 6.732448926274089e-06\n",
      "Step:169, train: 6.48476605310461e-06\n",
      "Step:170, train: 6.2443695286029785e-06\n",
      "Step:171, train: 6.0110856751695995e-06\n",
      "Step:172, train: 5.78474091066481e-06\n",
      "Step:173, train: 5.565169346823678e-06\n",
      "Step:174, train: 5.352208595238545e-06\n",
      "Step:175, train: 5.14570399908416e-06\n",
      "Step:176, train: 4.945502235718191e-06\n",
      "Step:177, train: 4.751455462473808e-06\n",
      "Step:178, train: 4.5634194548303005e-06\n",
      "Step:179, train: 4.381249109694085e-06\n",
      "Step:180, train: 4.204810704794921e-06\n",
      "Step:181, train: 4.033970410967853e-06\n",
      "Step:182, train: 3.868593556682219e-06\n",
      "Step:183, train: 3.7085527423513186e-06\n",
      "Step:184, train: 3.5537215896453136e-06\n",
      "Step:185, train: 3.403977696975431e-06\n",
      "Step:186, train: 3.259196953730578e-06\n",
      "Step:187, train: 3.1192634691007002e-06\n",
      "Step:188, train: 2.9840603079321323e-06\n",
      "Step:189, train: 2.85347489975187e-06\n",
      "Step:190, train: 2.7273921272539026e-06\n",
      "Step:191, train: 2.605708095877323e-06\n",
      "Step:192, train: 2.4883114036463682e-06\n",
      "Step:193, train: 2.375094646161942e-06\n",
      "Step:194, train: 2.265950714265397e-06\n",
      "Step:195, train: 2.1607794557338535e-06\n",
      "Step:196, train: 2.0594786307328564e-06\n",
      "Step:197, train: 1.9619489319635385e-06\n",
      "Step:198, train: 1.8680939669510605e-06\n",
      "Step:199, train: 1.7778142406806233e-06\n",
      "Step:200, train: 1.6910152468003576e-06\n",
      "Step:201, train: 1.6076005467769307e-06\n",
      "Step:202, train: 1.5274784909909283e-06\n",
      "Step:203, train: 1.4505573885392054e-06\n",
      "Step:204, train: 1.3767473123254622e-06\n",
      "Step:205, train: 1.3059560552704208e-06\n",
      "Step:206, train: 1.2381014600856874e-06\n",
      "Step:207, train: 1.173093773644298e-06\n",
      "Step:208, train: 1.1108490711509049e-06\n",
      "Step:209, train: 1.051282973691829e-06\n",
      "Step:210, train: 9.943125778677109e-07\n",
      "Step:211, train: 9.398545139765425e-07\n",
      "Step:212, train: 8.87831909974976e-07\n",
      "Step:213, train: 8.381638312667276e-07\n",
      "Step:214, train: 7.907735223221989e-07\n",
      "Step:215, train: 1.286795107756627e-06\n",
      "Step:216, train: 1.178010348901123e-06\n",
      "Step:217, train: 7.90997990232629e-07\n",
      "Step:218, train: 8.292536912039909e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:219, train: 8.605781996605048e-07\n",
      "Step:220, train: 8.853031601082331e-07\n",
      "Step:221, train: 9.038150691932643e-07\n",
      "Step:222, train: 9.165446190067548e-07\n",
      "Step:223, train: 9.239409585380534e-07\n",
      "Step:224, train: 9.264631408256592e-07\n",
      "Step:225, train: 9.245646092958362e-07\n",
      "Step:226, train: 9.186916131948694e-07\n",
      "Step:227, train: 9.092731917829219e-07\n",
      "Step:228, train: 8.967200847352295e-07\n",
      "Step:229, train: 8.814217789732307e-07\n",
      "Step:230, train: 8.637431594862668e-07\n",
      "Step:231, train: 8.440271806431791e-07\n",
      "Step:232, train: 8.225891932764927e-07\n",
      "Step:233, train: 7.997208169566883e-07\n",
      "Step:234, train: 7.756910699395151e-07\n",
      "Step:235, train: 7.507458994213094e-07\n",
      "Step:236, train: 7.251083061718589e-07\n",
      "Step:237, train: 6.989802546778814e-07\n",
      "Step:238, train: 6.725433052246984e-07\n",
      "Step:239, train: 6.459611075475691e-07\n",
      "Step:240, train: 9.16099524902623e-07\n",
      "Step:241, train: 6.390759680943775e-07\n",
      "Step:242, train: 6.539740624637857e-07\n",
      "Step:243, train: 6.643852663141969e-07\n",
      "Step:244, train: 6.706430544452204e-07\n",
      "Step:245, train: 6.73093972207132e-07\n",
      "Step:246, train: 6.720858757344272e-07\n",
      "Step:247, train: 6.679658150498919e-07\n",
      "Step:248, train: 6.610687383197075e-07\n",
      "Step:249, train: 6.517181243466958e-07\n",
      "Step:250, train: 6.402184206264717e-07\n",
      "Step:251, train: 6.268617920721607e-07\n",
      "Step:252, train: 6.119173555903982e-07\n",
      "Step:253, train: 5.956368687729747e-07\n",
      "Step:254, train: 5.78254466756485e-07\n",
      "Step:255, train: 7.918943427752886e-07\n",
      "Step:256, train: 5.878264826890622e-07\n",
      "Step:257, train: 6.105026721524957e-07\n",
      "Step:258, train: 6.28238070925262e-07\n",
      "Step:259, train: 6.413169037935155e-07\n",
      "Step:260, train: 6.500554573080429e-07\n",
      "Step:261, train: 6.547892394102586e-07\n",
      "Step:262, train: 6.558645273852922e-07\n",
      "Step:263, train: 6.53629983546997e-07\n",
      "Step:264, train: 6.484255759086807e-07\n",
      "Step:265, train: 6.405830745932754e-07\n",
      "Step:266, train: 6.304189046451144e-07\n",
      "Step:267, train: 6.182332517478289e-07\n",
      "Step:268, train: 6.04310291616171e-07\n",
      "Step:269, train: 5.889135108971168e-07\n",
      "Step:270, train: 5.722883879840075e-07\n",
      "Step:271, train: 5.54661407819781e-07\n",
      "Step:272, train: 5.362390261117396e-07\n",
      "Step:273, train: 5.17210754238746e-07\n",
      "Step:274, train: 6.037648601806565e-07\n",
      "Step:275, train: 5.456993311935435e-07\n",
      "Step:276, train: 5.877605034258792e-07\n",
      "Step:277, train: 6.238331824622992e-07\n",
      "Step:278, train: 6.539805427276836e-07\n",
      "Step:279, train: 6.783845975185457e-07\n",
      "Step:280, train: 6.973070668328277e-07\n",
      "Step:281, train: 7.110720360601955e-07\n",
      "Step:282, train: 7.200405745370818e-07\n",
      "Step:283, train: 7.245895125272402e-07\n",
      "Step:284, train: 7.251084242427516e-07\n",
      "Step:285, train: 7.219882586165242e-07\n",
      "Step:286, train: 7.156082657520693e-07\n",
      "Step:287, train: 7.063401056083484e-07\n",
      "Step:288, train: 6.945347081337814e-07\n",
      "Step:289, train: 6.805295605695188e-07\n",
      "Step:290, train: 6.646369778774744e-07\n",
      "Step:291, train: 6.471505775251013e-07\n",
      "Step:292, train: 6.283437982351368e-07\n",
      "Step:293, train: 6.084642221411193e-07\n",
      "Step:294, train: 5.877427149442789e-07\n",
      "Step:295, train: 5.663860988250045e-07\n",
      "Step:296, train: 5.445852677441428e-07\n",
      "Step:297, train: 5.225086186850909e-07\n",
      "Step:298, train: 5.003091365690912e-07\n",
      "Step:299, train: 4.781224223944217e-07\n",
      "Step:300, train: 4.5606785598603114e-07\n",
      "Step:301, train: 4.342497617459015e-07\n",
      "Step:302, train: 4.7135822253932977e-07\n",
      "Step:303, train: 4.728235090140143e-07\n",
      "Step:304, train: 5.274955869652269e-07\n",
      "Step:305, train: 5.762076227129367e-07\n",
      "Step:306, train: 6.186793554374387e-07\n",
      "Step:307, train: 6.548468647032588e-07\n",
      "Step:308, train: 6.848040954514059e-07\n",
      "Step:309, train: 7.087652349446434e-07\n",
      "Step:310, train: 7.270251205133239e-07\n",
      "Step:311, train: 7.399343016270143e-07\n",
      "Step:312, train: 7.478811849740693e-07\n",
      "Step:313, train: 7.512718860404442e-07\n",
      "Step:314, train: 7.505211462447522e-07\n",
      "Step:315, train: 7.460407189081728e-07\n",
      "Step:316, train: 7.382347725831639e-07\n",
      "Step:317, train: 7.274909887762239e-07\n",
      "Step:318, train: 7.1418101106642e-07\n",
      "Step:319, train: 6.986567666244296e-07\n",
      "Step:320, train: 6.812445987024947e-07\n",
      "Step:321, train: 6.622531696923844e-07\n",
      "Step:322, train: 6.419640221183812e-07\n",
      "Step:323, train: 6.206366833554322e-07\n",
      "Step:324, train: 5.985087731682227e-07\n",
      "Step:325, train: 5.757949885704335e-07\n",
      "Step:326, train: 5.526919091244667e-07\n",
      "Step:327, train: 5.293733442300487e-07\n",
      "Step:328, train: 5.059951799884634e-07\n",
      "Step:329, train: 4.826948505041262e-07\n",
      "Step:330, train: 4.595932338929162e-07\n",
      "Step:331, train: 4.367973939133625e-07\n",
      "Step:332, train: 4.143987962775691e-07\n",
      "Step:333, train: 3.924749402247905e-07\n",
      "Step:334, train: 3.71093274639184e-07\n",
      "Step:335, train: 3.503086784368572e-07\n",
      "Step:336, train: 3.301664950765181e-07\n",
      "Step:337, train: 5.912014290439317e-07\n",
      "Step:338, train: 5.473536381308505e-07\n",
      "Step:339, train: 3.334470005367221e-07\n",
      "Step:340, train: 3.514836620894071e-07\n",
      "Step:341, train: 3.661198099172103e-07\n",
      "Step:342, train: 3.7748250579335377e-07\n",
      "Step:343, train: 3.857459492533459e-07\n",
      "Step:344, train: 3.911131226035037e-07\n",
      "Step:345, train: 3.9380682197146416e-07\n",
      "Step:346, train: 3.9405966061568915e-07\n",
      "Step:347, train: 3.9210740856199043e-07\n",
      "Step:348, train: 3.881848141263607e-07\n",
      "Step:349, train: 3.825211058031427e-07\n",
      "Step:350, train: 3.7533554910031027e-07\n",
      "Step:351, train: 3.668389799443318e-07\n",
      "Step:352, train: 3.5722861512004345e-07\n",
      "Step:353, train: 3.4668973043542074e-07\n",
      "Step:354, train: 3.3539244406552235e-07\n",
      "Step:355, train: 3.234948477201479e-07\n",
      "Step:356, train: 3.1114008454602884e-07\n",
      "Step:357, train: 2.9845875088244047e-07\n",
      "Step:358, train: 3.1074342015648834e-07\n",
      "Step:359, train: 3.66080746527297e-07\n",
      "Step:360, train: 4.441869976038417e-07\n",
      "Step:361, train: 5.177837608040112e-07\n",
      "Step:362, train: 5.854507586190266e-07\n",
      "Step:363, train: 6.462888619308467e-07\n",
      "Step:364, train: 6.997998977033765e-07\n",
      "Step:365, train: 7.457855110072659e-07\n",
      "Step:366, train: 7.842728317988704e-07\n",
      "Step:367, train: 8.154572665747861e-07\n",
      "Step:368, train: 8.396519053958711e-07\n",
      "Step:369, train: 8.572530826871163e-07\n",
      "Step:370, train: 8.687107957756256e-07\n",
      "Step:371, train: 8.745102507190984e-07\n",
      "Step:372, train: 8.751503292021768e-07\n",
      "Step:373, train: 8.711301235470452e-07\n",
      "Step:374, train: 8.629469618424557e-07\n",
      "Step:375, train: 8.510783254151484e-07\n",
      "Step:376, train: 8.359839718718685e-07\n",
      "Step:377, train: 8.180970600113701e-07\n",
      "Step:378, train: 7.97830562714919e-07\n",
      "Step:379, train: 7.755633701433783e-07\n",
      "Step:380, train: 7.516503585674911e-07\n",
      "Step:381, train: 7.264184205328576e-07\n",
      "Step:382, train: 7.001635057078785e-07\n",
      "Step:383, train: 6.731568008450825e-07\n",
      "Step:384, train: 6.456425429414194e-07\n",
      "Step:385, train: 6.178384582761657e-07\n",
      "Step:386, train: 5.899412361142223e-07\n",
      "Step:387, train: 5.62124354841681e-07\n",
      "Step:388, train: 5.345409050220353e-07\n",
      "Step:389, train: 5.073225152600981e-07\n",
      "Step:390, train: 4.805851298419725e-07\n",
      "Step:391, train: 4.5442734726066093e-07\n",
      "Step:392, train: 4.2893288347613564e-07\n",
      "Step:393, train: 4.041691854287956e-07\n",
      "Step:394, train: 3.801934915856073e-07\n",
      "Step:395, train: 3.570502540776337e-07\n",
      "Step:396, train: 3.347723497410028e-07\n",
      "Step:397, train: 3.13385995010994e-07\n",
      "Step:398, train: 2.92906189719108e-07\n",
      "Step:399, train: 2.7334303434834244e-07\n",
      "Step:400, train: 2.546972658817581e-07\n",
      "Step:401, train: 2.3696617005397994e-07\n",
      "Step:402, train: 2.2014060588362846e-07\n",
      "Step:403, train: 2.761354466845523e-07\n",
      "Step:404, train: 2.118626981619156e-07\n",
      "Step:405, train: 2.175834856363759e-07\n",
      "Step:406, train: 2.2147982516511615e-07\n",
      "Step:407, train: 2.236801246003413e-07\n",
      "Step:408, train: 2.2432370098504052e-07\n",
      "Step:409, train: 2.2355564274055717e-07\n",
      "Step:410, train: 2.2152296541839306e-07\n",
      "Step:411, train: 2.183705450934071e-07\n",
      "Step:412, train: 2.1423975279641835e-07\n",
      "Step:413, train: 2.0926575457970887e-07\n",
      "Step:414, train: 2.0357618823240794e-07\n",
      "Step:415, train: 1.972916493865554e-07\n",
      "Step:416, train: 2.521820096299861e-07\n",
      "Step:417, train: 2.065621332900427e-07\n",
      "Step:418, train: 2.2023687114532835e-07\n",
      "Step:419, train: 2.3153738200047807e-07\n",
      "Step:420, train: 2.4051495230252485e-07\n",
      "Step:421, train: 2.472614660716095e-07\n",
      "Step:422, train: 2.5190386410539595e-07\n",
      "Step:423, train: 2.5458642126121336e-07\n",
      "Step:424, train: 2.554683047307372e-07\n",
      "Step:425, train: 2.547155921868023e-07\n",
      "Step:426, train: 2.5249599434838263e-07\n",
      "Step:427, train: 2.489777039222659e-07\n",
      "Step:428, train: 2.4432262529957205e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:429, train: 2.386872951411324e-07\n",
      "Step:430, train: 2.3221866561367886e-07\n",
      "Step:431, train: 2.2505675138025286e-07\n",
      "Step:432, train: 2.1733085263476447e-07\n",
      "Step:433, train: 2.0916066854762598e-07\n",
      "Step:434, train: 2.0065472951419992e-07\n",
      "Step:435, train: 1.919118999820072e-07\n",
      "Step:436, train: 1.8302208238395936e-07\n",
      "Step:437, train: 1.7406495278458124e-07\n",
      "Step:438, train: 3.071534892434456e-07\n",
      "Step:439, train: 2.508188371799382e-07\n",
      "Step:440, train: 1.8718125841044604e-07\n",
      "Step:441, train: 2.036301699773594e-07\n",
      "Step:442, train: 2.1769386120709272e-07\n",
      "Step:443, train: 2.2935269004823362e-07\n",
      "Step:444, train: 2.386480087662483e-07\n",
      "Step:445, train: 2.4566764241546556e-07\n",
      "Step:446, train: 2.505338819760762e-07\n",
      "Step:447, train: 2.5339311003395054e-07\n",
      "Step:448, train: 2.5440526766153797e-07\n",
      "Step:449, train: 2.5373957676051053e-07\n",
      "Step:450, train: 2.5156800386721793e-07\n",
      "Step:451, train: 2.480620775603205e-07\n",
      "Step:452, train: 2.433879425323428e-07\n",
      "Step:453, train: 2.3770663710173372e-07\n",
      "Step:454, train: 2.3117109533390915e-07\n",
      "Step:455, train: 2.2392489982031202e-07\n",
      "Step:456, train: 2.1610132901105588e-07\n",
      "Step:457, train: 2.078234550888685e-07\n",
      "Step:458, train: 1.992042550228787e-07\n",
      "Step:459, train: 1.9034592513617664e-07\n",
      "Step:460, train: 1.8134064895688598e-07\n",
      "Step:461, train: 1.7227101653099937e-07\n",
      "Step:462, train: 1.6320994264688077e-07\n",
      "Step:463, train: 1.5422026148732948e-07\n",
      "Step:464, train: 2.65444638238173e-07\n",
      "Step:465, train: 2.208170547751261e-07\n",
      "Step:466, train: 1.643842365615081e-07\n",
      "Step:467, train: 1.7881531879855214e-07\n",
      "Step:468, train: 1.9111804500655763e-07\n",
      "Step:469, train: 2.0127337196655872e-07\n",
      "Step:470, train: 2.0931818803253095e-07\n",
      "Step:471, train: 2.1533228589683993e-07\n",
      "Step:472, train: 2.1942674650679334e-07\n",
      "Step:473, train: 2.217331925628482e-07\n",
      "Step:474, train: 2.223971007304856e-07\n",
      "Step:475, train: 2.215710840875968e-07\n",
      "Step:476, train: 2.1941083760222602e-07\n",
      "Step:477, train: 2.160713118266703e-07\n",
      "Step:478, train: 2.117043340308352e-07\n",
      "Step:479, train: 2.0645468100926007e-07\n",
      "Step:480, train: 2.004599043129995e-07\n",
      "Step:481, train: 1.9384996662952665e-07\n",
      "Step:482, train: 1.86746040236928e-07\n",
      "Step:483, train: 1.7925740411162585e-07\n",
      "Step:484, train: 1.7148548078370723e-07\n",
      "Step:485, train: 1.6352255444169361e-07\n",
      "Step:486, train: 1.5545113656101525e-07\n",
      "Step:487, train: 1.473441409089536e-07\n",
      "Step:488, train: 1.3926592791672128e-07\n",
      "Step:489, train: 2.0185207398674084e-07\n",
      "Step:490, train: 1.3614626791422942e-07\n",
      "Step:491, train: 1.3968930915843668e-07\n",
      "Step:492, train: 1.419792135873384e-07\n",
      "Step:493, train: 1.431063998380581e-07\n",
      "Step:494, train: 1.431705137295763e-07\n",
      "Step:495, train: 1.4227384342366347e-07\n",
      "Step:496, train: 1.4052204011432987e-07\n",
      "Step:497, train: 1.380188130451452e-07\n",
      "Step:498, train: 1.348653189734303e-07\n",
      "Step:499, train: 1.3115918478888863e-07\n",
      "Step:500, train: 1.5538825963044056e-07\n",
      "Step:501, train: 1.4485010617734627e-07\n",
      "Step:502, train: 1.608706131694496e-07\n",
      "Step:503, train: 1.7486519938981187e-07\n",
      "Step:504, train: 1.8674265423100972e-07\n",
      "Step:505, train: 1.9648577330591905e-07\n",
      "Step:506, train: 2.0413351019303499e-07\n",
      "Step:507, train: 2.0976803647892341e-07\n",
      "Step:508, train: 2.1350285057115584e-07\n",
      "Step:509, train: 2.1547063983290557e-07\n",
      "Step:510, train: 2.1581968283705675e-07\n",
      "Step:511, train: 2.1470431287974277e-07\n",
      "Step:512, train: 2.1228086616343309e-07\n",
      "Step:513, train: 2.0870553190415408e-07\n",
      "Step:514, train: 2.0413021237161775e-07\n",
      "Step:515, train: 1.98701266564689e-07\n",
      "Step:516, train: 1.9255754186108792e-07\n",
      "Step:517, train: 1.8582821822201478e-07\n",
      "Step:518, train: 1.7863486257319647e-07\n",
      "Step:519, train: 1.710864385032625e-07\n",
      "Step:520, train: 1.6328449836079596e-07\n",
      "Step:521, train: 1.5531947078377883e-07\n",
      "Step:522, train: 1.4727316125970456e-07\n",
      "Step:523, train: 1.3921779230342037e-07\n",
      "Step:524, train: 1.3121667721675784e-07\n",
      "Step:525, train: 1.233244436766791e-07\n",
      "Step:526, train: 1.1558823265552568e-07\n",
      "Step:527, train: 1.973956985202301e-07\n",
      "Step:528, train: 1.6914129089513914e-07\n",
      "Step:529, train: 1.2180626122702764e-07\n",
      "Step:530, train: 1.3237415704649015e-07\n",
      "Step:531, train: 1.413081020125379e-07\n",
      "Step:532, train: 1.4859446108133048e-07\n",
      "Step:533, train: 1.542643521364059e-07\n",
      "Step:534, train: 1.5838057356561343e-07\n",
      "Step:535, train: 1.6103342725063244e-07\n",
      "Step:536, train: 1.6232605623635008e-07\n",
      "Step:537, train: 1.6237512780697682e-07\n",
      "Step:538, train: 1.6130210574832019e-07\n",
      "Step:539, train: 1.5923041256064895e-07\n",
      "Step:540, train: 1.5628345876518785e-07\n",
      "Step:541, train: 1.5257870003117055e-07\n",
      "Step:542, train: 1.482323385084863e-07\n",
      "Step:543, train: 1.4335240607063147e-07\n",
      "Step:544, train: 1.380394701285443e-07\n",
      "Step:545, train: 1.3238843600410223e-07\n",
      "Step:546, train: 1.2648552224095358e-07\n",
      "Step:547, train: 1.204089381187646e-07\n",
      "Step:548, train: 1.1422868025697012e-07\n",
      "Step:549, train: 1.0800775545604607e-07\n",
      "Step:550, train: 1.0180086546694614e-07\n",
      "Step:551, train: 1.9185104718707927e-07\n",
      "Step:552, train: 1.6825689206501412e-07\n",
      "Step:553, train: 1.0692955842012571e-07\n",
      "Step:554, train: 1.1527717452965358e-07\n",
      "Step:555, train: 1.2221168275518386e-07\n",
      "Step:556, train: 1.2773785288886496e-07\n",
      "Step:557, train: 1.3189598427752688e-07\n",
      "Step:558, train: 1.3475171190953784e-07\n",
      "Step:559, train: 1.3638769972626782e-07\n",
      "Step:560, train: 1.3690096097602042e-07\n",
      "Step:561, train: 1.3639381243890574e-07\n",
      "Step:562, train: 1.3497320657243186e-07\n",
      "Step:563, train: 1.3274694079193463e-07\n",
      "Step:564, train: 1.2982108035439375e-07\n",
      "Step:565, train: 1.262965223702252e-07\n",
      "Step:566, train: 1.2227175907938623e-07\n",
      "Step:567, train: 1.1783754361620531e-07\n",
      "Step:568, train: 1.1307975422504731e-07\n",
      "Step:569, train: 1.080769984439686e-07\n",
      "Step:570, train: 1.0290071388889352e-07\n",
      "Step:571, train: 9.761492508162267e-08\n",
      "Step:572, train: 9.227799160569892e-08\n",
      "Step:573, train: 1.7505971451562366e-07\n",
      "Step:574, train: 1.4961398077643528e-07\n",
      "Step:575, train: 9.830923566412157e-08\n",
      "Step:576, train: 1.0662818580503359e-07\n",
      "Step:577, train: 1.136011832597733e-07\n",
      "Step:578, train: 1.1921904421720757e-07\n",
      "Step:579, train: 1.2351201096962973e-07\n",
      "Step:580, train: 1.2653585673564806e-07\n",
      "Step:581, train: 1.2836621053059898e-07\n",
      "Step:582, train: 1.290926141005211e-07\n",
      "Step:583, train: 1.2881334243939194e-07\n",
      "Step:584, train: 1.276312726573836e-07\n",
      "Step:585, train: 1.2564940981224392e-07\n",
      "Step:586, train: 1.2297139233797085e-07\n",
      "Step:587, train: 1.196967282041386e-07\n",
      "Step:588, train: 1.1592194481151744e-07\n",
      "Step:589, train: 1.1173656238180454e-07\n",
      "Step:590, train: 1.0722552027420024e-07\n",
      "Step:591, train: 1.0246579724419856e-07\n",
      "Step:592, train: 9.752903384260044e-08\n",
      "Step:593, train: 9.247844321869846e-08\n",
      "Step:594, train: 8.737231149420734e-08\n",
      "Step:595, train: 1.221693855996775e-07\n",
      "Step:596, train: 8.665263417281263e-08\n",
      "Step:597, train: 9.004534446819407e-08\n",
      "Step:598, train: 9.247586692764554e-08\n",
      "Step:599, train: 9.399771668457593e-08\n",
      "Step:600, train: 9.467571692534785e-08\n",
      "Step:601, train: 9.458314969166735e-08\n",
      "Step:602, train: 9.379643194062684e-08\n",
      "Step:603, train: 9.239487614727624e-08\n",
      "Step:604, train: 9.045660203128929e-08\n",
      "Step:605, train: 8.805813520260497e-08\n",
      "Step:606, train: 8.527314727132751e-08\n",
      "Step:607, train: 8.217135176029364e-08\n",
      "Step:608, train: 1.0522078565015973e-07\n",
      "Step:609, train: 8.692052889262969e-08\n",
      "Step:610, train: 9.386718327098813e-08\n",
      "Step:611, train: 9.961821628135187e-08\n",
      "Step:612, train: 1.0417502331362142e-07\n",
      "Step:613, train: 1.0756938785998144e-07\n",
      "Step:614, train: 1.0985622459656097e-07\n",
      "Step:615, train: 1.1110707867656494e-07\n",
      "Step:616, train: 1.1140473470904007e-07\n",
      "Step:617, train: 1.1083807523109167e-07\n",
      "Step:618, train: 1.094998222008954e-07\n",
      "Step:619, train: 1.0748360148605642e-07\n",
      "Step:620, train: 1.048816593194735e-07\n",
      "Step:621, train: 1.017823369161072e-07\n",
      "Step:622, train: 9.827132930997125e-08\n",
      "Step:623, train: 9.442744898504573e-08\n",
      "Step:624, train: 9.032503125732127e-08\n",
      "Step:625, train: 8.60317548260681e-08\n",
      "Step:626, train: 8.160976830453969e-08\n",
      "Step:627, train: 7.711458589422298e-08\n",
      "Step:628, train: 1.0279913807056915e-07\n",
      "Step:629, train: 7.754829339038109e-08\n",
      "Step:630, train: 8.153434727110283e-08\n",
      "Step:631, train: 8.456970382627482e-08\n",
      "Step:632, train: 8.669170894577334e-08\n",
      "Step:633, train: 8.795414540099919e-08\n",
      "Step:634, train: 8.84210001386586e-08\n",
      "Step:635, train: 8.816330685347317e-08\n",
      "Step:636, train: 8.725614470278123e-08\n",
      "Step:637, train: 8.577537027093682e-08\n",
      "Step:638, train: 8.379718605629585e-08\n",
      "Step:639, train: 8.13950309628188e-08\n",
      "Step:640, train: 7.863976107895448e-08\n",
      "Step:641, train: 7.559796981371089e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:642, train: 7.233175931187e-08\n",
      "Step:643, train: 9.166819538843469e-08\n",
      "Step:644, train: 7.597029369357113e-08\n",
      "Step:645, train: 8.201122913717295e-08\n",
      "Step:646, train: 8.69870796370248e-08\n",
      "Step:647, train: 9.089940445717606e-08\n",
      "Step:648, train: 9.377770730922923e-08\n",
      "Step:649, train: 9.567196186380408e-08\n",
      "Step:650, train: 9.664706046473577e-08\n",
      "Step:651, train: 9.677792540849356e-08\n",
      "Step:652, train: 9.61451867286254e-08\n",
      "Step:653, train: 9.483301385151138e-08\n",
      "Step:654, train: 9.292658833766579e-08\n",
      "Step:655, train: 9.05087840065619e-08\n",
      "Step:656, train: 8.765977920348935e-08\n",
      "Step:657, train: 8.44559579697349e-08\n",
      "Step:658, train: 8.096923279522069e-08\n",
      "Step:659, train: 7.726578133149238e-08\n",
      "Step:660, train: 7.340686909365837e-08\n",
      "Step:661, train: 6.944724869878422e-08\n",
      "Step:662, train: 6.543594031406204e-08\n",
      "Step:663, train: 1.1855598704608595e-07\n",
      "Step:664, train: 9.50476888084453e-08\n",
      "Step:665, train: 7.185884497010421e-08\n",
      "Step:666, train: 7.969038027023309e-08\n",
      "Step:667, train: 8.642213610052395e-08\n",
      "Step:668, train: 9.200654693662907e-08\n",
      "Step:669, train: 9.643728203481358e-08\n",
      "Step:670, train: 9.974028609819638e-08\n",
      "Step:671, train: 1.0196576200754213e-07\n",
      "Step:672, train: 1.031813406567972e-07\n",
      "Step:673, train: 1.0346637013577729e-07\n",
      "Step:674, train: 1.0290834054120567e-07\n",
      "Step:675, train: 1.0159810196280803e-07\n",
      "Step:676, train: 9.96276658939965e-08\n",
      "Step:677, train: 9.708790697413345e-08\n",
      "Step:678, train: 9.406778792222652e-08\n",
      "Step:679, train: 9.065090033935911e-08\n",
      "Step:680, train: 8.691682974361725e-08\n",
      "Step:681, train: 8.29385609633805e-08\n",
      "Step:682, train: 7.878375658842835e-08\n",
      "Step:683, train: 7.451344920741096e-08\n",
      "Step:684, train: 7.018203654565165e-08\n",
      "Step:685, train: 6.583861138922146e-08\n",
      "Step:686, train: 6.152540289988016e-08\n",
      "Step:687, train: 9.110776058224734e-08\n",
      "Step:688, train: 5.960149126931092e-08\n",
      "Step:689, train: 6.124040387305187e-08\n",
      "Step:690, train: 6.22324739739496e-08\n",
      "Step:691, train: 6.262460316565206e-08\n",
      "Step:692, train: 6.246849557311252e-08\n",
      "Step:693, train: 6.182153610921515e-08\n",
      "Step:694, train: 6.074081711366727e-08\n",
      "Step:695, train: 5.9284590059470944e-08\n",
      "Step:696, train: 5.750921418406676e-08\n",
      "Step:697, train: 8.439780051756593e-08\n",
      "Step:698, train: 6.007528724137778e-08\n",
      "Step:699, train: 6.386937145162869e-08\n",
      "Step:700, train: 6.684831339958181e-08\n",
      "Step:701, train: 6.90317221004651e-08\n",
      "Step:702, train: 7.045618958714575e-08\n",
      "Step:703, train: 7.117134360898601e-08\n",
      "Step:704, train: 7.123513386940527e-08\n",
      "Step:705, train: 7.070985963519879e-08\n",
      "Step:706, train: 6.966219800438895e-08\n",
      "Step:707, train: 6.815805028564496e-08\n",
      "Step:708, train: 6.626308085890443e-08\n",
      "Step:709, train: 6.40404341401552e-08\n",
      "Step:710, train: 6.15510056544052e-08\n",
      "Step:711, train: 5.885096114621764e-08\n",
      "Step:712, train: 5.599232444100687e-08\n",
      "Step:713, train: 6.979716352547382e-08\n",
      "Step:714, train: 5.8985364633473156e-08\n",
      "Step:715, train: 6.41092479623126e-08\n",
      "Step:716, train: 6.835250431384023e-08\n",
      "Step:717, train: 7.170708746759729e-08\n",
      "Step:718, train: 7.419125645334158e-08\n",
      "Step:719, train: 7.584255259335198e-08\n",
      "Step:720, train: 7.671324600563421e-08\n",
      "Step:721, train: 7.686462955447033e-08\n",
      "Step:722, train: 7.636598417546786e-08\n",
      "Step:723, train: 7.528833937894825e-08\n",
      "Step:724, train: 7.370385846356287e-08\n",
      "Step:725, train: 7.168498412042807e-08\n",
      "Step:726, train: 6.930167374445994e-08\n",
      "Step:727, train: 6.662000550538433e-08\n",
      "Step:728, train: 6.370259314364342e-08\n",
      "Step:729, train: 6.06072353150976e-08\n",
      "Step:730, train: 5.738652568019035e-08\n",
      "Step:731, train: 5.408885867281651e-08\n",
      "Step:732, train: 5.075674286738909e-08\n",
      "Step:733, train: 9.159837380238736e-08\n",
      "Step:734, train: 7.269761653565447e-08\n",
      "Step:735, train: 5.614951532345089e-08\n",
      "Step:736, train: 6.275592064623122e-08\n",
      "Step:737, train: 6.845860521862495e-08\n",
      "Step:738, train: 7.32045103905625e-08\n",
      "Step:739, train: 7.697927681537943e-08\n",
      "Step:740, train: 7.97988345966166e-08\n",
      "Step:741, train: 8.170147558674557e-08\n",
      "Step:742, train: 8.274261217219523e-08\n",
      "Step:743, train: 8.298882400276243e-08\n",
      "Step:744, train: 8.251420855304861e-08\n",
      "Step:745, train: 8.139684429826457e-08\n",
      "Step:746, train: 7.971673458528518e-08\n",
      "Step:747, train: 7.755338483547523e-08\n",
      "Step:748, train: 7.498321855890193e-08\n",
      "Step:749, train: 7.208071680855027e-08\n",
      "Step:750, train: 6.891448943575712e-08\n",
      "Step:751, train: 6.554899822785446e-08\n",
      "Step:752, train: 6.20423760450727e-08\n",
      "Step:753, train: 5.8448550730794905e-08\n",
      "Step:754, train: 5.481472068260953e-08\n",
      "Step:755, train: 5.118285306275616e-08\n",
      "Step:756, train: 4.7589455307533383e-08\n",
      "Step:757, train: 7.587319192002548e-08\n",
      "Step:758, train: 4.545892584192969e-08\n",
      "Step:759, train: 4.633432423532198e-08\n",
      "Step:760, train: 4.672688326599494e-08\n",
      "Step:761, train: 4.6676714439861877e-08\n",
      "Step:762, train: 4.6228073841253154e-08\n",
      "Step:763, train: 4.5427628935760114e-08\n",
      "Step:764, train: 5.227679043899483e-08\n",
      "Step:765, train: 5.5291555561559706e-08\n",
      "Step:766, train: 6.565667838356696e-08\n",
      "Step:767, train: 7.51252262004711e-08\n",
      "Step:768, train: 8.350041822366799e-08\n",
      "Step:769, train: 9.066387505525273e-08\n",
      "Step:770, train: 9.655936068425558e-08\n",
      "Step:771, train: 1.0117828888591509e-07\n",
      "Step:772, train: 1.0454953983643855e-07\n",
      "Step:773, train: 1.0672981345446313e-07\n",
      "Step:774, train: 1.0779644728854344e-07\n",
      "Step:775, train: 1.0784110614652248e-07\n",
      "Step:776, train: 1.0696330812585481e-07\n",
      "Step:777, train: 1.0526740477648095e-07\n",
      "Step:778, train: 1.0285902641951453e-07\n",
      "Step:779, train: 9.984303778601036e-08\n",
      "Step:780, train: 9.631993223512704e-08\n",
      "Step:781, train: 9.23858330813572e-08\n",
      "Step:782, train: 8.81302106455236e-08\n",
      "Step:783, train: 8.363662390080544e-08\n",
      "Step:784, train: 7.898099270851874e-08\n",
      "Step:785, train: 7.423157589375749e-08\n",
      "Step:786, train: 6.944906027794803e-08\n",
      "Step:787, train: 6.468705716952418e-08\n",
      "Step:788, train: 5.999141605382933e-08\n",
      "Step:789, train: 5.540166774584508e-08\n",
      "Step:790, train: 5.0950481930813645e-08\n",
      "Step:791, train: 4.666486634948658e-08\n",
      "Step:792, train: 4.256568713328824e-08\n",
      "Step:793, train: 6.197566217231991e-08\n",
      "Step:794, train: 3.970060490504932e-08\n",
      "Step:795, train: 4.028359616487252e-08\n",
      "Step:796, train: 4.0451723474456113e-08\n",
      "Step:797, train: 4.0242689871287255e-08\n",
      "Step:798, train: 3.9697443257770946e-08\n",
      "Step:799, train: 3.885776080931884e-08\n",
      "Step:800, train: 5.975312182994974e-08\n",
      "Step:801, train: 4.1320543900156405e-08\n",
      "Step:802, train: 4.427310048240526e-08\n",
      "Step:803, train: 4.66106314973151e-08\n",
      "Step:804, train: 4.8341573764309853e-08\n",
      "Step:805, train: 4.9489159391504155e-08\n",
      "Step:806, train: 5.0088281906903385e-08\n",
      "Step:807, train: 5.018221766344951e-08\n",
      "Step:808, train: 4.981883519820153e-08\n",
      "Step:809, train: 4.904863919882596e-08\n",
      "Step:810, train: 4.792436181548347e-08\n",
      "Step:811, train: 4.649777828493196e-08\n",
      "Step:812, train: 4.481941651844363e-08\n",
      "Step:813, train: 4.293731262760308e-08\n",
      "Step:814, train: 4.0896834832031027e-08\n",
      "Step:815, train: 3.873953973888297e-08\n",
      "Step:816, train: 4.4827013662970473e-08\n",
      "Step:817, train: 4.2882978891791525e-08\n",
      "Step:818, train: 4.864278277151011e-08\n",
      "Step:819, train: 5.3671674314422496e-08\n",
      "Step:820, train: 5.790416891105152e-08\n",
      "Step:821, train: 6.131261565136661e-08\n",
      "Step:822, train: 6.390054297713556e-08\n",
      "Step:823, train: 6.569295977765492e-08\n",
      "Step:824, train: 6.67318019165684e-08\n",
      "Step:825, train: 6.707194302777512e-08\n",
      "Step:826, train: 6.677579177167665e-08\n",
      "Step:827, train: 6.591085231224933e-08\n",
      "Step:828, train: 6.454742146345594e-08\n",
      "Step:829, train: 6.275569030650386e-08\n",
      "Step:830, train: 6.06038477558523e-08\n",
      "Step:831, train: 5.8158014885914276e-08\n",
      "Step:832, train: 5.5480208858395905e-08\n",
      "Step:833, train: 5.262822117398603e-08\n",
      "Step:834, train: 4.965510492673415e-08\n",
      "Step:835, train: 4.6609043332180714e-08\n",
      "Step:836, train: 4.353262500284262e-08\n",
      "Step:837, train: 4.046308665444917e-08\n",
      "Step:838, train: 3.743336612920041e-08\n",
      "Step:839, train: 3.447084796217117e-08\n",
      "Step:840, train: 6.803870443061705e-08\n",
      "Step:841, train: 6.292066324546831e-08\n",
      "Step:842, train: 3.54639727240384e-08\n",
      "Step:843, train: 3.8435371604331726e-08\n",
      "Step:844, train: 4.084464390756117e-08\n",
      "Step:845, train: 4.2689360190827565e-08\n",
      "Step:846, train: 4.398407101530566e-08\n",
      "Step:847, train: 4.475572262189382e-08\n",
      "Step:848, train: 4.5040898606290626e-08\n",
      "Step:849, train: 4.488182361271887e-08\n",
      "Step:850, train: 4.432520528539204e-08\n",
      "Step:851, train: 4.341972251336954e-08\n",
      "Step:852, train: 4.2213457542957316e-08\n",
      "Step:853, train: 4.075499590095474e-08\n",
      "Step:854, train: 3.909063129783623e-08\n",
      "Step:855, train: 3.726397935950847e-08\n",
      "Step:856, train: 3.531532956013959e-08\n",
      "Step:857, train: 3.328272789850463e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:858, train: 4.722138286645633e-08\n",
      "Step:859, train: 3.353955260114587e-08\n",
      "Step:860, train: 3.5394415166302785e-08\n",
      "Step:861, train: 3.676771407421229e-08\n",
      "Step:862, train: 3.767698754034253e-08\n",
      "Step:863, train: 3.8148902976930044e-08\n",
      "Step:864, train: 3.821744548798054e-08\n",
      "Step:865, train: 3.7921098337456534e-08\n",
      "Step:866, train: 3.730074399922431e-08\n",
      "Step:867, train: 3.639847152055358e-08\n",
      "Step:868, train: 3.525665262562082e-08\n",
      "Step:869, train: 3.391609174470846e-08\n",
      "Step:870, train: 3.241644184007955e-08\n",
      "Step:871, train: 3.07942466227404e-08\n",
      "Step:872, train: 5.524906776212834e-08\n",
      "Step:873, train: 3.055907046320729e-08\n",
      "Step:874, train: 3.1623340424886146e-08\n",
      "Step:875, train: 3.229294579197414e-08\n",
      "Step:876, train: 3.259304997212268e-08\n",
      "Step:877, train: 3.255403604048631e-08\n",
      "Step:878, train: 3.2210114578636216e-08\n",
      "Step:879, train: 3.1597372547201324e-08\n",
      "Step:880, train: 3.0752298635818346e-08\n",
      "Step:881, train: 2.971125635154978e-08\n",
      "Step:882, train: 5.0421232294753384e-08\n",
      "Step:883, train: 3.0620664580458965e-08\n",
      "Step:884, train: 3.228378307617282e-08\n",
      "Step:885, train: 3.3503164384876536e-08\n",
      "Step:886, train: 3.4295554364591364e-08\n",
      "Step:887, train: 3.468643300281112e-08\n",
      "Step:888, train: 3.470721726213877e-08\n",
      "Step:889, train: 3.439449468145545e-08\n",
      "Step:890, train: 3.378610859623615e-08\n",
      "Step:891, train: 3.292181274699731e-08\n",
      "Step:892, train: 3.184085877756816e-08\n",
      "Step:893, train: 3.058085018099706e-08\n",
      "Step:894, train: 2.9178511263203364e-08\n",
      "Step:895, train: 4.575623646471381e-08\n",
      "Step:896, train: 2.9691779229315014e-08\n",
      "Step:897, train: 3.1279247988613506e-08\n",
      "Step:898, train: 3.243524296348752e-08\n",
      "Step:899, train: 3.317623703475938e-08\n",
      "Step:900, train: 3.352795350055542e-08\n",
      "Step:901, train: 3.3520947100581204e-08\n",
      "Step:902, train: 3.319114275315747e-08\n",
      "Step:903, train: 3.257607413414045e-08\n",
      "Step:904, train: 3.171453951397882e-08\n",
      "Step:905, train: 3.0644430839256984e-08\n",
      "Step:906, train: 2.9403413574906505e-08\n",
      "Step:907, train: 2.8026613619773884e-08\n",
      "Step:908, train: 4.524535029985646e-08\n",
      "Step:909, train: 2.8378759889051682e-08\n",
      "Step:910, train: 2.979440764649393e-08\n",
      "Step:911, train: 3.0802057430312426e-08\n",
      "Step:912, train: 3.14192026960466e-08\n",
      "Step:913, train: 3.167142218908908e-08\n",
      "Step:914, train: 3.1589461725253046e-08\n",
      "Step:915, train: 3.120767378450652e-08\n",
      "Step:916, train: 3.056269144762076e-08\n",
      "Step:917, train: 2.9691147927826088e-08\n",
      "Step:918, train: 2.8629978837196018e-08\n",
      "Step:919, train: 2.741381371469735e-08\n",
      "Step:920, train: 3.775681421574965e-08\n",
      "Step:921, train: 2.8917351374016556e-08\n",
      "Step:922, train: 3.1301763286292046e-08\n",
      "Step:923, train: 3.3210902536792804e-08\n",
      "Step:924, train: 3.464323881414797e-08\n",
      "Step:925, train: 3.561241528166588e-08\n",
      "Step:926, train: 3.6143137039621656e-08\n",
      "Step:927, train: 3.6267105221183695e-08\n",
      "Step:928, train: 3.602209168380092e-08\n",
      "Step:929, train: 3.544923304113803e-08\n",
      "Step:930, train: 3.459054489188421e-08\n",
      "Step:931, train: 3.34891542607264e-08\n",
      "Step:932, train: 3.218627919329896e-08\n",
      "Step:933, train: 3.072194136754656e-08\n",
      "Step:934, train: 2.9133728612115593e-08\n",
      "Step:935, train: 2.745683228648569e-08\n",
      "Step:936, train: 2.5722623820815048e-08\n",
      "Step:937, train: 4.6718899098497736e-08\n",
      "Step:938, train: 3.2174163722333766e-08\n",
      "Step:939, train: 3.13350579698429e-08\n",
      "Step:940, train: 3.7360084620071434e-08\n",
      "Step:941, train: 4.2831565828948615e-08\n",
      "Step:942, train: 4.762310814212726e-08\n",
      "Step:943, train: 5.1659145100381196e-08\n",
      "Step:944, train: 5.490415782799393e-08\n",
      "Step:945, train: 5.735440249434752e-08\n",
      "Step:946, train: 5.90307231082319e-08\n",
      "Step:947, train: 5.997295372667346e-08\n",
      "Step:948, train: 6.023333319159714e-08\n",
      "Step:949, train: 5.98743458102912e-08\n",
      "Step:950, train: 5.89640118976637e-08\n",
      "Step:951, train: 5.7571906238348e-08\n",
      "Step:952, train: 5.576923198399385e-08\n",
      "Step:953, train: 5.3625699425253506e-08\n",
      "Step:954, train: 5.120771231109025e-08\n",
      "Step:955, train: 4.8578761023751056e-08\n",
      "Step:956, train: 4.579710760476692e-08\n",
      "Step:957, train: 4.29157752645554e-08\n",
      "Step:958, train: 3.998268375638553e-08\n",
      "Step:959, train: 3.7040306885646404e-08\n",
      "Step:960, train: 3.41250869889845e-08\n",
      "Step:961, train: 3.1269107826304024e-08\n",
      "Step:962, train: 2.8498406745519644e-08\n",
      "Step:963, train: 2.5834827749630194e-08\n",
      "Step:964, train: 2.3295379473694407e-08\n",
      "Step:965, train: 5.1487296035429547e-08\n",
      "Step:966, train: 5.183396239879273e-08\n",
      "Step:967, train: 2.7597492255747166e-08\n",
      "Step:968, train: 3.094699557091422e-08\n",
      "Step:969, train: 3.9138867307667504e-08\n",
      "Step:970, train: 4.688453026097531e-08\n",
      "Step:971, train: 5.393491686263381e-08\n",
      "Step:972, train: 6.012224749032396e-08\n",
      "Step:973, train: 6.534399040275805e-08\n",
      "Step:974, train: 6.955126888653046e-08\n",
      "Step:975, train: 7.273703088213542e-08\n",
      "Step:976, train: 7.492606309787322e-08\n",
      "Step:977, train: 7.616825577372752e-08\n",
      "Step:978, train: 7.653044514572722e-08\n",
      "Step:979, train: 7.609240939145647e-08\n",
      "Step:980, train: 7.494128594888655e-08\n",
      "Step:981, train: 7.316797316831846e-08\n",
      "Step:982, train: 7.086395611437024e-08\n",
      "Step:983, train: 6.811921545369492e-08\n",
      "Step:984, train: 6.502027056580424e-08\n",
      "Step:985, train: 6.164878682074723e-08\n",
      "Step:986, train: 5.807996132610745e-08\n",
      "Step:987, train: 5.438331079499207e-08\n",
      "Step:988, train: 5.0620439235510394e-08\n",
      "Step:989, train: 4.684645445291936e-08\n",
      "Step:990, train: 4.3108855559684606e-08\n",
      "Step:991, train: 3.9449264997759273e-08\n",
      "Step:992, train: 3.590163616039749e-08\n",
      "Step:993, train: 3.2493628838554856e-08\n",
      "Step:994, train: 2.9247313574857393e-08\n",
      "Step:995, train: 2.6179776407073316e-08\n",
      "Step:996, train: 2.3302515178019847e-08\n",
      "Step:997, train: 2.0622865476269995e-08\n",
      "Step:998, train: 4.783826353271629e-08\n",
      "Step:999, train: 5.10964455473193e-08\n",
      "5.10964455473193e-08\n"
     ]
    }
   ],
   "source": [
    "model.train(inputs = [],\n",
    "            targets = [],\n",
    "            num_iter = 1000,\n",
    "            N = 0,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qiskit",
   "language": "python",
   "name": "env_qiskit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
